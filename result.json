[{"year":"2022","abstract":"We present Samanantar the largest publicly available parallel corpora collection for Indic languages. The collection contains a total of 49.7 million sentence pairs between English and 11 Indic languages (from two language families). Specifically we compile 12.4 million sentence pairs from existing publicly available parallel corpora and additionally mine 37.4 million sentence pairs from the Web resulting in a 4mboxtimes increase. We mine the parallel sentences from the Web by combining many corpora tools and methods: (a) Web-crawled monolingual corpora (b) document OCR for extracting sentences from scanned documents (c) multilingual representation models for aligning sentences and (d) approximate nearest neighbor search for searching in a large collection of sentences. Human evaluation of samples from the newly mined corpora validate the high quality of the parallel sentences across 11 languages. Further we extract 83.4 million sentence pairs between all 55 Indic language pairs from the English-centric parallel corpus using English as the pivot language. We trained multilingual NMT models spanning all these languages on Samanantar which outperform existing models and baselines on publicly available benchmarks such as FLORES establishing the utility of Samanantar. Our data and models are available publicly at Samanantar and we hope they will help advance research in NMT and multilingual NLP for Indic languages.","title":"Samanantar: The Largest Publicly Available Parallel Corpora Collection for 11 Indic Languages","url":"https:\\aclanthology.org\2022.tacl-1.9"},{"year":"2022","abstract":"We present the AGODA (Analyse s'emantique et Graphes relationnels pour l'Ouverture des D'ebats `a l'Assembl'ee nationale) project which aims to create a platform for consulting and exploring digitised French parliamentary debates (1881-1940) available in the digital library of the National Library of France. This project brings together historians and NLP specialists: parliamentary debates are indeed an essential source for French history of the contemporary period but also for linguistics. This project therefore aims to produce a corpus of texts that can be easily exploited with computational methods and that respect the TEI standard. Ancient parliamentary debates are also an excellent case study for the development and application of tools for publishing and exploring large historical corpora. In this paper we present the steps necessary to produce such a corpus. We detail the processing and publication chain of these documents in particular by mentioning the problems linked to the extraction of texts from digitised images. We also introduce the first analyses that we have carried out on this corpus with ``bag-of-words'' techniques not too sensitive to OCR quality (namely topic modelling and word embedding).","title":"Between History and Natural Language Processing: Study Enrichment and Online Publication of French Parliamentary Debates of the Early Third Republic (1881-1899)","url":"https:\\aclanthology.org\2022.parlaclarin-1.3"},{"year":"2022","abstract":"Deep Learning (DL) is dominating the fields of Natural Language Processing (NLP) and Computer Vision (CV) in the recent times. However DL commonly relies on the availability of large data annotations so other alternative or complementary pattern-based techniques can help to improve results. In this paper we build upon Key Information Extraction (KIE) in purchase documents using both DL and rule-based corrections. Our system initially trusts on Optical Character Recognition (OCR) and text understanding based on entity tagging to identify purchase facts of interest (e.g. product codes descriptions quantities or prices). These facts are then linked to a same product group which is recognized by means of line detection and some grouping heuristics. Once these DL approaches are processed we contribute several mechanisms consisting of rule-based corrections for improving the baseline DL predictions. We prove the enhancements provided by these rule-based corrections over the baseline DL results in the presented experiments for purchase documents from public and NielsenIQ datasets.","title":"Key Information Extraction in Purchase Documents using Deep Learning and Rule-based Corrections","url":"https:\\aclanthology.org\2022.pandl-1.2"},{"year":"2022","abstract":"Document Information Extraction (DIE) has attracted increasing attention due to its various advanced applications in the real world. Although recent literature has already achieved competitive results these approaches usually fail when dealing with complex documents with noisy OCR results or mutative layouts. This paper proposes Generative Multi-modal Network (GMN) for real-world scenarios to address these problems which is a robust multi-modal generation method without predefined label categories. With the carefully designed spatial encoder and modal-aware mask module GMN can deal with complex documents that are hard to serialized into sequential order. Moreover GMN tolerates errors in OCR results and requires no character-level annotation which is vital because fine-grained annotation of numerous documents is laborious and even requires annotators with specialized domain knowledge. Extensive experiments show that GMN achieves new state-of-the-art performance on several public DIE datasets and surpasses other methods by a large margin especially in realistic scenes.","title":"GMN: Generative Multi-modal Network for Practical Document Information Extraction","url":"https:\\aclanthology.org\2022.naacl-main.276"},{"year":"2022","abstract":"In recent years the availability of medieval charter texts has increased thanks to advances in OCR and HTR techniques. But the lack of models that automatically structure the textual output continues to hinder the extraction of large-scale lectures from these historical sources that are among the most important for medieval studies. This paper presents the process of annotating and modelling a corpus to automatically detect named entities in medieval charters in Latin French and Spanish and address the problem of multilingual writing practices in the Late Middle Ages. It introduces a new annotated multilingual corpus and presents a training pipeline using two approaches: (1) a method using contextual and static embeddings coupled to a Bi-LSTM-CRF classifier; (2) a fine-tuning method using the pre-trained multilingual BERT and RoBERTa models. The experiments described here are based on a corpus encompassing about 2.3M words (7576 charters) coming from five charter collections ranging from the 10th to the 15th centuries. The evaluation proves that both multilingual classifiers based on general purpose models and those specifically designed achieve high-performance results and do not show performance drop compared to their monolingual counterparts. This paper describes the corpus and the annotation guideline and discusses the issues related to the linguistic of the charters the multilingual writing practices so as to interpret the results within a larger historical perspective.","title":"Multilingual Named Entity Recognition for Medieval Charters Using Stacked Embeddings and Bert-based Models.","url":"https:\\aclanthology.org\2022.lt4hala-1.17"},{"year":"2022","abstract":"CAMIO (Corpus of Annotated Multilingual Images for OCR) is a new corpus created by Linguistic Data Consortium to serve as a resource to support the development and evaluation of optical character recognition (OCR) and related technologies for 35 languages across 24 unique scripts. The corpus comprises nearly 70000 images of machine printed text covering a wide variety of topics and styles document domains attributes and scanning\capture artifacts. Most images have been exhaustively annotated for text localization resulting in over 2.3M line-level bounding boxes. For 13 of the 35 languages 1250 images\language have been further annotated with orthographic transcriptions of each line plus specification of reading order yielding over 2.4M tokens of transcribed text. The resulting annotations are represented in a comprehensive XML output format defined for this corpus. The paper discusses corpus design and implementation challenges encountered baseline performance results obtained on the corpus for text localization and OCR decoding and plans for corpus publication.","title":"CAMIO: A Corpus for OCR in Multiple Languages","url":"https:\\aclanthology.org\2022.lrec-1.129"},{"year":"2022","abstract":"Parliamentary debates represent a large and partly unexploited treasure trove of publicly accessible texts. In the German-speaking area there is a certain deficit of uniformly accessible and annotated corpora covering all German-speaking parliaments at the national and federal level. To address this gap we introduce the German Parliamentary Corpus (GerParCor). GerParCor is a genre-specific corpus of (predominantly historical) German-language parliamentary protocols from three centuries and four countries including state and federal level data. In addition GerParCor contains conversions of scanned protocols and in particular of protocols in Fraktur converted via an OCR process based on Tesseract. All protocols were preprocessed by means of the NLP pipeline of spaCy3 and automatically annotated with metadata regarding their session date. GerParCor is made available in the XMI format of the UIMA project. In this way GerParCor can be used as a large corpus of historical texts in the field of political communication for various tasks in NLP.","title":"German Parliamentary Corpus (GerParCor)","url":"https:\\aclanthology.org\2022.lrec-1.202"},{"year":"2022","abstract":"We hypothesise and evaluate a language model-based approach for scoring the quality of OCR transcriptions in the British Library Newspapers (BLN) corpus parts 1 and 2 to identify the best quality OCR for use in further natural language processing tasks with a wider view to link individual newspaper reports of crime in nineteenth-century London to the Digital Panopticon---a structured repository of criminal lives. We mitigate the absence of gold standard transcriptions of the BLN corpus by utilising a corpus of genre-adjacent texts that capture the common and legal parlance of nineteenth-century London---the Proceedings of the Old Bailey Online---with a view to rank the BLN transcriptions by their OCR quality.","title":"A Language Modelling Approach to Quality Assessment of OCR'ed Historical Text","url":"https:\\aclanthology.org\2022.lrec-1.630"},{"year":"2022","abstract":"Bodo is a scheduled Indian language spoken largely by the Bodo community of Assam and other northeastern Indian states. Due to a lack of resources it is difficult for young languages to communicate more effectively with the rest of the world. This leads to a lack of research in low-resource languages. The creation of a dataset is a tedious and costly process particularly for languages with no participatory research. This is more visible for languages that are young and have recently adopted standard writing scripts. In this paper we present a methodology using Google Keep for OCR to generate a monolingual Bodo corpus from different books. In this work a Bodo text corpus of 192327 tokens and 32268 unique tokens is generated using free accessible and daily-usable applications. Moreover some essential characteristics of the Bodo language are discussed that are neglected by Natural Language Progressing (NLP) researchers.","title":"Generating Monolingual Dataset for Low Resource Language Bodo from old books using Google Keep","url":"https:\\aclanthology.org\2022.lrec-1.705"},{"year":"2022","abstract":"It is well-known that the deep learning-based optical character recognition (OCR) system needs a large amount of data to train a high-performance character recognizer. However it is costly to collect a large amount of realistic handwritten characters. This paper introduces a Y-Autoencoder (Y-AE)-based handwritten character generator to generate multiple Japanese Hiragana characters with a single image to increase the amount of data for training a handwritten character recognizer. The adaptive instance normalization (AdaIN) layer allows the generator to be trained and generate handwritten character images without paired-character image labels. The experiment shows that the Y-AE could generate Japanese character images then used to train the handwritten character recognizer producing an F1-score improved from 0.8664 to 0.9281. We further analyzed the usefulness of the Y-AE-based generator with shape images out-of-character (OOC) images which have different character images styles in model training. The result showed that the generator could generate a handwritten image with a similar style to that of the input character.","title":"Handwritten Character Generation using Y-Autoencoder for Character Recognition Model Training","url":"https:\\aclanthology.org\2022.lrec-1.799"},{"year":"2022","abstract":"In this paper we describe a BERT model trained on the Eighteenth Century Collections Online (ECCO) dataset of digitized documents. The ECCO dataset poses unique modelling challenges due to the presence of Optical Character Recognition (OCR) artifacts. We establish the performance of the BERT model on a publication year prediction task against linear baseline models and human judgement finding the BERT model to be superior to both and able to date the works on average with less than 7 years absolute error. We also explore how language change over time affects the model by analyzing the features the model uses for publication year predictions as given by the Integrated Gradients model explanation method.","title":"Explainable Publication Year Prediction of Eighteenth Century Texts with the BERT Model","url":"https:\\aclanthology.org\2022.lchange-1.7"},{"year":"2022","abstract":"Legal field is characterized by its exclusivity and non-transparency. Despite the frequency and relevance of legal dealings legal documents like contracts remains elusive to non-legal professionals for the copious usage of legal jargon. There has been little advancement in making legal contracts more comprehensible. This paper presents how Machine Learning and NLP can be applied to solve this problem further considering the challenges of applying ML to the high length of contract documents and training in a low resource environment. The largest open-source contract dataset so far the Contract Understanding Atticus Dataset (CUAD) is utilized. Various pre-processing experiments and hyperparameter tuning have been carried out and we successfully managed to eclipse SOTA results presented for models in the CUAD dataset trained on RoBERTa-base. Our model A-type-RoBERTa-base achieved an AUPR score of 46.6% compared to 42.6% on the original RoBERT-base. This model is utilized in our end to end contract understanding application which is able to take a contract and highlight the clauses a user is looking to find along with it's descriptions to aid due diligence before signing. Alongside digital i.e. searchable contracts the system is capable of processing scanned i.e. non-searchable contracts using tesseract OCR. This application is aimed to not only make contract review a comprehensible process to non-legal professionals but also to help lawyers and attorneys more efficiently review contracts.","title":"An Open Source Contractual Language Understanding Application Using Machine Learning","url":"https:\\aclanthology.org\2022.lateraisse-1.6"},{"year":"2022","abstract":"La variation dans les donn'ees textuelles en particulier le bruit est un facteur limitant la performance des syst`emes de Reconnaissance d'Entit'es Nomm'ees (REN). Les syst`emes de REN sont en effet g'en'eralement entra^in'es sur des donn'ees « propres » non-bruit'ees ce qui n'est pas le cas des donn'ees des humanit'es num'eriques obtenues par reconnaissance optique de caract`eres (OCR). De fait la qualit'e des transcriptions OCR est souvent perccue comme la source principale des erreurs faites par les outils de REN. Cependant des r'esultats obtenus avec diff'erents syst`emes REN sur des transcriptions OCR d'un corpus du 19`eme si`ecle (ELTeC) tendent `a montrer une certaine robustesse modulo la pr'esence de formes bruit'ees parfois dites « contamin'ees ». La difficult'e est alors de lier ces formes contamin'ees avec leur forme de r'ef'erence par exemple pour rapprocher la cha^ine « Parisl »et la cha^ine « Paris ». Il s'agit de mod'eliser le fait que diff'erentes variations se rapprochent du m^eme terme. Des questions quant `a l'automatisation de cette t^ache et sa g'en'eralisation `a toutes les variations d'un m^eme terme restent ouvertes. Nous montrons dans cet article diff'erentes exp'eriences visant `a traiter ce probl`eme sous l`angle de la d'esambiguisation morphologique des entit'es nomm'ees (EN) en aval de la cha^ine de traitement plut^ot que par la correction en amont des donn'ees de l'OCR.","title":"Reconnaissance d'entit'es nomm'ees sur des sorties OCR bruit'ees : des pistes pour la d'esambiguisation morphologique automatique (Resolution of entity linking issues on noisy OCR output : automatic disambiguation tracks)","url":"https:\\aclanthology.org\2022.jeptalnrecital-humanum.6"},{"year":"2022","abstract":"L'extraction d'information offre de nouvelles perspectives au sein des recherches historiques. Cependant la majorit'e des recherches li'ees `a ce domaine s'effectue sur des donn'ees contemporaines. Malgr'e l''evolution constante des syst`emes d'OCR les textes historiques r'esultant de ce proc'ed'e contiennent toujours de multiples erreurs. Du fait d'un manque de ressources historiques d'edi'ees au TAL le traitement de ce domaine reste d'ependant de l'utilisation de ressources contemporaines. De nombreuses 'etudes ont d'emontr'e l'impact n'egatif que pouvaient avoir les erreurs d'OCR sur les syst`emes pr^ets `a l'emploi contemporains. Mais l''evaluation des nouvelles architectures proposant des r'esultats prometteurs sur des donn'ees r'ecentes face `a ce probl`eme reste encore tr`es minime. Dans cette 'etude nous quantifions l'impact des erreurs d'OCR sur trois t^aches d'extraction d'information en utilisant plusieurs architectures de type Transformers. Au vu de ces r'esultats nous proposons une approche permettant de r'eduire de plus de 50% cet impact sans avoir recours `a des ressources historiques sp'ecialis'ees.","title":"Simulation d'erreurs d'OCR dans les syst`emes de TAL pour le traitement de donn'ees anachroniques (Simulation of OCR errors in NLP systems for processing anachronistic data)","url":"https:\\aclanthology.org\2022.jeptalnrecital-humanum.9"},{"year":"2022","abstract":"Le projet Toolbox propose une cha^ine de traitement pour la manipulation et le traitement de corpus textuels incluant la num'erisation (OCR\HTR) la conversion au format TEI la fouille de texte (reconnaissance d'entit'es nomm'ees) et la visualisation de donn'ees. Les fonctionnalit'es sont accessibles via une interface en ligne qui sert de surcouche graphique `a des scripts d'evelopp'es par nos soins ou utilisant des outils externes. Elles permettent d'automatiser les t^aches 'el'ementaires de traitement de corpus pour les chercheurs en humanit'es num'eriques. Cet outil est ouvert aux contributions externes.","title":"Toolbox : une cha^ine de traitement de corpus pour les humanit'es num'eriques (Toolbox : a corpus processing pipeline for digital humanities)","url":"https:\\aclanthology.org\2022.jeptalnrecital-demo.4"},{"year":"2022","abstract":"Ad creatives are ads served to users on a webpage app or other digital environments. The demand for compelling ad creatives surges drastically with the ever-increasing popularity of digital marketing. The two most essential elements of (display) ad creatives are the advertising message such as headlines and description texts and the visual component such as images and videos. Traditionally ad creatives are composed by professional copywriters and creative designers. The process requires significant human effort limiting the scalability and efficiency of digital ad campaigns. This work introduces AUTOCREATIVE a novel system to automatically generate ad creatives relying on natural language generation and computer vision techniques. The system generates multiple ad copies (ad headlines\description texts) using a sequence-to-sequence model and selects images most suitable to the generated ad copies based on heuristic-based visual appeal metrics and a text-image retrieval pipeline.","title":"Automated Ad Creative Generation","url":"https:\\aclanthology.org\2022.inlg-demos.3"},{"year":"2022","abstract":"The aim of the paper is to apply for historical texts the methodology used commonly to solve various NLP tasks defined for contemporary data i.e. pre-train and fine-tune large Transformer models. This paper introduces an ML challenge named Challenging America (ChallAm) based on OCR-ed excerpts from historical newspapers collected from the Chronicling America portal. ChallAm provides a dataset of clippings labeled with metadata on their origin and paired with their textual contents retrieved by an OCR tool. Three publicly available ML tasks are defined in the challenge: to determine the article date to detect the location of the issue and to deduce a word in a text gap (cloze test). Strong baselines are provided for all three ChallAm tasks. In particular we pre-trained a RoBERTa model from scratch from the historical texts. We also discuss the issues of discrimination and hate-speech present in the historical American texts.","title":"Challenging America: Modeling language in longer time scales","url":"https:\\aclanthology.org\2022.findings-naacl.56"},{"year":"2022","abstract":"Sanskrit is a classical language with about 30 million extant manuscripts fit for digitisation available in written printed or scanned-image forms. However it is still considered to be a low-resource language when it comes to available digital resources. In this work we release a post-OCR text correction dataset containing around 218000 sentences with 1.5 million words from 30 different books. Texts in Sanskrit are known to be diverse in terms of their linguistic and stylistic usage since Sanskrit was the `lingua francua' for discourse in the Indian subcontinent for about 3 millennia. Keeping this in mind we release a multi-domain dataset from areas as diverse as astronomy medicine and mathematics with some of them as old as 18 centuries. Further we release multiple strong baselines as benchmarks for the task based on pre-trained Seq2Seq language models. We find that our best-performing model consisting of byte level tokenization in conjunction with phonetic encoding (Byt5+SLP1) yields a 23% point increase over the OCR output in terms of word and character error rates. Moreover we perform extensive experiments in evaluating these models on their performance and analyse common causes of mispredictions both at the graphemic and lexical levels. Our code and dataset is publicly available at https:\\github.com\ayushbits\pe-ocr-sanskrit.","title":"A Benchmark and Dataset for Post-OCR text correction in Sanskrit","url":"https:\\aclanthology.org\2022.findings-emnlp.466"},{"year":"2022","abstract":"We aim to investigate the performance of current OCR systems on low resource languages and low resource scripts.We introduce and make publicly available a novel benchmark OCR4MT consisting of real and synthetic data enriched with noise for 60 low-resource languages in low resource scripts. We evaluate state-of-the-art OCR systems on our benchmark and analyse most common errors. We show that OCR monolingual data is a valuable resource that can increase performance of Machine Translation models when used in backtranslation. We then perform an ablation study to investigate how OCR errors impact Machine Translation performance and determine what is the minimum level of OCR quality needed for the monolingual data to be useful for Machine Translation.","title":"OCR Improves Machine Translation for Low-Resource Languages","url":"https:\\aclanthology.org\2022.findings-acl.92"},{"year":"2022","abstract":"In this paper we look at the case of a Generic text-to-text NMT model that has to deal with data coming from various modalities like speech images or noisy text extracted from the web. We propose a two-step method based on composable adapters to deal with this problem of Multimodal Robustness. In a first step we separately learn domain adapters and modality specific adapters to deal with noisy input coming from various sources: ASR OCR or noisy text (UGC). In a second step we combine these components at runtime via dynamic routing or when the source of noise is unknown via two new transfer learning mechanisms (Fast Fusion and Multi Fusion). We show that our method provides a flexible state-of-the-art architecture able to deal with noisy multimodal inputs.","title":"Multimodal Robustness for Neural Machine Translation","url":"https:\\aclanthology.org\2022.emnlp-main.582"},{"year":"2022","abstract":"To automatically correct handwritten assignments the traditional approach is to use an OCR model to recognize characters and compare them to answers. The OCR model easily gets confused on recognizing handwritten Chinese characters and the textual information of the answers is missing during the model inference. However teachers always have these answers in mind to review and correct assignments. In this paper we focus on the Chinese cloze tests correction and propose a multimodal approach(named AiM). The encoded representations of answers interact with the visual information of students' handwriting. Instead of predicting `right' or `wrong' we perform the sequence labeling on the answer text to infer which answer character differs from the handwritten content in a fine-grained way. We take samples of OCR datasets as the positive samples for this task and develop a negative sample augmentation method to scale up the training data. Experimental results show that AiM outperforms OCR-based methods by a large margin. Extensive studies demonstrate the effectiveness of our multimodal approach.","title":"AiM: Taking Answers in Mind to Correct Chinese Cloze Tests in Educational Applications","url":"https:\\aclanthology.org\2022.coling-1.269"},{"year":"2022","abstract":"Humans use document formatting to discover document and section titles and important phrases. But when machines process a paper--especially documents OCRed from images--these cues are often invisible to downstream processes: words in footnotes or body text are treated as just as important as words in titles. It would be better for indexing and summarization tools to be guided by implicit document structure. In an ODNI-sponsored project ARLIS looked at discovering formatting in OCRed text as a way to infer document structure. Most OCR engines output results as hOCR (an XML format) giving bounding boxes around characters. In theory this also provides style information such as bolding and italicization but in practice this capability is limited. For example the Tesseract OCR tool provides bounding boxes but does not attempt to detect bold text (relevant to author emphasis and specialized fields in e.g. print dictionaries) and its discrimination of italicization is poor. Our project inferred font size from hOCR bounding boxes and using that and other cues (e.g. the fact that titles tend to be short) determined which text constituted section titles; from this a document outline can be created. We also experimented with algorithms for detecting bold text. Our best algorithm has a much improved recall and precision although the exact numbers are font-dependent. The next step is to incorporate inferred structure into the output of machine translation. One way is to embed XML tags for inferred structure into the text extracted from the imaged document and to either pass the strings enclosed by XML tags to the MT engine individually or pass the tags through the MT engine without modification. This structural information can guide downstream bulk processing tasks such as summarization and search and also enables building tables of contents for human users examining individual documents.","title":"You've translated it now what?","url":"https:\\aclanthology.org\2022.amta-upg.27"},{"year":"2021","abstract":"Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text- and visual-modalities. To this effect Facebook released the Hateful Memes Challenge a dataset of memes with pre-extracted text captions but it is unclear whether these synthetic examples generalize to `memes in the wild'. In this paper we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. We find that `memes in the wild' differ in two key aspects: 1) Captions must be extracted via OCR injecting noise and diminishing performance of multimodal models and 2) Memes are more diverse than `traditional memes' including screenshots of conversations or text on a plain background. This paper thus serves as a reality-check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.","title":"Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset","url":"https:\\aclanthology.org\2021.woah-1.4"},{"year":"2021","abstract":"Optical character recognition (OCR) from newspaper page images is susceptible to noise due to degradation of old documents and variation in typesetting. In this report we present a novel approach to OCR post-correction. We cast error correction as a translation task and fine-tune BART a transformer-based sequence-to-sequence language model pretrained to denoise corrupted text. We are the first to use sentence-level transformer models for OCR post-correction and our best model achieves a 29.4% improvement in character accuracy over the original noisy OCR text. Our results demonstrate the utility of pretrained language models for dealing with noisy text.","title":"BART for Post-Correction of OCR Newspaper Text","url":"https:\\aclanthology.org\2021.wnut-1.31"},{"year":"2021","abstract":"The domain-specialised application of Named Entity Recognition (NER) is known as Biomedical NER (BioNER) which aims to identify and classify biomedical concepts that are of interest to researchers such as genes proteins chemical compounds drugs mutations diseases and so on. The BioNER task is very similar to general NER but recognising Biomedical Named Entities (BNEs) is more challenging than recognising proper names from newspapers due to the characteristics of biomedical nomenclature. In order to address the challenges posed by BioNER seven machine learning models were implemented comparing a transfer learning approach based on fine-tuned BERT with Bi-LSTM based neural models and a CRF model used as baseline. Precision Recall and F1-score were used as performance scores evaluating the models on two well-known biomedical corpora: JNLPBA and BIOCREATIVE IV (BC-IV). Strict and partial matching were considered as evaluation criteria. The reported results show that a transfer learning approach based on fine-tuned BERT outperforms all others methods achieving the highest scores for all metrics on both corpora.","title":"A Comparison between Named Entity Recognition Models in the Biomedical Domain","url":"https:\\aclanthology.org\2021.triton-1.9"},{"year":"2021","abstract":"Abstract Optical character recognition (OCR) is crucial for a deeper access to historical collections. OCR needs to account for orthographic variations typefaces or language evolution (i.e. new letters word spellings) as the main source of character word or word segmentation transcription errors. For digital corpora of historical prints the errors are further exacerbated due to low scan quality and lack of language standardization. For the task of OCR post-hoc correction we propose a neural approach based on a combination of recurrent (RNN) and deep convolutional network (ConvNet) to correct OCR transcription errors. At character level we flexibly capture errors and decode the corrected output based on a novel attention mechanism. Accounting for the input and output similarity we propose a new loss function that rewards the model's correcting behavior. Evaluation on a historical book corpus in German language shows that our models are robust in capturing diverse OCR transcription errors and reduce the word error rate of 32.3% by more than 89%.","title":"Neural OCR Post-Hoc Correction of Historical Corpora","url":"https:\\aclanthology.org\2021.tacl-1.29"},{"year":"2021","abstract":"Abstract Much of the existing linguistic data in many languages of the world is locked away in non- digitized books and documents. Optical character recognition (OCR) can be used to produce digitized text and previous work has demonstrated the utility of neural post-correction methods that improve the results of general- purpose OCR systems on recognition of less- well-resourced languages. However these methods rely on manually curated post- correction data which are relatively scarce compared to the non-annotated raw images that need to be digitized. In this paper we present a semi-supervised learning method that makes it possible to utilize these raw images to improve performance specifically through the use of self-training a technique where a model is iteratively trained on its own outputs. In addition to enforce consistency in the recognized vocabulary we introduce a lexically aware decoding method that augments the neural post-correction model with a count-based language model constructed from the recognized texts implemented using weighted finite-state automata (WFSA) for efficient and effective decoding. Results on four endangered languages demonstrate the utility of the proposed method with relative error reductions of 15%--29% where we find the combination of self-training and lexically aware decoding essential for achieving consistent improvements.1","title":"Lexically Aware Semi-Supervised Learning for OCR Post-Correction","url":"https:\\aclanthology.org\2021.tacl-1.76"},{"year":"2021","abstract":"We describe our systems of subtask1 and subtask3 for SemEval-2021 Task 6 on Detection of Persuasion Techniques in Texts and Images. The purpose of subtask1 is to identify propaganda techniques given textual content and the goal of subtask3 is to detect them given both textual and visual content. For subtask1 we investigate transfer learning based on pre-trained language models (PLMs) such as BERT RoBERTa to solve data sparsity problems. For subtask3 we extract heterogeneous visual representations (i.e. face features OCR features and multimodal representations) and explore various multimodal fusion strategies to combine the textual and visual representations. The official evaluation shows our ensemble model ranks 1st for subtask1 and 2nd for subtask3.","title":"MinD at SemEval-2021 Task 6: Propaganda Detection using Transfer Learning and Multimodal Fusion","url":"https:\\aclanthology.org\2021.semeval-1.150"},{"year":"2021","abstract":"Deep CNN--LSTM hybrid neural networks have proven to improve the accuracy of Optical Character Recognition (OCR) models for different languages. In this paper we examine to what extent these networks improve the OCR accuracy rates on Swedish historical newspapers. By experimenting with the open source OCR engine Calamari we are able to show that mixed deep CNN--LSTM hybrid models outperform previous models on the task of character recognition of Swedish historical newspapers spanning 1818--1848. We achieved an average character accuracy rate (CAR) of 97.43% which is a new state--of--the--art result on 19th century Swedish newspaper text. Our data code and models are released under CC-BY licence.","title":"OCR Processing of Swedish Historical Newspapers Using Deep Hybrid CNN--LSTM Networks","url":"https:\\aclanthology.org\2021.ranlp-1.23"},{"year":"2021","abstract":"Post processing is the most conventional approach for correcting errors that are caused by Optical Character Recognition(OCR) systems. Two steps are usually taken to correct OCR errors: detection and corrections. For the first task supervised machine learning methods have shown state-of-the-art performances. Previously proposed approaches have focused most prominently on combining lexical contextual and statistical features for detecting errors. In this study we report a novel system to error detection which is based merely on the n-gram counts of a candidate token. In addition to being simple and computationally less expensive our proposed system beats previous systems reported in the ICDAR2019 competition on OCR-error detection with notable margins. We achieved state-of-the-art F1-scores for eight out of the ten involved European languages. The maximum improvement is for Spanish which improved from 0.69 to 0.90 and the minimum for Polish from 0.82 to 0.84.","title":"A Novel Machine Learning Based Approach for Post-OCR Error Detection","url":"https:\\aclanthology.org\2021.ranlp-1.164"},{"year":"2021","abstract":"In this work we show the process of building a large-scale training set from digital and digitized collections at a national library. The resulting Bidirectional Encoder Representations from Transformers (BERT)-based language model for Norwegian outperforms multilingual BERT (mBERT) models in several token and sequence classification tasks for both Norwegian Bokmaal and Norwegian Nynorsk. Our model also improves the mBERT performance for other languages present in the corpus such as English Swedish and Danish. For languages not included in the corpus the weights degrade moderately while keeping strong multilingual properties. Therefore we show that building high-quality models within a memory institution using somewhat noisy optical character recognition (OCR) content is feasible and we hope to pave the way for other memory institutions to follow.","title":"Operationalizing a National Digital Library: The Case for a Norwegian Transformer Model","url":"https:\\aclanthology.org\2021.nodalida-main.3"},{"year":"2021","abstract":"Historical corpora are known to contain errors introduced by OCR (optical character recognition) methods used in the digitization process often said to be degrading the performance of NLP systems. Correcting these errors manually is a time-consuming process and a great part of the automatic approaches have been relying on rules or supervised machine learning. We build on previous work on fully automatic unsupervised extraction of parallel data to train a character-based sequence-to-sequence NMT (neural machine translation) model to conduct OCR error correction designed for English and adapt it to Finnish by proposing solutions that take the rich morphology of the language into account. Our new method shows increased performance while remaining fully unsupervised with the added benefit of spelling normalisation. The source code and models are available on GitHub and Zenodo.","title":"An Unsupervised method for OCR Post-Correction and Spelling Normalisation for Finnish","url":"https:\\aclanthology.org\2021.nodalida-main.24"},{"year":"2021","abstract":"Named entity recognition is of high interest to digital humanities in particular when mining historical documents. Although the task is mature in the field of NLP results of contemporary models are not satisfactory on challenging documents corresponding to out-of-domain genres noisy OCR output or old-variants of the target language. In this paper we study how model transfer methods in the context of the aforementioned challenges can improve historical named entity recognition according to how much effort is allocated to describing the target data manually annotating small amounts of texts or matching pre-training resources. In particular we explore the situation where the class labels as well as the quality of the documents to be processed are different in the source and target domains. We perform extensive experiments with the transformer architecture on the LitBank and HIPE historical datasets with different annotation schemes and character-level noise. They show that annotating 250 sentences can recover 93% of the full-data performance when models are pre-trained that the choice of self-supervised and target-task pre-training data is crucial in the zero-shot setting and that OCR errors can be handled by simulating noise on pre-training data and resorting to recent character-aware transformers.","title":"Transferring Modern Named Entity Recognition to the Historical Domain: How to Take the Step?","url":"https:\\aclanthology.org\2021.nlp4dh-1.18"},{"year":"2021","abstract":"Older legal texts are often scanned and digitized via Optical Character Recognition (OCR) which results in numerous errors. Although spelling and grammar checkers can correct much of the scanned text automatically Named Entity Recognition (NER) is challenging making correction of names difficult. To solve this we developed an ensemble language model using a transformer neural network architecture combined with a finite state machine to extract names from English-language legal text. We use the US-based English language Harvard Caselaw Access Project for training and testing. Then the extracted names are subjected to heuristic textual analysis to identify errors make corrections and quantify the extent of problems. With this system we are able to extract most names automatically correct numerous errors and identify potential mistakes that can later be reviewed for manual correction.","title":"Named Entity Recognition in Historic Legal Text: A Transformer and State Machine Ensemble Method","url":"https:\\aclanthology.org\2021.nllp-1.18"},{"year":"2021","abstract":"We investigate video-aided grammar induction which learns a constituency parser from both unlabeled text and its corresponding video. Existing methods of multi-modal grammar induction focus on grammar induction from text-image pairs with promising results showing that the information from static images is useful in induction. However videos provide even richer information including not only static objects but also actions and state changes useful for inducing verb phrases. In this paper we explore rich features (e.g. action object scene audio face OCR and speech) from videos taking the recent Compound PCFG model as the baseline. We further propose a Multi-Modal Compound PCFG model (MMC-PCFG) to effectively aggregate these rich features from different modalities. Our proposed MMC-PCFG is trained end-to-end and outperforms each individual modality and previous state-of-the-art systems on three benchmarks i.e. DiDeMo YouCook2 and MSRVTT confirming the effectiveness of leveraging video information for unsupervised grammar induction.","title":"Video-aided Unsupervised Grammar Induction","url":"https:\\aclanthology.org\2021.naacl-main.119"},{"year":"2021","abstract":"The use of automatic methods for the study of lexical semantic change (LSC) has led to the creation of evaluation benchmarks. Benchmark datasets however are intimately tied to the corpus used for their creation questioning their reliability as well as the robustness of automatic methods. This contribution investigates these aspects showing the impact of unforeseen social and cultural dimensions. We also identify a set of additional issues (OCR quality named entities) that impact the performance of the automatic methods especially when used to discover LSC.","title":"The Corpora They Are a-Changing: a Case Study in Italian Newspapers","url":"https:\\aclanthology.org\2021.lchange-1.3"},{"year":"2021","abstract":"Substantial amounts of work are required to clean large collections of digitized books for NLP analysis both because of the presence of errors in the scanned text and the presence of duplicate volumes in the corpora. In this paper we consider the issue of deduplication in the presence of optical character recognition (OCR) errors. We present methods to handle these errors evaluated on a collection of 19347 texts from the Project Gutenberg dataset and 96635 texts from the HathiTrust Library. We demonstrate that improvements in language models now enable the detection and correction of OCR errors without consideration of the scanning image itself. The inconsistencies found by aligning pairs of scans of the same underlying work provides training data to build models for detecting and correcting errors. We identify the canonical version for each of 17136 repeatedly-scanned books from 58808 scans. Finally we investigate methods to detect and correct errors in single-copy texts. We show that on average our method corrects over six times as many errors as it introduces. We also provide interesting analysis on the relation between scanning quality and other factors such as location and publication year.","title":"Cleaning Dirty Books: Post-OCR Processing for Previously Scanned Texts","url":"https:\\aclanthology.org\2021.findings-emnlp.356"},{"year":"2021","abstract":"Chinese Spelling Check (CSC) is to detect and correct Chinese spelling errors. Many models utilize a predefined confusion set to learn a mapping between correct characters and its visually similar or phonetically similar misuses but the mapping may be out-of-domain. To that end we propose SpellBERT a pretrained model with graph-based extra features and independent on confusion set. To explicitly capture the two erroneous patterns we employ a graph neural network to introduce radical and pinyin information as visual and phonetic features. For better fusing these features with character representations we devise masked language model alike pre-training tasks. With this feature-rich pre-training SpellBERT with only half size of BERT can show competitive performance and make a state-of-the-art result on the OCR dataset where most of the errors are not covered by the existing confusion set.","title":"SpellBERT: A Lightweight Pretrained Model for Chinese Spelling Check","url":"https:\\aclanthology.org\2021.emnlp-main.287"},{"year":"2021","abstract":"Reading order detection is the cornerstone to understanding visually-rich documents (e.g. receipts and forms). Unfortunately no existing work took advantage of advanced deep learning models because it is too laborious to annotate a large enough dataset. We observe that the reading order of WORD documents is embedded in their XML metadata; meanwhile it is easy to convert WORD documents to PDFs or images. Therefore in an automated manner we construct ReadingBank a benchmark dataset that contains reading order text and layout information for 500000 document images covering a wide spectrum of document types. This first-ever large-scale dataset unleashes the power of deep neural networks for reading order detection. Specifically our proposed LayoutReader captures the text and layout information for reading order prediction using the seq2seq model. It performs almost perfectly in reading order detection and significantly improves both open-source and commercial OCR engines in ordering text lines in their results in our experiments. The dataset and models are publicly available at https:\\aka.ms\layoutreader.","title":"LayoutReader: Pre-training of Text and Layout for Reading Order Detection","url":"https:\\aclanthology.org\2021.emnlp-main.389"},{"year":"2021","abstract":"We investigate post-OCR correction in a setting where we have access to different OCR views of the same document. The goal of this study is to understand if a pretrained language model (LM) can be used in an unsupervised way to reconcile the different OCR views such that their combination contains fewer errors than each individual view. This approach is motivated by scenarios in which unconstrained text generation for error correction is too risky. We evaluated different pretrained LMs on two datasets and found significant gains in realistic scenarios with up to 15% WER improvement over the best OCR view. We also show the importance of domain adaptation for post-OCR correction on out-of-domain documents.","title":"Unsupervised Multi-View Post-OCR Error Correction With Language Models","url":"https:\\aclanthology.org\2021.emnlp-main.680"},{"year":"2021","abstract":"We consider the following tokenization repair problem: Given a natural language text with any combination of missing or spurious spaces correct these. Spelling errors can be present but it's not part of the problem to correct them. For example given: ``Tispa per isabout token izaionrep air'' compute ``Tis paper is about tokenizaion repair''. We identify three key ingredients of high-quality tokenization repair all missing from previous work: deep language models with a bidirectional component training the models on text with spelling errors and making use of the space information already present. Our methods also improve existing spell checkers by fixing not only more tokenization errors but also more spelling errors: once it is clear which characters form a word it is much easier for them to figure out the correct word. We provide six benchmarks that cover three use cases (OCR errors text extraction from PDF human errors) and the cases of partially correct space information and all spaces missing. We evaluate our methods against the best existing methods and a non-trivial baseline. We provide full reproducibility under https:\\ad.informatik.uni-freiburg.de\publications.","title":"Tokenization Repair in the Presence of Spelling Errors","url":"https:\\aclanthology.org\2021.conll-1.22"},{"year":"2021","abstract":"We describe a simple procedure for the automatic creation of word-level alignments between printed documents and their respective full-text versions. The procedure is unsupervised uses standard off-the-shelf components only and reaches an F-score of 85.01 in the basic setup and up to 86.63 when using pre- and post-processing. Potential areas of application are manual database curation (incl. document triage) and biomedical expression OCR.","title":"Word-Level Alignment of Paper Documents with their Electronic Full-Text Counterparts","url":"https:\\aclanthology.org\2021.bionlp-1.19"},{"year":"2021","abstract":"The Quechua linguistic family has a limited number of NLP resources most of them being dedicated to Southern Quechua whereas the varieties of Central Quechua have to the best of our knowledge no specific resources (software lexicon or corpus). Our work addresses this issue by producing two resources for the Ancash Quechua: a full digital version of a dictionary and an OCR model adapted to the considered variety. In this paper we describe the steps towards this goal: we first measure performances of existing models for the task of digitising a Quechua dictionary then adapt a model for the Ancash variety and finally create a reliable resource for NLP in XML-TEI format. We hope that this work will be a basis for initiating NLP projects for Central Quechua and that it will encourage digitisation initiatives for under-resourced languages.","title":"Toward Creation of Ancash Lexical Resources from OCR","url":"https:\\aclanthology.org\2021.americasnlp-1.18"},{"year":"2021","abstract":"Open pit mines left many regions worldwide inhospitable or uninhabitable. Many sites are left behind in a hazardous or contaminated state show remnants of waste or have other restrictions imposed upon them e.g. for the protection of human or nature. Such information has to be permanently managed in order to reuse those areas in the future. In this work we present and evaluate an automated workflow for supporting the post-mining management of former lignite open pit mines in the eastern part of Germany where prior to any planned land reuse aforementioned information has to be acquired to ensure the safety and validity of such an endeavor. Usually this information is found in expert reports either in the form of paper documents or in the best case as digitized unstructured text---all of them in German language. However due to the size and complexity of these documents any inquiry is tedious and time-consuming thereby slowing down or even obstructing the reuse of related areas. Since no training data is available we employ active learning in order to perform multi-label sentence classification for two categories of restrictions and seven categories of topics. The final system integrates optical character recognition (OCR) active-learning-based text classification and geographic information system visualization in order to effectively extract query and visualize this information for any area of interest. Active learning and text classification results are twofold: Whereas the restriction categories were reasonably accurate (textgreater0.85 F1) the seven topic-oriented categories seemed to be complex even for human annotators and achieved mediocre evaluation scores (textless0.70 F1).","title":"Supporting Land Reuse of Former Open Pit Mining Sites using Text Classification and Active Learning","url":"https:\\aclanthology.org\2021.acl-long.320"},{"year":"2020","abstract":"The preparation of parallel corpora is a challenging task particularly for languages that suffer from under-representation in the digital world. In a multi-lingual country like India the need for such parallel corpora is stringent for several low-resource languages. In this work we provide an extended English-Odia parallel corpus OdiEnCorp 2.0 aiming particularly at Neural Machine Translation (NMT) systems which will help translate English↔Odia. OdiEnCorp 2.0 includes existing English-Odia corpora and we extended the collection by several other methods of data acquisition: parallel data scraping from many websites including Odia Wikipedia but also optical character recognition (OCR) to extract parallel data from scanned images. Our OCR-based data extraction approach for building a parallel corpus is suitable for other low resource languages that lack in online content. The resulting OdiEnCorp 2.0 contains 98302 sentences and 1.69 million English and 1.47 million Odia tokens. To the best of our knowledge OdiEnCorp 2.0 is the largest Odia-English parallel corpus covering different domains and available freely for non-commercial and research purposes.","title":"OdiEnCorp 2.0: Odia-English Parallel Corpus for Machine Translation","url":"https:\\aclanthology.org\2020.wildre-1.3"},{"year":"2020","abstract":"Memotion analysis is a very crucial and important subject in today's world that is dominated by social media. This paper presents the results and analysis of the SemEval-2020 Task-8: Memotion analysis by team Kraken that qualified as winners for the task. This involved performing multimodal sentiment analysis on memes commonly posted over social media. The task comprised of 3 subtasks Task A was to find the overall sentiment of a meme and classify it into positive negative or neutral Task B was to classify it into the different types which were namely humour sarcasm offensive or motivation where a meme could have more than one category Task C was to further quantify the classifications achieved in task B. An imbalanced data of 6992 rows was utilized for this which contained images (memes) text (extracted OCR) and their annotations in 17 classes provided by the task organisers. In this paper the authors proposed a hybrid neural Naive-Bayes Support Vector Machine and logistic regression to solve a multilevel 17 class classification problem. It achieved the best result in Task B i.e 0.70 F1 score. The authors were ranked third in Task B.","title":"BennettNLP at SemEval-2020 Task 8: Multimodal sentiment classification Using Hybrid Hierarchical Classifier","url":"https:\\aclanthology.org\2020.semeval-1.143"},{"year":"2020","abstract":"A meme is a pictorial representation of an idea or theme. In the age of emerging volume of social media platforms memes are spreading rapidly from person to person and becoming a trending ways of opinion expression. However due to the multimodal characteristics of meme contents detecting and analyzing the underlying emotion of a meme is a formidable task. In this paper we present our approach for detecting the emotion of a meme defined in the SemEval-2020 Task 8. Our team CSECU_KDE_MA employs an attention-based neural network model to tackle the problem. Upon extracting the text contents from a meme using an optical character reader (OCR) we represent it using the distributed representation of words. Next we perform the convolution based on multiple kernel sizes to obtain the higher-level feature sequences. The feature sequences are then fed into the attentive time-distributed bidirectional LSTM model to learn the long-term dependencies effectively. Experimental results show that our proposed neural model obtained competitive performance among the participants' systems.","title":"CSECU_KDE_MA at SemEval-2020 Task 8: A Neural Attention Model for Memotion Analysis","url":"https:\\aclanthology.org\2020.semeval-1.146"},{"year":"2020","abstract":"Memes are steadily taking over the feeds of the public on social media. There is always the threat of malicious users on the internet posting offensive content even through memes. Hence the automatic detection of offensive images\memes is imperative along with detection of offensive text. However this is a much more complex task as it involves both visual cues as well as language understanding and cultural\context knowledge. This paper describes our approach to the task of SemEval-2020 Task 8: Memotion Analysis. We chose to participate only in Task A which dealt with Sentiment Classification which we formulated as a text classification problem. Through our experiments we explored multiple training models to evaluate the performance of simple text classification algorithms on the raw text obtained after running OCR on meme images. Our submitted model achieved an accuracy of 72.69% and exceeded the existing baseline's Macro F1 score by 8% on the official test dataset. Apart from describing our official submission we shall elucidate how different classification models respond to this task.","title":"SIS@IIITH at SemEval-2020 Task 8: An Overview of Simple Text Classification Methods for Meme Analysis","url":"https:\\aclanthology.org\2020.semeval-1.157"},{"year":"2020","abstract":"Optical character recognition (OCR) for historical documents is a complex procedure subject to a unique set of material issues including inconsistencies in typefaces and low quality scanning. Consequently even the most sophisticated OCR engines produce errors. This paper reports on a tool built for postediting the output of Tesseract more specifically for correcting common errors in digitized historical documents. The proposed tool suggests alternatives for word forms not found in a specified vocabulary. The assumed error is replaced by a presumably correct alternative in the post-edition based on the scores of a Language Model (LM). The tool is tested on a chapter of the book An Essay Towards Regulating the Trade and Employing the Poor of this Kingdom (Cary 1719). As demonstrated below the tool is successful in correcting a number of common errors. If sometimes unreliable it is also transparent and subject to human intervention.","title":"A Tool for Facilitating OCR Postediting in Historical Documents","url":"https:\\aclanthology.org\2020.lt4hala-1.7"},{"year":"2020","abstract":"The Book of Hours was the bestseller of the late Middle Ages and Renaissance. It is a historical invaluable treasure documenting the devotional practices of Christians in the late Middle Ages. Up to now its textual content has been scarcely studied because of its manuscript nature its length and its complex content. At first glance it looks too standardized. However the study of book of hours raises important challenges: (i) in image analysis its often lavish ornamentation (illegible painted initials line-fillers etc.) abbreviated words multilingualism are difficult to address in Handwritten Text Recognition (HTR); (ii) its hierarchical entangled structure offers a new field of investigation for text segmentation; (iii) in digital humanities its textual content gives opportunities for historical analysis. In this paper we provide the first corpus of books of hours which consists of Latin transcriptions of 300 books of hours generated by Handwritten Text Recognition (HTR) - that is like Optical Character Recognition (OCR) but for handwritten and not printed texts. We designed a structural scheme of the book of hours and annotated manually two books of hours according to this scheme. Lastly we performed a systematic evaluation of the main state of the art text segmentation approaches.","title":"Books of Hours. the First Liturgical Data Set for Text Segmentation.","url":"https:\\aclanthology.org\2020.lrec-1.97"},{"year":"2020","abstract":"The massive digitization efforts related to historical newspapers over the past decades have focused on mass media sources and ordinary people as their primary recipients. Much less attention has been paid to newspapers published for a more specialized audience e.g. those aiming at scholarly or cultural exchange within intellectual communities much narrower in scope such as newspapers devoted to music criticism arts or philosophy. Only some few of these specialized newspapers have been digitized up until now but they are usually not well curated in terms of digitization quality data formatting completeness redundancy (de-duplication) supply of metadata and hence searchability. This paper describes our approach to eliminate these drawbacks for a major German-language newspaper resource of the Romantic Age the Allgemeine Musikalische Zeitung (General Music Gazette). We here focus on a workflow that copes with a posteriori digitization problems inconsistent OCRing and index building for searchability. In addition we provide a user-friendly graphic interface to empower content-centric access to this (and other) digital resource(s) adopting open-source software for the purpose of Web presentation.","title":"Allgemeine Musikalische Zeitung as a Searchable Online Corpus","url":"https:\\aclanthology.org\2020.lrec-1.122"},{"year":"2020","abstract":"In this paper we propose a full pipeline of analysis of a large corpus about a century of public meeting in historical Australian news papers from construction to visual exploration. The corpus construction method is based on image processing and OCR. We digitize and transcribe texts of the specific topic of public meeting. Experiments show that our proposed method achieves a F-score of 87.8% for corpus construction. As a result we built a content search tool for temporal and semantic content analysis.","title":"Constructing a Public Meeting Corpus","url":"https:\\aclanthology.org\2020.lrec-1.238"},{"year":"2020","abstract":"Historical dictionaries of the pre-digital period are important resources for the study of older languages. Taking the example of the `Altfranzosisches Worterbuch' an Old French dictionary published from 1925 onwards this contribution shows how the printed dictionaries can be turned into a more easily accessible and more sustainable lexical database even though a full-text retro-conversion is too costly. Over 57000 German sense definitions were identified in uncorrected OCR output. For verbs and nouns 34000 senses of more than 20000 lemmas were matched with GermaNet a semantic network for German and in a second step linked to synsets of the English WordNet. These results are relevant for the automatic processing of Old French for the annotation and exploitation of Old French text corpora and for the philological study of Old French in general.","title":"Preserving Semantic Information from Old Dictionaries: Linking Senses of the `Altfranzosisches Worterbuch' to WordNet","url":"https:\\aclanthology.org\2020.lrec-1.374"},{"year":"2020","abstract":"Recent advances in Optical Character Recognition (OCR) and Handwritten Text Recognition (HTR) have led to more accurate textrecognition of historical documents. The Digital Humanities heavily profit from these developments but they still struggle whenchoosing from the plethora of OCR systems available on the one hand and when defining workflows for their projects on the other hand.In this work we present our approach to build a ground truth for a historical German-language newspaper published in black letter. Wealso report how we used it to systematically evaluate the performance of different OCR engines. Additionally we used this ground truthto make an informed estimate as to how much data is necessary to achieve high-quality OCR results. The outcomes of our experimentsshow that HTR architectures can successfully recognise black letter text and that a ground truth size of 50 newspaper pages suffices toachieve good OCR accuracy. Moreover our models perform equally well on data they have not seen during training which means thatadditional manual correction for diverging data is superfluous.","title":"How Much Data Do You Need? About the Creation of a Ground Truth for Black Letter and the Effectiveness of Neural OCR","url":"https:\\aclanthology.org\2020.lrec-1.436"},{"year":"2020","abstract":"Named entity recognition (NER) identifies spans of text that contain names. Many researchers have reported the results of NER on text created through optical character recognition (OCR) over the past two decades. Unfortunately the test collections that support this research are annotated with named entities after optical character recognition (OCR) has been run. This means that the collection must be re-annotated if the OCR output changes. Instead by tying annotations to character locations on the page a collection can be built that supports OCR and NER research without requiring re-annotation when either improves. This means that named entities are annotated on the transcribed text. The transcribed text is all that is needed to evaluate the performance of OCR. For NER evaluation the tagged OCR output is aligned to the transcriptions the aligned files creating modified files of each which are scored. This paper presents a methodology for building such a test collection and releases a collection of Chinese OCR-NER data constructed using the methodology. The paper provides performance baselines for current OCR and NER systems applied to this new collection.","title":"Building OCR\NER Test Collections","url":"https:\\aclanthology.org\2020.lrec-1.570"},{"year":"2020","abstract":"Word embeddings have proven to be an effective method for capturing semantic relations among distinct terms within a large corpus. In this paper we present a set of word embeddings learnt from three large Lebanese news archives which collectively consist of 609386 scanned newspaper images and spanning a total of 151 years ranging from 1933 till 2011. The diversified ideological nature of the news archives alongside the temporal variability of the embeddings offer a rare glimpse onto the variation of word representation across the left-right political spectrum. To train the word embeddings Google's Tesseract 4.0 OCR engine was employed to transcribe the scanned news archives and various archive-level as well as decade-level word embeddings were learnt. To evaluate the accuracy of the learnt word embeddings a benchmark of analogy tasks was used. Finally we demonstrate an interactive system that allows the end user to visualize for a given word of interest the variation of the top-k closest words in the embedding space as a function of time and across news archives using an animated scatter plot.","title":"Time-Aware Word Embeddings for Three Lebanese News Archives","url":"https:\\aclanthology.org\2020.lrec-1.580"},{"year":"2020","abstract":"The quality of Optical Character Recognition (OCR) is a key factor in the digitisation of historical documents. OCR errors are a major obstacle for downstream tasks and have hindered advances in the usage of the digitised documents. In this paper we present a two-step approach to automatic OCR post-correction. The first component is responsible for detecting erroneous sequences in a set of OCRed texts while the second is designed for correcting OCR errors in them. We show that applying the preceding detection model reduces both the character error rate (CER) compared to a simple one-step correction model and the amount of falsely changed correct characters.","title":"A Two-Step Approach for Automatic OCR Post-Correction","url":"https:\\aclanthology.org\2020.latechclfl-1.6"},{"year":"2020","abstract":"Pour comparer deux sorties de logiciels d'OCR le Character Error Rate (ou CER) est fr'equemment utilis'e. Moyennant l'existence d'une transcription de r'ef'erence de qualit'e pour certains documents du corpus le CER calcule le taux d'erreurs de ces pi`eces et permet ensuite de s'electionner le logiciel d'OCR le plus adapt'e. Toutefois ces transcriptions sont tr`es co^uteuses `a produire et peuvent freiner certaines 'etudes m^eme prospectives. Nous explorons l'exploitation des mod`eles de langue en agr'egeant selon diff'erentes m'ethodes les probabilit'es offertes par ceux-ci pour estimer la qualit'e d'une sortie d'OCR. L'indice de corr'elation Pearson est ici utilis'e pour comprendre dans quelle mesure ces estimations issues de mod`eles de langue co-varient avec le CER mesure de r'ef'erence.","title":"Exploiter des mod`eles de langue pour 'evaluer des sorties de logiciels d'OCR pour des documents franccais du XVIIe si`ecle ()","url":"https:\\aclanthology.org\2020.jeptalnrecital-recital.16"},{"year":"2020","abstract":"Form understanding depends on both textual contents and organizational structure. Although modern OCR performs well it is still challenging to realize general form understanding because forms are commonly used and of various formats. The table detection and handcrafted features in previous works cannot apply to all forms because of their requirements on formats. Therefore we concentrate on the most elementary components the key-value pairs and adopt multimodal methods to extract features. We consider the form structure as a tree-like or graph-like hierarchy of text fragments. The parent-child relation corresponds to the key-value pairs in forms. We utilize the state-of-the-art models and design targeted extraction modules to extract multimodal features from semantic contents layout information and visual images. A hybrid fusion method of concatenation and feature shifting is designed to fuse the heterogeneous features and provide an informative joint representation. We adopt an asymmetric algorithm and negative sampling in our model as well. We validate our method on two benchmarks MedForm and FUNSD and extensive experiments demonstrate the effectiveness of our method.","title":"DocStruct: A Multimodal Method to Extract Hierarchy Structure in Document for General Form Understanding","url":"https:\\aclanthology.org\2020.findings-emnlp.80"},{"year":"2020","abstract":"Local sequence transduction (LST) tasks are sequence transduction tasks where there exists massive overlapping between the source and target sequences such as grammatical error correction and spell or OCR correction. Motivated by this characteristic of LST tasks we propose Pseudo-Bidirectional Decoding (PBD) a simple but versatile approach for LST tasks. PBD copies the representation of source tokens to the decoder as pseudo future context that enables the decoder self-attention to attends to its bi-directional context. In addition the bidirectional decoding scheme and the characteristic of LST tasks motivate us to share the encoder and the decoder of LST models. Our approach provides right-side context information for the decoder reduces the number of parameters by half and provides good regularization effects. Experimental results on several benchmark datasets show that our approach consistently improves the performance of standard seq2seq models on LST tasks.","title":"Pseudo-Bidirectional Decoding for Local Sequence Transduction","url":"https:\\aclanthology.org\2020.findings-emnlp.136"},{"year":"2020","abstract":"There is little to no data available to build natural language processing models for most endangered languages. However textual data in these languages often exists in formats that are not machine-readable such as paper books and scanned images. In this work we address the task of extracting text from these resources. We create a benchmark dataset of transcriptions for scanned books in three critically endangered languages and present a systematic analysis of how general-purpose OCR tools are not robust to the data-scarce setting of endangered languages. We develop an OCR post-correction method tailored to ease training in this data-scarce setting reducing the recognition error rate by 34% on average across the three languages.","title":"OCR Post Correction for Endangered Language Texts","url":"https:\\aclanthology.org\2020.emnlp-main.478"},{"year":"2020","abstract":"The OCCAM project (Optical Character recognition ClassificAtion & Machine Translation) aims at integrating the CEF (Connecting Europe Facility) Automated Translation service with image classification Translation Memories (TMs) Optical Character Recognition (OCR) and Machine Translation (MT). It will support the automated translation of scanned business documents (a document format that currently cannot be processed by the CEF eTranslation service) and will also lead to a tool useful for the Digital Humanities domain.","title":"OCR Classification& Machine Translation (OCCAM)","url":"https:\\aclanthology.org\2020.eamt-1.62"},{"year":"2020","abstract":"This paper tackles the task of named entity recognition (NER) applied to digitized historical texts obtained from processing digital images of newspapers using optical character recognition (OCR) techniques. We argue that the main challenge for this task is that the OCR process leads to misspellings and linguistic errors in the output text. Moreover historical variations can be present in aged documents which can impact the performance of the NER process. We conduct a comparative evaluation on two historical datasets in German and French against previous state-of-the-art models and we propose a model based on a hierarchical stack of Transformers to approach the NER task for historical data. Our findings show that the proposed model clearly improves the results on both historical datasets and does not degrade the results for modern datasets.","title":"Alleviating Digitization Errors in Named Entity Recognition for Historical Documents","url":"https:\\aclanthology.org\2020.conll-1.35"},{"year":"2020","abstract":"Understanding image advertisements is a challenging task often requiring non-literal interpretation. We argue that standard image-based predictions are insufficient for symbolism prediction. Following the intuition that texts and images are complementary in advertising we introduce a multimodal ensemble of a state of the art image-based classifier a classifier based on an object detection architecture and a fine-tuned language model applied to texts extracted from ads by OCR. The resulting system establishes a new state of the art in symbolism prediction.","title":"Ad Lingua: Text Classification Improves Symbolism Prediction in Image Advertisements","url":"https:\\aclanthology.org\2020.coling-main.171"},{"year":"2020","abstract":"Image text carries essential information to understand the scene and perform reasoning. Text-based visual question answering (text VQA) task focuses on visual questions that require reading text in images. Existing text VQA systems generate an answer by selecting from optical character recognition (OCR) texts or a fixed vocabulary. Positional information of text is underused and there is a lack of evidence for the generated answer. As such this paper proposes a localization-aware answer prediction network (LaAP-Net) to address this challenge. Our LaAP-Net not only generates the answer to the question but also predicts a bounding box as evidence of the generated answer. Moreover a context-enriched OCR representation (COR) for multimodal fusion is proposed to facilitate the localization task. Our proposed LaAP-Net outperforms existing approaches on three benchmark datasets for the text VQA task by a noticeable margin.","title":"Finding the Evidence: Localization-aware Answer Prediction for Text Visual Question Answering","url":"https:\\aclanthology.org\2020.coling-main.278"},{"year":"2020","abstract":"Sequence labeling systems should perform reliably not only under ideal conditions but also with corrupted inputs---as these systems often process user-generated text or follow an error-prone upstream component. To this end we formulate the noisy sequence labeling problem where the input may undergo an unknown noising process and propose two Noise-Aware Training (NAT) objectives that improve robustness of sequence labeling performed on perturbed input: Our data augmentation method trains a neural model using a mixture of clean and noisy samples whereas our stability training algorithm encourages the model to create a noise-invariant latent representation. We employ a vanilla noise model at training time. For evaluation we use both the original data and its variants perturbed with real OCR errors and misspellings. Extensive experiments on English and German named entity recognition benchmarks confirmed that NAT consistently improved robustness of popular sequence labeling models preserving accuracy on the original input. We make our code and data publicly available for the research community.","title":"NAT: Noise-Aware Training for Robust Neural Sequence Labeling","url":"https:\\aclanthology.org\2020.acl-main.138"},{"year":"2020","abstract":"This research paper reports on the generation of the first Drenjongke corpus based on texts taken from a phrase book for beginners written in the Tibetan script. A corpus of sentences was created after correcting errors in the text scanned through optical character reading (OCR). A total of 34 Part-of-Speech (PoS) tags were defined based on manual annotation performed by the three authors one of whom is a native speaker of Drenjongke. The first corpus of the Drenjongke language comprises 275 sentences and 1379 tokens which we plan to expand with other materials to promote further studies of this language.","title":"Building a Part-of-Speech Tagged Corpus for Drenjongke (Bhutia)","url":"https:\\aclanthology.org\2020.aacl-srw.9"},{"year":"2019","abstract":"We present initial experiments to evaluate the performance of tasks such as Part of Speech Tagging on data corrupted by Optical Character Recognition (OCR). Our results based on English and German data using artificial experiments as well as initial real OCRed data indicate that already a small drop in OCR quality considerably increases the error rates which would have a significant impact on subsequent processing steps.","title":"OCR Quality and NLP Preprocessing","url":"https:\\aclanthology.org\W19-3633"},{"year":"2019","abstract":"This article focuses on the problem of identifying articles and recovering their text from within and across newspaper pages when OCR just delivers one text file per page. We frame the task as a segmentation plus clustering step. Our results on a sample of 1912 New York Tribune magazine shows that performing the clustering based on similarities computed with word embeddings outperforms a similarity measure based on character n-grams and words. Furthermore the automatic segmentation based on the text results in low scores due to the low quality of some OCRed documents.","title":"Clustering-Based Article Identification in Historical Newspapers","url":"https:\\aclanthology.org\W19-2502"},{"year":"2019","abstract":"A great deal of historical corpora suffer from errors introduced by the OCR (optical character recognition) methods used in the digitization process. Correcting these errors manually is a time-consuming process and a great part of the automatic approaches have been relying on rules or supervised machine learning. We present a fully automatic unsupervised way of extracting parallel data for training a character-based sequence-to-sequence NMT (neural machine translation) model to conduct OCR error correction.","title":"From the Paft to the Fiiture: a Fully Automatic NMT and Word Embeddings Method for OCR Post-Correction","url":"https:\\aclanthology.org\R19-1051"},{"year":"2019","abstract":"The Specialized Information Service Biodiversity Research (BIOfid) has been launched to mobilize valuable biological data from printed literature hidden in German libraries for over the past 250 years. In this project we annotate German texts converted by OCR from historical scientific literature on the biodiversity of plants birds moths and butterflies. Our work enables the automatic extraction of biological information previously buried in the mass of papers and volumes. For this purpose we generated training data for the tasks of Named Entity Recognition (NER) and Taxa Recognition (TR) in biological documents. We use this data to train a number of leading machine learning tools and create a gold standard for TR in biodiversity literature. More specifically we perform a practical analysis of our newly generated BIOfid dataset through various downstream-task evaluations and establish a new state of the art for TR with 80.23% F-score. In this sense our paper lays the foundations for future work in the field of information extraction in biology texts.","title":"BIOfid Dataset: Publishing a German Gold Standard for Named Entity Recognition in Historical Biodiversity Literature","url":"https:\\aclanthology.org\K19-1081"},{"year":"2019","abstract":"We present a Parallel Iterative Edit (PIE) model for the problem of local sequence transduction arising in tasks like Grammatical error correction (GEC). Recent approaches are based on the popular encoder-decoder (ED) model for sequence to sequence learning. The ED model auto-regressively captures full dependency among output tokens but is slow due to sequential decoding. The PIE model does parallel decoding giving up the advantage of modeling full dependency in the output yet it achieves accuracy competitive with the ED model for four reasons: 1. predicting edits instead of tokens 2. labeling sequences instead of generating sequences 3. iteratively refining predictions to capture dependencies and 4. factorizing logits over edits and their token argument to harness pre-trained language models like BERT. Experiments on tasks spanning GEC OCR correction and spell correction demonstrate that the PIE model is an accurate and significantly faster alternative for local sequence transduction.","title":"Parallel Iterative Edit Models for Local Sequence Transduction","url":"https:\\aclanthology.org\D19-1435"}]