<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>ACL Search Engine</title>
    <link rel="stylesheet" href="src/main/resources/static/bootstrap.min.css">
</head>
<body>
<div class="container-fluid">
    <h3>Search Result</h3>
    <p>Total Matching Results: 1889</p>
    <ol class="list-group list-group-numbered">
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a very simple method for extending pretrained machine translation metrics to incorporate document-level context. We apply our method to four popular metrics: BERTScore Prism COMET and the reference-free metric COMET-QE. We evaluate our document-level metrics on the MQM annotations from the WMT 2021 metrics shared task and find that the document-level metrics outperform their sentence-level counterparts in about 85% of the tested conditions when excluding results on low-quality human references. Additionally we show that our document-level extension of COMET-QE dramatically improves accuracy on discourse phenomena tasks supporting our hypothesis that our document-level metrics are resolving ambiguities in the reference sentence by using additional context.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Embarrassingly Easy Document-Level MT Metrics: How to Convert Any Pretrained Metric into a Document-Level Metric</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.wmt-1.6" target="_blank">https://aclanthology.org/2022.wmt-1.6</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Topic models represent groups of documents as a list of words (the topic labels). This work asks whether an alternative approach to topic labeling can be developed that is closer to a natural language description of a topic than a word list. To this end we present an approach to generating human-like topic labels using abstractive multi-document summarization (MDS). We investigate our approach with an exploratory case study. We model topics in citation sentences in order to understand what further research needs to be done to fully operationalize MDS for topic labeling. Our case study shows that in addition to more human-like topics there are additional advantages to evaluation by using clustering and summarization measures instead of topic model measures. However we find that there are several developments needed before we can design a well-powered study to evaluate MDS for topic modeling fully. Namely improving cluster cohesion improving the factuality and faithfulness of MDS and increasing the number of documents that might be supported by MDS. We present a number of ideas on how these can be tackled and conclude with some thoughts on how topic modeling can also be used to improve MDS in general.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Moving beyond word lists: towards abstractive topic labels for human-like topics of scientific documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.wiesp-1.11" target="_blank">https://aclanthology.org/2022.wiesp-1.11</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present our submission to the structured document translation task organized by WAT 2022. In structured document translation the key challenge is the handling of inline tags which annotate text. Specifically the text that is annotated by tags should be translated in such a way that in the translation should contain the tags annotating the translation. This challenge is further compounded by the lack of training data containing sentence pairs with inline XML tag annotated content. However to our surprise we find that existing multilingual NMT systems are able to handle the translation of text annotated with XML tags without any explicit training on data containing said tags. Specifically massively multilingual translation models like M2M-100 perform well despite not being explicitly trained to handle structured content. This direct translation approach is often either as good as if not better than the traditional approach of ``remove tag translate and re-inject tag&#39;&#39; also known as the ``detag-and-project&#39;&#39; approach.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>NICT&#39;s Submission to the WAT 2022 Structured Document Translation Task</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.wat-1.6" target="_blank">https://aclanthology.org/2022.wat-1.6</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents a new task of predicting the coverage of a text document for relation extraction (RE): Does the document contain many relational tuples for a given entity? Coverage predictions are useful in selecting the best documents for knowledge base construction with large input corpora. To study this problem we present a dataset of 31366 diverse documents for 520 entities. We analyze the correlation of document coverage with features like length entity mention frequency Alexa rank language complexity and information retrieval scores. Each of these features has only moderate predictive power. We employ methods combining features with statistical models like TF-IDF and language models like BERT. The model combining features and BERT HERB achieves an F1 score of up to 46%. We demonstrate the utility of coverage predictions on two use cases: KB construction and claim refutation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Predicting Document Coverage for Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.tacl-1.12" target="_blank">https://aclanthology.org/2022.tacl-1.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The availability of large-scale datasets has driven the development of neural models that create generic summaries for single or multiple documents. For query-focused summarization (QFS) labeled training data in the form of queries documents and summaries is not readily available. We provide a unified modeling framework for any kind of summarization under the assumption that all summaries are a response to a query which is observed in the case of QFS and latent in the case of generic summarization. We model queries as discrete latent variables over document tokens and learn representations compatible with observed and unobserved query verbalizations. Our framework formulates summarization as a generative process and jointly optimizes a latent query model and a conditional language model. Despite learning from generic summarization data only our approach outperforms strong comparison systems across benchmarks query types document settings and target domains.1</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Summarization with Latent Queries</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.tacl-1.36" target="_blank">https://aclanthology.org/2022.tacl-1.36</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Neural language models have seen a dramatic increase in size in the last years. While many still advocate that `bigger is better&#39; work in model distillation has shown that the number of parameters used by very large networks is actually more than what is required for state-of-the-art performance. This prompts an obvious question: can we build smaller models from scratch rather than going through the inefficient process of training at scale and subsequently reducing model size. In this paper we investigate the behaviour of a biologically inspired algorithm based on the fruit fly&#39;s olfactory system. This algorithm has shown good performance in the past on the task of learning word embeddings. We now put it to the test on the task of semantic hashing. Specifically we compare the fruit fly to a standard binary network on the task of generating locality-sensitive hashes for text documents measuring both task performance and energy consumption. Our results indicate that the two algorithms have complementary strengths while showing similar electricity usage.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Algorithmic Diversity and Tiny Models: Comparing Binary Networks and the Fruit Fly Algorithm on Document Representation Tasks</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sustainlp-1.4" target="_blank">https://aclanthology.org/2022.sustainlp-1.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document retrieval is a core component of many knowledge-intensive natural language processing task formulations such as fact verification. Sources of textual knowledge such as Wikipedia articles condition the generation of answers from the models. Recent advances in retrieval use sequence-to-sequence models to incrementally predict the title of the appropriate Wikipedia page given an input instance. However this method requires supervision in the form of human annotation to label which Wikipedia pages contain appropriate context.This paper introduces a distant-supervision method that does not require any annotation train auto-regressive retrievers that attain competitive R-Precision and Recall in a zero-shot setting.Furthermore we show that with task-specific supervised fine-tuning auto-regressive retrieval performance for two Wikipedia-based fact verification tasks can approach or even exceed full supervision using less than 1/4 of the annotated data. We release all code and models</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Data-Efficient Auto-Regressive Document Retrieval for Fact Verification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sustainlp-1.7" target="_blank">https://aclanthology.org/2022.sustainlp-1.7</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The standard approach for inducing narrative chains considers statistics gathered per individual document. We consider whether statistics gathered using cross-document relations can lead to improved chain induction. Our study is motivated by legal narratives where cases typically cite thematically similar cases. We consider four novel variations on pointwise mutual information (PMI) each accounting for cross-document relations in a different way. One proposed PMI variation performs 58% better relative to standard PMI on recall@50 and induces qualitatively better narrative chains.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improved Induction of Narrative Chains via Cross-Document Relations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.starsem-1.18" target="_blank">https://aclanthology.org/2022.starsem-1.18</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Granular events instantiated in a document by predicates can usually be grouped into more general events called complex events. Together they capture the major content of the document. Recent work grouped granular events by defining event regions filtering out sentences that are irrelevant to the main content. However this approach assumes that a given complex event is always described in consecutive sentences which does not always hold in practice. In this paper we introduce the task of complex event identification. We address this task as a pipeline first predicting whether two granular events mentioned in the text belong to the same complex event independently of their position in the text and then using this to cluster them into complex events. Due to the difficulty of predicting whether two granular events belong to the same complex event in isolation we propose a context-augmented representation learning approach CONTEXTRL that adds additional context to better model the pairwise relation between granular events. We show that our approach outperforms strong baselines on the complex event identification task and further present a promising case study exploring the effectiveness of using complex events as input for document-level argument extraction.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Capturing the Content of a Document through Complex Event Identification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.starsem-1.29" target="_blank">https://aclanthology.org/2022.starsem-1.29</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes our approaches used to solve the SocialDisNER task which belongs to the Social Media Mining for Health Applications (SMM4H) shared task. This task aims to identify disease mentions in tweets written in Spanish. The proposed model is an architecture based on the FLERT approach. It consists of fine-tuning a language model that creates an input representation of a sentence based on its neighboring sentences thus obtaining the document-level context. The best result was obtained using an ensemble of six language models using the FLERT approach. The system achieved an F1 score of 0.862 significantly surpassing the average performance among competitor models of 0.680 on the test partition.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>PLN CMM at SocialDisNER: Improving Detection of Disease Mentions in Tweets by Using Document-Level Features</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.smm4h-1.15" target="_blank">https://aclanthology.org/2022.smm4h-1.15</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We describe a sign language documentation project funded by the Endangered Languages Documentation Project (ELDP) in the province of Kermanshah a city in west of Iran. The deposit at ELDP archive (elararchive.org) includes recording of 38 native signers of Zaban Eshareh Irani living in Kermanshah. The recordings start with an elicitation of the signs of the Farsi alphabet along with fingerspelling of some words as well as vocabulary elicitation of some basic concepts. Subsequently the participants are asked to watch short movies and then they are asked to retell the story. Later the participants have natural conversations in pairs guided by a deaf moderator. Initial annotations of ID-glosses and translations to Persian and English were also archived. ID-glosses are stored as a dataset in Global Signbank along with a citation form of signs and their phonological description. The resulting datasets and one-hour annotation of the conversations are available to other researchers in ELDP archive.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Documenting the Use of Iranian Sign Language (ZEI) in Kermanshah</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.signlang-1.6" target="_blank">https://aclanthology.org/2022.signlang-1.6</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Libras Portal is an interface that makes available in one single site a series of elements and tools related to the Brazilian Sign Language (Libras) and comprises Libras documentation which may be employed for research and for educational aims. Libras Portal was developed to codify tools that prop an education network and practice community making possible the sharing of knowledge data and interaction in Libras and Portuguese. It involves accessibility and usability of the web especially videos in Libras. The latter are access-friendly to available hyperlinks and tools related to communication with the target practice community. The layout also employs visual and textual resources for deaf users. The portal makes available resources for research and the teaching of language namely Libras Grammar Libras corpus Sign Bank and Literary Anthology of Libras. It is also a store for the sharing of literary academic and didactic materials courses glossaries anthologies lesson models and grammar analyses. Consequently tools were developed for the accessibility of deaf people for easy web browsing index information video upload research and development of products for communities of deaf people. The current paper will describe the development of research and resources for accessibility.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Libras Portal: A Way of Documentation a Way of Sharing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.signlang-1.8" target="_blank">https://aclanthology.org/2022.signlang-1.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Collecting data for training dialog systems can be extremely expensive due to the involvement of human participants and the need for extensive annotation. Especially in document-grounded dialog systems human experts need to carefully read the unstructured documents to answer the users&#39; questions. As a result existing document-grounded dialog datasets are relatively small-scale and obstruct the effective training of dialogue systems. In this paper we propose an automatic data augmentation technique grounded on documents through a generative dialogue model. The dialogue model consists of a user bot and agent bot that can synthesize diverse dialogues given an input document which is then used to train a downstream model. When supplementing the original dataset our method achieves significant improvement over traditional data augmentation methods. We also achieve great performance in the low-resource setting.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DG2: Data Augmentation Through Document Grounded Dialogue Generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sigdial-1.21" target="_blank">https://aclanthology.org/2022.sigdial-1.21</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>To build a goal-oriented dialogue system that can generate responses given a knowledge base identifying the relevant pieces of information to be grounded in is vital. When the number of documents in the knowledge base is large retrieval approaches are typically used to identify the top relevant documents. However most prior work simply uses an entire dialogue history to guide retrieval rather than exploiting a dialogue&#39;s topical structure. In this work we examine the importance of building the proper contextualized dialogue history when document-level topic shifts are present. Our results suggest that excluding irrelevant turns from the dialogue history (e.g. excluding turns not grounded in the same document as the current turn) leads to better retrieval results. We also propose a cascading approach utilizing the topical nature of a knowledge-grounded conversation to further manipulate the dialogue history used as input to the retrieval models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Getting Better Dialogue Context for Knowledge Identification by Leveraging Document-level Topic Shift</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sigdial-1.36" target="_blank">https://aclanthology.org/2022.sigdial-1.36</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents our system for document-level semantic textual similarity (STS) evaluation at SemEval-2022 Task 8: ``Multilingual News Article Similarity&#39;&#39;. The semantic information used is obtained by using different semantic models ranging from the extraction of key terms and named entities to the document classification and obtaining similarity from automatic summarization of documents. All these semantic information&#39;s are then used as features to feed a supervised system in order to evaluate the degree of similarity of a pair of documents. We obtained a Pearson correlation score of 0.706 compared to the best score of 0.818 from teams that participated in this task.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>BL.Research at SemEval-2022 Task 8: Using various Semantic Information to evaluate document-level Semantic Textual Similarity</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.semeval-1.173" target="_blank">https://aclanthology.org/2022.semeval-1.173</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Proceedings of the Third Workshop on Scholarly Document Processing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sdp-1.0" target="_blank">https://aclanthology.org/2022.sdp-1.0</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>With the ever-increasing pace of research and high volume of scholarly communication scholars face a daunting task. Not only must they keep up with the growing literature in their own and related fields scholars increasingly also need to rebut pseudo-science and disinformation. These needs have motivated an increasing focus on computational methods for enhancing search summarization and analysis of scholarly documents. However the various strands of research on scholarly document processing remain fragmented. To reach out to the broader NLP and AI/ML community pool distributed efforts in this area and enable shared access to published research we held the 3rd Workshop on Scholarly Document Processing (SDP) at COLING as a hybrid event (https://sdproc.org/2022/). The SDP workshop consisted of a research track three invited talks and five Shared Tasks: 1) MSLR22: Multi-Document Summarization for Literature Reviews 2) DAGPap22: Detecting automatically generated scientific papers 3) SV-Ident 2022: Survey Variable Identification in Social Science Publications 4) SKGG: Scholarly Knowledge Graph Generation 5) MuP 2022: Multi Perspective Scientific Document Summarization. The program was geared towards NLP information retrieval and data mining for scholarly documents with an emphasis on identifying and providing solutions to open challenges.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Overview of the Third Workshop on Scholarly Document Processing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sdp-1.1" target="_blank">https://aclanthology.org/2022.sdp-1.1</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Existing dense retrieval models for scientific documents have been optimized for either retrieval by short queries or for document similarity but usually not for both. In this paper we explore the space of combining multiple objectives to achieve a single representation model that presents a good balance between both modes of dense retrieval combining the relevance judgements from MS MARCO with the citation similarity of SPECTER and the self-supervised objective of independent cropping. We also consider the addition of training data from document co-citation in a sentence context and domain-specific synthetic data. We show that combining multiple objectives yields models that generalize well across different benchmark tasks improving up to 73% over models trained on a single objective.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-objective Representation Learning for Scientific Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sdp-1.9" target="_blank">https://aclanthology.org/2022.sdp-1.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Automatically organizing scholarly literature is a necessary and challenging task. By assigning scientific research publications key concepts researchers policymakers and the general public are able to search for and discover relevant research literature. The organization of scientific research evolves with new discoveries and publications requiring an up-to-date and scalable text classification model. Additionally scientific research publications benefit from multi-label classification particularly with more fine-grained sub-domains. Prior work has focused on classifying scientific publications from one research area (e.g. computer science) referencing static concept descriptions and implementing an English-only classification model. We propose a multi-label classification model that can be implemented in non-English languages across all of scientific literature with updatable concept descriptions.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-label Classification of Scientific Research Documents Across Domains and Languages</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sdp-1.12" target="_blank">https://aclanthology.org/2022.sdp-1.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Long document summarisation a challenging summarisation scenario is the focus of the recently proposed LongSumm shared task. One of the limitations of this shared task has been its use of a single family of metrics for evaluation (the ROUGE metrics). In contrast other fields like text generation employ multiple metrics. We replicated the LongSumm evaluation using multiple test set samples (vs. the single test set of the official shared task) and investigated how different metrics might complement each other in this evaluation framework. We show that under this more rigorous evaluation (1) some of the key learnings from Longsumm 2020 and 2021 still hold but the relative ranking of systems changes and (2) the use of additional metrics reveals additional high-quality summaries missed by ROUGE and (3) we show that SPICE is a candidate metric for summarisation evaluation for LongSumm.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Investigating Metric Diversity for Evaluating Long Document Summarisation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sdp-1.13" target="_blank">https://aclanthology.org/2022.sdp-1.13</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We provide an overview of the MSLR2022 shared task on multi-document summarization for literature reviews. The shared task was hosted at the Third Scholarly Document Processing (SDP) Workshop at COLING 2022. For this task we provided data consisting of gold summaries extracted from review papers along with the groups of input abstracts that were synthesized into these summaries split into two subtasks. In total six teams participated making 10 public submissions 6 to the Cochrane subtask and 4 to the MS^2 subtask. The top scoring systems reported over 2 points ROUGE-L improvement on the Cochrane subtask though performance improvements are not consistently reported across all automated evaluation metrics; qualitative examination of the results also suggests the inadequacy of current evaluation metrics for capturing factuality and consistency on this task. Significant work is needed to improve system performance and more importantly to develop better methods for automatically evaluating performance on this task.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Overview of MSLR2022: A Shared Task on Multi-document Summarization for Literature Reviews</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sdp-1.20" target="_blank">https://aclanthology.org/2022.sdp-1.20</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we report the experiments performed for the submission to the Multidocument summarisation for Literature Review (MSLR) Shared Task. In particular we adopt Primera model to the biomedical domain by placing global attention on important biomedical entities in several ways. We analyse the outputs of 23 resulting models and report some patterns related to the presence of additional global attention number of training steps and the inputs configuration.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>LED down the rabbit hole: exploring the potential of global attention for biomedical multi-document summarisation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sdp-1.21" target="_blank">https://aclanthology.org/2022.sdp-1.21</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Systematic literature reviews in the biomedical space are often expensive to conduct. Automation through machine learning and large language models could improve the accuracy and research outcomes from such reviews. In this study we evaluate a pre-trained LongT5 model on the MSLR22: Multi-Document Summarization for Literature Reviews Shared Task datasets. We weren&#39;t able to make any improvements on the dataset benchmark but we do establish some evidence that current summarization metrics are insufficient in measuring summarization accuracy. A multi-document summarization web tool was also built to demonstrate the viability of summarization models for future investigators: https://ben-yu.github.io/summarizer</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Evaluating Pre-Trained Language Models on Multi-Document Summarization for Literature Reviews</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sdp-1.22" target="_blank">https://aclanthology.org/2022.sdp-1.22</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper is a description of our participation in the Multi-document Summarization for Literature Review (MSLR) Shared Task in which we explore summarization models to create an automatic review of scientific results. Rather than maximizing the metrics using expensive computational models we placed ourselves in a situation of scarce computational resources and explore the limits of a base sequence to sequence models (thus with a limited input length) to the task. Although we explore methods to feed the abstractive model with salient sentences only (using a first extractive step) we find the results still need some improvements.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploring the limits of a base BART for multi-document summarization in the medical domain</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sdp-1.23" target="_blank">https://aclanthology.org/2022.sdp-1.23</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Text summarization has been a trending domain of research in NLP in the past few decades. The medical domain is no exception to the same. Medical documents often contain a lot of jargon pertaining to certain domains and performing an abstractive summarization on the same remains a challenge. This paper presents a summary of the findings that we obtained based on the shared task of Multidocument Summarization for Literature Review (MSLR). We stood fourth in the leaderboards for evaluation on the MS^2 and Cochrane datasets. We finetuned pre-trained models such as BART-large DistilBART and T5-base on both these datasets. These models&#39; accuracy was later tested with a part of the same dataset using ROUGE scores as the evaluation metrics.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Abstractive Approaches To Multidocument Summarization Of Medical Literature Reviews</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sdp-1.24" target="_blank">https://aclanthology.org/2022.sdp-1.24</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Research in the biomedical domain is con- stantly challenged by its large amount of ever- evolving textual information. Biomedical re- searchers are usually required to conduct a lit- erature review before any medical interven- tion to assess the effectiveness of the con- cerned research. However the process is time- consuming and therefore automation to some extent would help reduce the accompanying information overload. Multi-document sum- marization of scientific articles for literature reviews is one approximation of such automa- tion. Here in this paper we describe our pipelined approach for the aforementioned task. We design a BERT-based extractive method followed by a BigBird PEGASUS-based ab- stractive pipeline for generating literature re- view summaries from the abstracts of biomedi- cal trial reports as part of the Multi-document Summarization for Literature Review (MSLR) shared task1 in the Scholarly Document Pro- cessing (SDP) workshop 20222. Our proposed model achieves the best performance on the MSLR-Cochrane leaderboard3 on majority of the evaluation metrics. Human scrutiny of our automatically generated summaries indicates that our approach is promising to yield readable multi-article summaries for conducting such lit- erature reviews.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Extractive-Abstractive Approach for Multi-document Summarization of Scientific Articles for Literature Review</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sdp-1.25" target="_blank">https://aclanthology.org/2022.sdp-1.25</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a new gold-standard dataset and a benchmark for the Research Theme Identification task a sub-task of the Scholarly Knowledge Graph Generation shared task at the 3rd Workshop on Scholarly Document Processing. The objective of the shared task was to label given research papers with research themes from a total of 36 themes. The benchmark was compiled using data drawn from the largest overall assessment of university research output ever undertaken globally (the Research Excellence Framework - 2014). We provide a performance comparison of a transformer-based ensemble which obtains multiple predictions for a research paper given its multiple textual fields (e.g. title abstract reference) with traditional machine learning models. The ensemble involves enriching the initial data with additional information from open-access digital libraries and Argumentative Zoning techniques (CITATION). It uses a weighted sum aggregation for the multiple predictions to obtain a final single prediction for the given research paper. Both data and the ensemble are publicly available on https://www.kaggle.com/competitions/sdp2022-scholarly-knowledge-graph-generation/data?select=task1_test_no_label.csv and https://github.com/ProjectDoSSIER/sdp2022 respectively.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Benchmark for Research Theme Classification of Scholarly Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sdp-1.31" target="_blank">https://aclanthology.org/2022.sdp-1.31</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present the main findings of MuP 2022 shared task the first shared task on multi-perspective scientific document summarization. The task provides a testbed representing challenges for summarization of scientific documents and facilitates development of better models to leverage summaries generated from multiple perspectives. We received 139 total submissions from 9 teams. We evaluated submissions both by automated metrics (i.e. Rouge) and human judgments on faithfulness coverage and readability which provided a more nuanced view of the differences between the systems. While we observe encouraging results from the participating teams we conclude that there is still significant room left for improving summarization leveraging multiple references. Our dataset is available at https://github.com/allenai/mup.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Overview of the First Shared Task on Multi Perspective Scientific Document Summarization (MuP)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sdp-1.32" target="_blank">https://aclanthology.org/2022.sdp-1.32</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>It is well recognized that creating summaries of scientific texts can be difficult. For each given document the majority of summarizing research believes there is only one best gold summary. Having just one gold summary limits our capacity to assess the effectiveness of summarizing algorithms because creating summaries is an art. Likewise because it takes subject-matter experts a lot of time to read and comprehend lengthy scientific publications annotating several gold summaries for scientific documents can be very expensive. The shared task known as the Multi perspective Scientific Document Summarization (Mup) is an exploration of various methods to produce multi perspective scientific summaries. Utilizing Graph Attention Networks (GATs) we take an extractive text summarization approach to the issue as a kind of sentence ranking task. Although the results produced by the suggested model are not particularly impressive comparing them to the state-of-the-arts demonstrates the model&#39;s potential for improvement.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi Perspective Scientific Document Summarization With Graph Attention Networks (GATS)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sdp-1.33" target="_blank">https://aclanthology.org/2022.sdp-1.33</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents our approach for the MuP 2022 shared task ----Multi-Perspective Scientific Document Summarization where the objective is to enable summarization models to explore methods for generating multi-perspective summaries for scientific papers. We explore two orthogonal ways to cope with this task. The first approach involves incorporating a neural topic model (i.e. NTM) into the state-of-the-art abstractive summarizer (LED); the second approach involves adding a two-step summarizer that extracts the salient sentences from the document and then writes abstractive summaries from those sentences. Our latter model outperformed our other submissions on the official test set. Specifically among 10 participants (including organizers&#39; baseline) who made their results public with 163 total runs. Our best system ranks first in Rouge-1 (F) and second in Rouge-1 (R) Rouge-2 (F) and Average Rouge (F) scores.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>GUIR @ MuP 2022: Towards Generating Topic-aware Multi-perspective Summaries for Scientific Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sdp-1.34" target="_blank">https://aclanthology.org/2022.sdp-1.34</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The MuP-2022 shared task focuses on multiperspective scientific document summarization. Given a scientific document with multiple reference summaries our goal was to develop a model that can produce a generic summary covering as many aspects of the document as covered by all of its reference summaries. This paper describes our best official model a finetuned BART-large along with a discussion on the challenges of this task and some of our unofficial models including SOTA generation models. Our submitted model out performedthe given MuP 2022 shared task baselines on ROUGE-2 ROUGE-L and average ROUGE F1-scores. Code of our submission can be ac- cessed here.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>LTRC @MuP 2022: Multi-Perspective Scientific Document Summarization Using Pre-trained Generation Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sdp-1.35" target="_blank">https://aclanthology.org/2022.sdp-1.35</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper introduces the proposed summarization system of the AINLPML team for the First Shared Task on Multi-Perspective Scientific Document Summarization at SDP 2022. We present a method to produce abstractive summaries of scientific documents. First we perform an extractive summarization step to identify the essential part of the paper. The extraction step includes utilizing a contributing sentence identification model to determine the contributing sentences in selected sections and portions of the text. In the next step the extracted relevant information is used to condition the transformer language model to generate an abstractive summary. In particular we fine-tuned the pre-trained BART model on the extracted summary from the previous step. Our proposed model successfully outperformed the baseline provided by the organizers by a significant margin. Our approach achieves the best average Rouge F1 Score Rouge-2 F1 Score and Rouge-L F1 Score among all submissions.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Team AINLPML @ MuP in SDP 2021: Scientific Document Summarization by End-to-End Extractive and Abstractive Approach</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.sdp-1.36" target="_blank">https://aclanthology.org/2022.sdp-1.36</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Unsupervised extractive summarization has recently gained importance since it does not require labeled data. Among unsupervised methods graph-based approaches have achieved outstanding results. These methods represent each document by a graph with sentences as nodes and word-level similarity among sentences as edges. Common words can easily lead to a strong connection between sentence nodes. Thus sentences with many common words can be misinterpreted as salient sentences for a summary. This work addresses the common word issue with a phrase-level graph that (1) focuses on the noun phrases of a document based on grammar dependencies and (2) initializes edge weights by term-frequency within the target document and inverse document frequency over the entire corpus. The importance scores of noun phrases extracted from the graph are then used to select the most salient sentences. To preserve summary coherence the order of the selected sentences is re-arranged by a flow-aware orderBERT. The results reveal that our unsupervised framework outperformed other extractive methods on ROUGE as well as two human evaluations for semantic similarity and summary coherence.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Text Summarization of Long Documents using Dependency-based Noun Phrases and Contextual Order Arrangement</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.rocling-1.3" target="_blank">https://aclanthology.org/2022.rocling-1.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Providing structural information about civil cases for judgement prediction systems or recommendation systems can enhance the efficiency of the inference procedures and the justifiability of produced results. In this research we focus on the civil cases about alimony which is a relatively uncommon choice in current applications of artificial intelligence in law. We attempt to identify the statements for four types of legal functions in judgement documents i.e. the pleadings of the applicants the responses of the opposite parties the opinions of the courts and uses of laws to reach the final decisions. In addition we also try to identify the conflicting issues between the plaintiffs and the defendants in the judgement documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using Machine Learning and Pattern-Based Methods for Identifying Elements in Chinese Judgment Documents of Civil Cases</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.rocling-1.14" target="_blank">https://aclanthology.org/2022.rocling-1.14</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The goal of an information retrieval system is to retrieve documents that are most relevant to a given user query from a huge collection of documents which usually requires time-consuming multiple comparisons between the query and candidate documents so as to find the most relevant ones. Recently a novel retrieval modeling approach dubbed Differentiable Search Index (DSI) has been proposed. DSI dramatically simplifies the whole retrieval process by encoding all information about the document collection into the parameter space of a single Transformer model on top of which DSI can in turn generate the relevant document identities (IDs) in an autoregressive manner in response to a user query. Although DSI addresses the shortcomings of traditional retrieval systems previous studies have pointed out that DSI might fail to retrieve relevant documents because DSI uses the document IDs as the pivotal mechanism to establish the relationship between queries and documents whereas not every document in the document collection has its corresponding relevant and irrelevant queries for the training purpose. In view of this we put forward to leveraging supervised contrastive learning to better render the relationship between queries and documents in the latent semantic space. Furthermore an approximate nearest neighbor search strategy is employed at retrieval time to further assist the Transformer model in generating document IDs relevant to a posed query more efficiently. A series of experiments conducted on the Nature Question benchmark dataset confirm the effectiveness and practical feasibility of our approach in relation to some strong baseline systems.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Building an Enhanced Autoregressive Document Retriever Leveraging Supervised Contrastive Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.rocling-1.34" target="_blank">https://aclanthology.org/2022.rocling-1.34</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Deep Learning (DL) is dominating the fields of Natural Language Processing (NLP) and Computer Vision (CV) in the recent times. However DL commonly relies on the availability of large data annotations so other alternative or complementary pattern-based techniques can help to improve results. In this paper we build upon Key Information Extraction (KIE) in purchase documents using both DL and rule-based corrections. Our system initially trusts on Optical Character Recognition (OCR) and text understanding based on entity tagging to identify purchase facts of interest (e.g. product codes descriptions quantities or prices). These facts are then linked to a same product group which is recognized by means of line detection and some grouping heuristics. Once these DL approaches are processed we contribute several mechanisms consisting of rule-based corrections for improving the baseline DL predictions. We prove the enhancements provided by these rule-based corrections over the baseline DL results in the presented experiments for purchase documents from public and NielsenIQ datasets.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Key Information Extraction in Purchase Documents using Deep Learning and Rule-based Corrections</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.pandl-1.2" target="_blank">https://aclanthology.org/2022.pandl-1.2</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Layout detection is an essential step for accurately extracting structured contents from historical documents. The intricate and varied layouts present in these document images make it expensive to label the numerous layout regions that can be densely arranged on each page. Current active learning methods typically rank and label samples at the image level where the annotation budget is not optimally spent due to the overexposure of common objects per image. Inspired by recent progress in semi-supervised learning and self-training we propose OLALA an Object-Level Active Learning framework for efficient document layout Annotation. OLALA aims to optimize the annotation process by selectively annotating only the most ambiguous regions within an image while using automatically generated labels for the rest. Central to OLALA is a perturbation-based scoring function that determines which objects require manual annotation. Extensive experiments show that OLALA can significantly boost model performance and improve annotation efficiency facilitating the extraction of masses of structured text for downstream NLP applications.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>OLALA: Object-Level Active Learning for Efficient Document Layout Annotation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.nlpcss-1.19" target="_blank">https://aclanthology.org/2022.nlpcss-1.19</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Pre-trained Transformers currently dominate most NLP tasks. They impose however limits on the maximum input length (512 sub-words in BERT) which are too restrictive in the legal domain. Even sparse-attention models such as Longformer and BigBird which increase the maximum input length to 4096 sub-words severely truncate texts in three of the six datasets of LexGLUE. Simpler linear classifiers with TF-IDF features can handle texts of any length require far less resources to train and deploy but are usually outperformed by pre-trained Transformers. We explore two directions to cope with long legal texts: (i) modifying a Longformer warm-started from LegalBERT to handle even longer texts (up to 8192 sub-words) and (ii) modifying LegalBERT to use TF-IDF representations. The first approach is the best in terms of performance surpassing a hierarchical version of LegalBERT which was the previous state of the art in LexGLUE. The second approach leads to computationally more efficient models at the expense of lower performance but the resulting models still outperform overall a linear SVM with TF-IDF features in long legal document classification.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Processing Long Legal Documents with Pre-trained Transformers: Modding LegalBERT and Longformer</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.nllp-1.11" target="_blank">https://aclanthology.org/2022.nllp-1.11</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Legal documents are unstructured use legal jargon and have considerable length making them difficult to process automatically via conventional text processing techniques. A legal document processing system would benefit substantially if the documents could be segmented into coherent information units. This paper proposes a new corpus of legal documents annotated (with the help of legal experts) with a set of 13 semantically coherent units labels (referred to as Rhetorical Roles) e.g. facts arguments statute issue precedent ruling and ratio. We perform a thorough analysis of the corpus and the annotations. For automatically segmenting the legal documents we experiment with the task of rhetorical role prediction: given a document predict the text segments corresponding to various roles. Using the created corpus we experiment extensively with various deep learning-based baseline models for the task. Further we develop a multitask learning (MTL) based deep model with document rhetorical role label shift as an auxiliary task for segmenting a legal document. The proposed model shows superior performance over the existing models. We also experiment with model performance in the case of domain transfer and model distillation techniques to see the model performance in limited data conditions.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Semantic Segmentation of Legal Documents via Rhetorical Roles</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.nllp-1.13" target="_blank">https://aclanthology.org/2022.nllp-1.13</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Though many algorithms can be used to automatically summarize legal case decisions most fail to incorporate domain knowledge about how important sentences in a legal decision relate to a representation of its document structure. For example analysis of a legal case sum- marization dataset demonstrates that sentences serving different types of argumentative roles in the decision appear in different sections of the document. In this work we propose an unsupervised graph-based ranking model that uses a reweighting algorithm to exploit properties of the document structure of legal case decisions. We also explore the impact of using different methods to compute the document structure. Results on the Canadian Legal Case Law dataset show that our proposed method outperforms several strong baselines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Computing and Exploiting Document Structure to Improve Unsupervised Extractive Summarization of Legal Case Decisions</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.nllp-1.30" target="_blank">https://aclanthology.org/2022.nllp-1.30</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes our use of mixed incentives and the citizen science portal LanguageARC to prepare collect and quality control a large corpus of object namings for the purpose of providing speech data to document the under-represented Guanzhong dialect of Chinese spoken in the Shaanxi province in the environs of Xi&#39;an.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using Mixed Incentives to Document Xi&#39;an Guanzhong</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.nidcp-1.6" target="_blank">https://aclanthology.org/2022.nidcp-1.6</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Text classification has achieved great success with the prosperity of deep learning and pre-trained language models. However we often encounter labeled data deficiency problems in real-world text-classification tasks. To overcome such challenging scenarios interest in few-shot learning has increased whereas most few-shot text classification studies suffer from a difficulty of utilizing pre-trained language models. In the study we propose a novel learning method for learning how to attend called LEA through which meta-level attention aspects are derived based on our meta-learning strategy. This enables the generation of task-specific document embedding with leveraging pre-trained language models even though a few labeled data instances are given. We evaluate our proposed learning method on five benchmark datasets. The results show that the novel method robustly provides the competitive performance compared to recent few-shot learning methods for all the datasets.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>LEA: Meta Knowledge-Driven Self-Attentive Document Embedding for Few-Shot Text Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.7" target="_blank">https://aclanthology.org/2022.naacl-main.7</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>News article revision histories provide clues to narrative and factual evolution in news articles. To facilitate analysis of this evolution we present the first publicly available dataset of news revision histories NewsEdits. Our dataset is large-scale and multilingual; it contains 1.2 million articles with 4.6 million versions from over 22 English- and French-language newspaper sources based in three countries spanning 15 years of coverage (2006-2021).We define article-level edit actions: Addition Deletion Edit and Refactor and develop a high-accuracy extraction algorithm to identify these actions. To underscore the factual nature of many edit actions we conduct analyses showing that added and deleted sentences are more likely to contain updating events main content and quotes than unchanged sentences. Finally to explore whether edit actions are predictable we introduce three novel tasks aimed at predicting actions performed during version updates. We show that these tasks are possible for expert humans but are challenging for large NLP models. We hope this can spur research in narrative framing and help provide predictive tools for journalists chasing breaking news.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>NewsEdits: A News Article Revision Dataset and a Novel Document-Level Reasoning Challenge</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.10" target="_blank">https://aclanthology.org/2022.naacl-main.10</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>For emerging events human readers are often exposed to both real news and fake news. Multiple news articles may contain complementary or contradictory information that readers can leverage to help detect fake news. Inspired by this process we propose a novel task of cross-document misinformation detection. Given a cluster of topically related news documents we aim to detect misinformation at both document level and a more fine-grained level event level. Due to the lack of data we generate fake news by manipulating real news and construct 3 new datasets with 422 276 and 1413 clusters of topically related documents respectively. We further propose a graph-based detector that constructs a cross-document knowledge graph using cross-document event coreference resolution and employs a heterogeneous graph neural network to conduct detection at two levels. We then feed the event-level detection results into the document-level detector. Experimental results show that our proposed method significantly outperforms existing methods by up to 7 F1 points on this new task.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-document Misinformation Detection based on Event Graph Reasoning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.40" target="_blank">https://aclanthology.org/2022.naacl-main.40</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We introduce DocTime - a novel temporal dependency graph (TDG) parser that takes as input a text document and produces a temporal dependency graph. It outperforms previous BERT-based solutions by a relative 4-8% on three datasets from modeling the problem as a graph network with path-prediction loss to incorporate longer range dependencies. This work also demonstrates how the TDG graph can be used to improve the downstream tasks of temporal questions answering and NLI by a relative 4-10% with a new framework that incorporates the temporal dependency graph into the self-attention layer of Transformer models (Time-transformer). Finally we develop and evaluate on a new temporal dependency graph dataset for the domain of contractual documents which has not been previously explored in this setting.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DocTime: A Document-level Temporal Dependency Graph Parser</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.73" target="_blank">https://aclanthology.org/2022.naacl-main.73</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Compared with traditional sentence-level relation extraction document-level relation extraction is a more challenging task where an entity in a document may be mentioned multiple times and associated with multiple relations. However most methods of document-level relation extraction do not distinguish between mention-level features and entity-level features and just apply simple pooling operation for aggregating mention-level features into entity-level features. As a result the distinct semantics between the different mentions of an entity are overlooked. To address this problem we propose RSMAN in this paper which performs selective attentions over different entity mentions with respect to candidate relations. In this manner the flexible and relation-specific representations of entities are obtained which indeed benefit relation classification. Our extensive experiments upon two benchmark datasets show that our RSMAN can bring significant improvements for some backbone models to achieve state-of-the-art performance especially when an entity have multiple mentions in the document.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Relation-Specific Attentions over Entity Mentions for Enhanced Document-Level Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.109" target="_blank">https://aclanthology.org/2022.naacl-main.109</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Standard automatic metrics e.g. BLEU are not reliable for document-level MT evaluation. They can neither distinguish document-level improvements in translation quality from sentence-level ones nor identify the discourse phenomena that cause context-agnostic translations. This paper introduces a novel automatic metric BlonDe to widen the scope of automatic MT evaluation from sentence to document level. BlonDe takes discourse coherence into consideration by categorizing discourse-related spans and calculating the similarity-based F1 measure of categorized spans. We conduct extensive comparisons on a newly constructed dataset BWB. The experimental results show that BlonDe possesses better selectivity and interpretability at the document-level and is more sensitive to document-level nuances. In a large-scale human study BlonDe also achieves significantly higher Pearson&#39;s r correlation with human judgments compared to previous metrics.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>BlonDe: An Automatic Evaluation Metric for Document-level Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.111" target="_blank">https://aclanthology.org/2022.naacl-main.111</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>A notable challenge in Multi-Document Summarization (MDS) is the extremely-long length of the input. In this paper we present an extract-then-abstract Transformer framework to overcome the problem. Specifically we leverage pre-trained language models to construct a hierarchical extractor for salient sentence selection across documents and an abstractor for rewriting the selected contents as summaries. However learning such a framework is challenging since the optimal contents for the abstractor are generally unknown. Previous works typically create textitpseudo extraction oracle to enable the supervised learning for both the extractor and the abstractor. Nevertheless we argue that the performance of such methods could be restricted due to the insufficient information for prediction and inconsistent objectives between training and testing. To this end we propose a loss weighting mechanism that makes the model aware of the unequal importance for the sentences not in the pseudo extraction oracle and leverage the fine-tuned abstractor to generate summary references as auxiliary signals for learning the extractor. Moreover we propose a reinforcement learning method that can efficiently apply to the extractor for harmonizing the optimization between training and testing. Experiment results show that our framework substantially outperforms strong baselines with comparable model sizes and achieves the best results on the Multi-News Multi-XScience and WikiCatSum corpora.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Multi-Document Summarization through Referenced Flexible Extraction with Credit-Awareness</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.120" target="_blank">https://aclanthology.org/2022.naacl-main.120</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Text clustering methods were traditionally incorporated into multi-document summarization (MDS) as a means for coping with considerable information repetition. Particularly clusters were leveraged to indicate information saliency as well as to avoid redundancy. Such prior methods focused on clustering sentences even though closely related sentences usually contain also non-aligned parts. In this work we revisit the clustering approach grouping together sub-sentential propositions aiming at more precise information alignment. Specifically our method detects salient propositions clusters them into paraphrastic clusters and generates a representative sentence for each cluster via text fusion.Our summarization method improves over the previous state-of-the-art MDS method in the DUC 2004 and TAC 2011 datasets both in automatic ROUGE scores and human preference.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Proposition-Level Clustering for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.128" target="_blank">https://aclanthology.org/2022.naacl-main.128</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Stepping from sentence-level to document-level the research on relation extraction (RE) confronts increasing text length and more complicated entity interactions. Consequently it is more challenging to encode the key information sources---relevant contexts and entity types. However existing methods only implicitly learn to model these critical information sources while being trained for RE. As a result they suffer the problems of ineffective supervision and uninterpretable model predictions. In contrast we propose to explicitly teach the model to capture relevant contexts and entity types by supervising and augmenting intermediate steps (SAIS) for RE. Based on a broad spectrum of carefully designed tasks our proposed SAIS method not only extracts relations of better quality due to more effective supervision but also retrieves the corresponding supporting evidence more accurately so as to enhance interpretability. By assessing model uncertainty SAIS further boosts the performance via evidence-based data augmentation and ensemble inference while reducing the computational cost. Eventually SAIS delivers state-of-the-art RE results on three benchmarks (DocRED CDR and GDA) and outperforms the runner-up by 5.04% relatively in F1 score in evidence retrieval on DocRED.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SAIS: Supervising and Augmenting Intermediate Steps for Document-Level Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.171" target="_blank">https://aclanthology.org/2022.naacl-main.171</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Canonical automatic summary evaluation metrics such as ROUGE focus on lexical similarity which cannot well capture semantics nor linguistic quality and require a reference summary which is costly to obtain. Recently there have been a growing number of efforts to alleviate either or both of the two drawbacks. In this paper we present a proof-of-concept study to a weakly supervised summary evaluation approach without the presence of reference summaries. Massive data in existing summarization datasets are transformed for training by pairing documents with corrupted reference summaries. In cross-domain tests our strategy outperforms baselines with promising improvements and show a great advantage in gauging linguistic qualities over all metrics.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SueNes: A Weakly Supervised Approach to Evaluating Single-Document Summarization via Negative Sampling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.175" target="_blank">https://aclanthology.org/2022.naacl-main.175</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Neural abstractive summarization models are prone to generate summaries that are factually inconsistent with their source documents. Previous work has introduced the task of recognizing such factual inconsistency as a downstream application of natural language inference (NLI). However state-of-the-art NLI models perform poorly in this context due to their inability to generalize to the target task. In this work we show that NLI models can be effective for this task when the training data is augmented with high-quality task-oriented examples. We introduce Falsesum a data generation pipeline leveraging a controllable text generation model to perturb human-annotated summaries introducing varying types of factual inconsistencies. Unlike previously introduced document-level NLI datasets our generated dataset contains examples that are diverse and inconsistent yet plausible. We show that models trained on a Falsesum-augmented NLI dataset improve the state-of-the-art performance across four benchmarks for detecting factual inconsistency in summarization.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Falsesum: Generating Document-level NLI Examples for Recognizing Factual Inconsistency in Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.199" target="_blank">https://aclanthology.org/2022.naacl-main.199</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Providing conversation models with background knowledge has been shown to make open-domain dialogues more informative and engaging. Existing models treat knowledge selection as a sentence ranking or classification problem where each sentence is handled individually ignoring the internal semantic connection between sentences. In this work we propose to automatically convert the background knowledge documents into document semantic graphs and then perform knowledge selection over such graphs. Our document semantic graphs preserve sentence-level information through the use of sentence nodes and provide concept connections between sentences. We apply multi-task learning to perform sentence-level knowledge selection and concept-level knowledge selection showing that it improves sentence-level selection. Our experiments show that our semantic graph-based knowledge selection improves over sentence selection baselines for both the knowledge selection task and the end-to-end response generation task on HollE and improves generalization on unseen topics in WoW.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Enhancing Knowledge Selection for Grounded Dialogues via Document Semantic Graphs</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.202" target="_blank">https://aclanthology.org/2022.naacl-main.202</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level relation extraction (DocRE) aims to determine the relation between two entities from a document of multiple sentences. Recent studies typically represent the entire document by sequence- or graph-based models to predict the relations of all entity pairs. However we find that such a model is not robust and exhibits bizarre behaviors: it predicts correctly when an entire test document is fed as input but errs when non-evidence sentences are removed. To this end we propose a Sentence Importance Estimation and Focusing (SIEF) framework for DocRE where we design a sentence importance score and a sentence focusing loss encouraging DocRE models to focus on evidence sentences. Experimental results on two domains show that our SIEF not only improves overall performance but also makes DocRE models more robust. Moreover SIEF is a general framework shown to be effective when combined with a variety of base DocRE models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Relation Extraction with Sentences Importance Estimation and Focusing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.212" target="_blank">https://aclanthology.org/2022.naacl-main.212</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In document-level event argument extraction an argument is likely to appear multiple times in different expressions in the document. The redundancy of arguments underlying multiple sentences is beneficial but is often overlooked. In addition in event argument extraction most entities are regarded as class ``others&#39;&#39; i.e. Universum class which is defined as a collection of samples that do not belong to any class of interest. Universum class is composed of heterogeneous entities without typical common features. Classifiers trained by cross entropy loss could easily misclassify the Universum class because of their open decision boundary. In this paper to make use of redundant event information underlying a document we build an entity coreference graph with the graph2token module to produce a comprehensive and coreference-aware representation for every entity and then build an entity summary graph to merge the multiple extraction results. To better classify Universum class we propose a new loss function to build classifiers with closed boundaries. Experimental results show that our model outperforms the previous state-of-the-art models by 3.35% in F1-score.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Event Argument Extraction by Leveraging Redundant Information and Closed Boundary Loss</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.222" target="_blank">https://aclanthology.org/2022.naacl-main.222</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Identifying related entities and events within and across documents is fundamental to natural language understanding. We present an approach to entity and event coreference resolution utilizing contrastive representation learning. Earlier state-of-the-art methods have formulated this problem as a binary classification problem and leveraged large transformers in a cross-encoder architecture to achieve their results. For large collections of documents and corresponding set of n mentions the necessity of performing n^2 transformer computations in these earlier approaches can be computationally intensive. We show that it is possible to reduce this burden by applying contrastive learning techniques that only require n transformer computations at inference time. Our method achieves state-of-the-art results on a number of key metrics on the ECB+ corpus and is competitive on others.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Contrastive Representation Learning for Cross-Document Coreference Resolution of Events and Entities</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.267" target="_blank">https://aclanthology.org/2022.naacl-main.267</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document Information Extraction (DIE) has attracted increasing attention due to its various advanced applications in the real world. Although recent literature has already achieved competitive results these approaches usually fail when dealing with complex documents with noisy OCR results or mutative layouts. This paper proposes Generative Multi-modal Network (GMN) for real-world scenarios to address these problems which is a robust multi-modal generation method without predefined label categories. With the carefully designed spatial encoder and modal-aware mask module GMN can deal with complex documents that are hard to serialized into sequential order. Moreover GMN tolerates errors in OCR results and requires no character-level annotation which is vital because fine-grained annotation of numerous documents is laborious and even requires annotators with specialized domain knowledge. Extensive experiments show that GMN achieves new state-of-the-art performance on several public DIE datasets and surpasses other methods by a large margin especially in realistic scenes.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>GMN: Generative Multi-modal Network for Practical Document Information Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.276" target="_blank">https://aclanthology.org/2022.naacl-main.276</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Event extraction aims to identify an event and then extract the arguments participating in the event. Despite the great success in sentence-level event extraction events are more naturally presented in the form of documents with event arguments scattered in multiple sentences. However a major barrier to promote document-level event extraction has been the lack of large-scale and practical training and evaluation datasets. In this paper we present DocEE a new document-level event extraction dataset including 27000+ events 180000+ arguments. We highlight three features: large-scale manual annotations fine-grained argument types and application-oriented settings. Experiments show that there is still a big gap between state-of-the-art models and human beings (41% Vs 85% in F1 score) indicating that DocEE is an open issue. DocEE is now available at https://github.com/tongmeihan1995/DocEE.git.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DocEE: A Large-Scale and Fine-grained Benchmark for Document-level Event Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.291" target="_blank">https://aclanthology.org/2022.naacl-main.291</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a new scientific document similarity model based on matching fine-grained aspects of texts. To train our model we exploit a naturally-occurring source of supervision: sentences in the full-text of papers that cite multiple papers together (co-citations). Such co-citations not only reflect close paper relatedness but also provide textual descriptions of how the co-cited papers are related. This novel form of textual supervision is used for learning to match aspects across papers. We develop multi-vector representations where vectors correspond to sentence-level aspects of documents and present two methods for aspect matching: (1) A fast method that only matches single aspects and (2) a method that makes sparse multiple matches with an Optimal Transport mechanism that computes an Earth Mover&#39;s Distance between aspects. Our approach improves performance on document similarity tasks in four datasets. Further our fast single-match method achieves competitive results paving the way for applying fine-grained similarity to large scientific corpora.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.331" target="_blank">https://aclanthology.org/2022.naacl-main.331</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In document-level event extraction (DEE) task event arguments always scatter across sentences (across-sentence issue) and multipleevents may lie in one document (multi-event issue). In this paper we argue that the relation information of event arguments is of greatsignificance for addressing the above two issues and propose a new DEE framework which can model the relation dependencies calledRelation-augmented Document-level Event Extraction (ReDEE). More specifically this framework features a novel and tailored transformernamed as Relation-augmented Attention Transformer (RAAT). RAAT is scalable to capture multi-scale and multi-amount argument relations. To further leverage relation information we introduce a separate event relation prediction task and adopt multi-task learning method to explicitly enhance event extraction performance. Extensive experiments demonstrate the effectiveness of the proposed method which can achieve state-of-the-art performance on two public datasets.Our code is available at https://github.com/TencentYoutuResearch/RAAT.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>RAAT: Relation-Augmented Attention Transformer for Relation Modeling in Document-Level Event Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.367" target="_blank">https://aclanthology.org/2022.naacl-main.367</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Most previous studies aim at extracting events from a single sentence while document-level event extraction still remains under-explored. In this paper we focus on extracting event arguments from an entire document which mainly faces two critical problems: a) the long-distance dependency between trigger and arguments over sentences; b) the distracting context towards an event in the document. To address these issues we propose a textbfTwo-textbfStream textbfAbstract meaning textbfRepresentation enhanced extraction model (TSAR). TSAR encodes the document from different perspectives by a two-stream encoding module to utilize local and global information and lower the impact of distracting context. Besides TSAR introduces an AMR-guided interaction module to capture both intra-sentential and inter-sentential features based on the locally and globally constructed AMR semantic graphs. An auxiliary boundary loss is introduced to enhance the boundary information for text spans explicitly. Extensive experiments illustrate that TSAR outperforms previous state-of-the-art by a large margin with 2.54 F1 and 5.13 F1 performance gain on the public RAMS and WikiEvents datasets respectively showing the superiority in the cross-sentence arguments extraction. We release our code in https://github.com/ PKUnlp-icler/TSAR.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Two-Stream AMR-enhanced Model for Document-level Event Argument Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.370" target="_blank">https://aclanthology.org/2022.naacl-main.370</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Long documents like contracts financial documents etc. are often tedious to read through. Linearly consuming (via scrolling or navigation through default table of content) these documents is time-consuming and challenging. These documents are also authored to be consumed by varied entities (referred to as persona in the paper) interested in only certain parts of the document. In this work we describe DynamicToC a dynamic table of content-based navigator to aid in the task of non-linear persona-based document consumption. DynamicToC highlights sections of interest in the document as per the aspects relevant to different personas. DynamicToC is augmented with short questions to assist the users in understanding underlying content. This uses a novel deep-reinforcement learning technique to generate questions on these persona-clustered paragraphs. Human and automatic evaluations suggest the efficacy of both end-to-end pipeline and different components of DynamicToC.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DynamicTOC: Persona-based Table of Contents for Consumption of Long Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.378" target="_blank">https://aclanthology.org/2022.naacl-main.378</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We target on the document-level relation extraction in an end-to-end setting where the model needs to jointly perform mention extraction coreference resolution (COREF) and relation extraction (RE) at once and gets evaluated in an entity-centric way. Especially we address the two-way interaction between COREF and RE that has not been the focus by previous work and propose to introduce explicit interaction namely Graph Compatibility (GC) that is specifically designed to leverage task characteristics bridging decisions of two tasks for direct task interference. Our experiments are conducted on DocRED and DWIE; in addition to GC we implement and compare different multi-task settings commonly adopted in previous work including pipeline shared encoders graph propagation to examine the effectiveness of different interactions. The result shows that GC achieves the best performance by up to 2.3/5.1 F1 improvement over the baseline.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Modeling Task Interactions in Document-Level Joint Entity and Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.395" target="_blank">https://aclanthology.org/2022.naacl-main.395</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present FREDo a few-shot document-level relation extraction (FSDLRE) benchmark. As opposed to existing benchmarks which are built on sentence-level relation extraction corpora we argue that document-level corpora provide more realism particularly regarding none-of-the-above (NOTA) distributions. Therefore we propose a set of FSDLRE tasks and construct a benchmark based on two existing supervised learning data sets DocRED and sciERC. We adapt the state-of-the-art sentence-level method MNAV to the document-level and develop it further for improved domain adaptation. We find FSDLRE to be a challenging setting with interesting new characteristics such as the ability to sample NOTA instances from the support set. The data code and trained models are available online (https://github.com/nicpopovic/FREDo).</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Few-Shot Document-Level Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.naacl-main.421" target="_blank">https://aclanthology.org/2022.naacl-main.421</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper introduces the mwetoolkit-lib an adaptation of the mwetoolkit as a python library. The original toolkit performs the extraction and identification of multiword expressions (MWEs) in large text bases through the command line. One of the contributions of our work is the adaptation of the MWE extraction pipeline from the mwetoolkit allowing its usage in python development environments and integration in larger pipelines. The other contribution is the execution of a pilot experiment aiming to show the impact of MWE discovery in data professionals&#39; work. This experiment found that the addition of MWE knowledge to the Term Frequency-Inverse Document Frequency (TF-IDF) vectorization altered the word relevance order improving the linguistic quality of the clusters returned by k-means method.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>mwetoolkit-lib: Adaptation of the mwetoolkit as a Python Library and an Application to MWE-based Document Clustering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.mwe-1.16" target="_blank">https://aclanthology.org/2022.mwe-1.16</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Existing methods for open-retrieval question answering in lower resource languages (LRLs) lag significantly behind English. They not only suffer from the shortcomings of non-English document retrieval but are reliant on language-specific supervision for either the task or translation. We formulate a task setup more realistic to available resources that circumvents document retrieval to reliably transfer knowledge from English to lower resource languages. Assuming a strong English question answering model or database we compare and analyze methods that pivot through English: to map foreign queries to English and then English answers back to target language answers. Within this task setup we propose Reranked Multilingual Maximal Inner Product Search (RM-MIPS) akin to semantic similarity retrieval over the English training set with reranking which outperforms the strongest baselines by 2.7% on XQuAD and 6.2% on MKQA. Analysis demonstrates the particular efficacy of this strategy over state-of-the-art alternatives in challenging settings: low-resource languages with extensive distractor data and query distribution misalignment. Circumventing retrieval our analysis shows this approach offers rapid answer generation to many other languages off-the-shelf without necessitating additional training data in the target language.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Pivot Through English: Reliably Answering Multilingual Questions without Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.mia-1.3" target="_blank">https://aclanthology.org/2022.mia-1.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The DepSign-LT-EDI-ACL2022 shared task focuses on early prediction of severity of depression over social media posts. The BioNLP group at Department of Data Science and Engineering in Indian Institute of Science Education and Research Bhopal (IISERB) has participated in this challenge and submitted three runs based on three different text mining models. The severity of depression were categorized into three classes viz. no depression moderate and severe and the data to build models were released as part of this shared task. The objective of this work is to identify relevant features from the given social media texts for effective text classification. As part of our investigation we explored features derived from text data using document embeddings technique and simple bag of words model following different weighting schemes. Subsequently adaptive boosting logistic regression random forest and support vector machine (SVM) classifiers were used to identify the scale of depression from the given texts. The experimental analysis on the given validation data show that the SVM classifier using the bag of words model following term frequency and inverse document frequency weighting scheme outperforms the other models for identifying depression. However this framework could not achieve a place among the top ten runs of the shared task. This paper describes the potential of the proposed framework as well as the possible reasons behind mediocre performance on the given data.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>IISERB@LT-EDI-ACL2022: A Bag of Words and Document Embeddings Based Framework to Identify Severity of Depression Over Social Media</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.ltedi-1.33" target="_blank">https://aclanthology.org/2022.ltedi-1.33</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The detection and extraction of abbreviations from unstructured texts can help to improve the performance of Natural Language Processing tasks such as machine translation and information retrieval. However in terms of publicly available datasets there is not enough data for training deep-neural-networks-based models to the point of generalising well over data. This paper presents PLOD a large-scale dataset for abbreviation detection and extraction that contains 160k+ segments automatically annotated with abbreviations and their long forms. We performed manual validation over a set of instances and a complete automatic validation for this dataset. We then used it to generate several baseline models for detecting abbreviations and long forms. The best models achieved an F1-score of 0.92 for abbreviations and 0.89 for detecting their corresponding long forms. We release this dataset along with our code and all the models publicly at https://github.com/surrey-nlp/PLOD-AbbreviationDetection</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>PLOD: An Abbreviation Detection Dataset for Scientific Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.lrec-1.71" target="_blank">https://aclanthology.org/2022.lrec-1.71</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents a new training dataset for automatic genre identification GINCO which is based on 1125 crawled Slovenian web documents that consist of 650000 words. Each document was manually annotated for genre with a new annotation schema that builds upon existing schemata having primarily clarity of labels and inter-annotator agreement in mind. The dataset consists of various challenges related to web-based data such as machine translated content encoding errors multiple contents presented in one document etc. enabling evaluation of classifiers in realistic conditions. The initial machine learning experiments on the dataset show that (1) pre-Transformer models are drastically less able to model the phenomena with macro F1 metrics ranging around 0.22 while Transformer-based models achieve scores of around 0.58 and (2) multilingual Transformer models work as well on the task as the monolingual models that were previously proven to be superior to multilingual models on standard NLP tasks.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The GINCO Training Dataset for Web Genre Identification of Documents Out in the Wild</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.lrec-1.170" target="_blank">https://aclanthology.org/2022.lrec-1.170</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Our knowledge on speech is historically built on data comparing different speakers or data averaged across speakers. Consequently little is known on the variability in the speech of a single individual. Experimental studies have shown that speakers adapt to the linguistic and the speaking contexts and modify their speech according to their emotional or biological condition etc. However it is unclear how much speakers vary from one repetition to the next and how comparable are recordings that are collected days months or years apart. In this paper we introduce two French databases which contain recordings of 9 to 11 speakers recorded over 9 to 18 sessions allowing comparisons of speech tasks with a different delay between the repetitions: 3 repetitions within the same session 6 to 10 repetitions on different days during a two months period 5 to 9 repetitions on different years. Speakers are recorded on a large set of speech tasks including read and spontaneous speech as well as speech-like performance tasks. In this paper we provide detailed descriptions of the two databases and available annotations. We conclude by an illustration on how these data can inform on within-speaker variability of speech.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>PATATRA and PATAFreq: two French databases for the documentation of within-speaker variability in speech</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.lrec-1.207" target="_blank">https://aclanthology.org/2022.lrec-1.207</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Topological Data Analysis (TDA) focuses on the inherent shape of (spatial) data. As such it may provide useful methods to explore spatial representations of linguistic data (embeddings) which have become central in NLP. In this paper we aim to introduce TDA to researchers in language technology. We use TDA to represent document structure as so-called story trees. Story trees are hierarchical representations created from semantic vector representations of sentences via persistent homology. They can be used to identify and clearly visualize prominent components of a story line. We showcase their potential by using story trees to create extractive summaries for news stories.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Story Trees: Representing Documents using Topological Persistence</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.lrec-1.258" target="_blank">https://aclanthology.org/2022.lrec-1.258</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The QUEST (QUality ESTablished) project aims at ensuring the reusability of audio-visual datasets (Wamprechtshammer et al. 2022) by devising quality criteria and curating processes. RefCo (Reference Corpora) is an initiative within QUEST in collaboration with DoReCo (Documentation Reference Corpus Paschen et al. (2020)) focusing on language documentation projects. Previously Aznar and Seifart (2020) introduced a set of quality criteria dedicated to documenting fieldwork corpora. Based on these criteria we establish a semi-automatic review process for existing and work-in-progress corpora in particular for language documentation. The goal is to improve the quality of a corpus by increasing its reusability. A central part of this process is a template for machine-readable corpus documentation and automatic data verification based on this documentation. In addition to the documentation and automatic verification the process involves a human review and potentially results in a RefCo certification of the corpus. For each of these steps we provide guidelines and manuals. We describe the evaluation process in detail highlight the current limits for automatic evaluation and how the manual review is organized accordingly.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>RefCo and its Checker: Improving Language Documentation Corpora&#39;s Reusability Through a Semi-Automatic Review Process</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.lrec-1.291" target="_blank">https://aclanthology.org/2022.lrec-1.291</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper analyses how much context span is necessary to solve different context-related issues namely reference ellipsis gender number lexical ambiguity and terminology when translating from English into Portuguese. We use the DELA corpus which consists of 60 documents and six different domains (subtitles literary news reviews medical and legislation). We find that the shortest context span to disambiguate issues can appear in different positions in the document including preceding following global world knowledge. Moreover the average length depends on the issue types as well as the domain. Moreover we show that the standard approach of relying on only two preceding sentences as context might not be enough depending on the domain and issue types.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>How Much Context Span is Enough? Examining Context-Related Issues for Document-level MT</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.lrec-1.323" target="_blank">https://aclanthology.org/2022.lrec-1.323</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level Neural Machine Translation aims to increase the quality of neural translation models by taking into account contextual information. Properly modelling information beyond the sentence level can result in improved machine translation output in terms of coherence cohesion and consistency. Suitable corpora for context-level modelling are necessary to both train and evaluate context-aware systems but are still relatively scarce. In this work we describe TANDO a document-level corpus for the under-resourced Basque-Spanish language pair which we share with the scientific community. The corpus is composed of parallel data from three different domains and has been prepared with context-level information. Additionally the corpus includes contrastive test sets for fine-grained evaluations of gender and register contextual phenomena on both source and target language sides. To establish the usefulness of the corpus we trained and evaluated baseline Transformer models and context-aware variants based on context concatenation. Our results indicate that the corpus is suitable for fine-grained evaluation of document-level machine translation systems.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>TANDO: A Corpus for Document-level Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.lrec-1.324" target="_blank">https://aclanthology.org/2022.lrec-1.324</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Formal documents often are organized into sections of text each with a title and extracting this structure remains an under-explored aspect of natural language processing. This iterative title-text structure is valuable data for building models for headline generation and section title generation but there is no corpus that contains web documents annotated with titles and prose texts. Therefore we propose the first title-text dataset on web documents that incorporates a wide variety of domains to facilitate downstream training. We also introduce STAPI (Section Title And Prose text Identifier) a two-step system for labeling section titles and prose text in HTML documents. To filter out unrelated content like document footers its first step involves a filter that reads HTML documents and proposes a set of textual candidates. In the second step a typographic classifier takes the candidates from the filter and categorizes each one into one of the three pre-defined classes (title prose text and miscellany). We show that STAPI significantly outperforms two baseline models in terms of title-text identification. We release our dataset along with a web application to facilitate supervised and semi-supervised training in this domain.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>STAPI: An Automatic Scraper for Extracting Iterative Title-Text Structure from Web Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.lrec-1.371" target="_blank">https://aclanthology.org/2022.lrec-1.371</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The impressive progress in NLP techniques has been driven by the development of multi-task benchmarks such as GLUE and SuperGLUE. While these benchmarks focus on tasks for one or two input sentences there has been exciting work in designing efficient techniques for processing much longer inputs. In this paper we present MuLD: a new long document benchmark consisting of only documents over 10000 tokens. By modifying existing NLP tasks we create a diverse benchmark which requires models to successfully model long-term dependencies in the text. We evaluate how existing models perform and find that our benchmark is much more challenging than their `short document&#39; equivalents. Furthermore by evaluating both regular and efficient transformers we show that models with increased context length are better able to solve the tasks presented suggesting that future improvements in these models are vital for solving similar long document problems. We release the data and code for baselines to encourage further research on efficient NLP models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MuLD: The Multitask Long Document Benchmark</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.lrec-1.392" target="_blank">https://aclanthology.org/2022.lrec-1.392</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper proposes a new cross-document coreference resolution (CDCR) dataset for identifying co-referring radiological findings and medical devices across a patient&#39;s radiology reports. Our annotated corpus contains 5872 mentions (findings and devices) spanning 638 MIMIC-III radiology reports across 60 patients covering multiple imaging modalities and anatomies. There are a total of 2292 mention chains. We describe the annotation process in detail highlighting the complexities involved in creating a sizable and realistic dataset for radiology CDCR. We apply two baseline methods--string matching and transformer language models (BERT)--to identify cross-report coreferences. Our results indicate the requirement of further model development targeting better understanding of domain language and context to address this challenging and unexplored task. This dataset can serve as a resource to develop more advanced natural language processing CDCR methods in the future. This is one of the first attempts focusing on CDCR in the clinical domain and holds potential in benefiting physicians and clinical research through long-term tracking of radiology findings.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Cross-document Coreference Dataset for Longitudinal Tracking across Radiology Reports</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.lrec-1.393" target="_blank">https://aclanthology.org/2022.lrec-1.393</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes the process of data processing and training of an automatic speech recognition (ASR) system for Cook Islands M=aori (CIM) an Indigenous language spoken by approximately 22000 people in the South Pacific. We transcribed four hours of speech from adults and elderly speakers of the language and prepared two experiments. First we trained three ASR systems: one statistical Kaldi; and two based on Deep Learning DeepSpeech and XLSR-Wav2Vec2. Wav2Vec2 tied with Kaldi for lowest character error rate (CER=6mboxpm1) and was slightly behind in word error rate (WER=23mboxpm2 versus WER=18mboxpm2 for Kaldi). This provides evidence that Deep Learning ASR systems are reaching the performance of statistical methods on small datasets and that they can work effectively with extremely low-resource Indigenous languages like CIM. In the second experiment we used Wav2Vec2 to train models with held-out speakers. While the performance decreased (CER=15mboxpm7 WER=46mboxpm16) the system still showed considerable learning. We intend to use ASR to accelerate the documentation of CIM using newly transcribed texts to improve the ASR and also generate teaching and language revitalization materials. The trained model is available under a license based on the Kaitiakitanga License which provides for non-commercial use while retaining control of the model by the Indigenous community.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Development of Automatic Speech Recognition for the Documentation of Cook Islands M=aori</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.lrec-1.412" target="_blank">https://aclanthology.org/2022.lrec-1.412</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The need for large corpora raw corpora has dramatically increased in recent years with the introduction of transfer learning and semi-supervised learning methods to Natural Language Processing. And while there have been some recent attempts to manually curate the amount of data necessary to train large language models the main way to obtain this data is still through automatic web crawling. In this paper we take the existing multilingual web corpus OSCAR and its pipeline Ungoliant that extracts and classifies data from Common Crawl at the line level and propose a set of improvements and automatic annotations in order to produce a new document-oriented version of OSCAR that could prove more suitable to pre-train large generative language models as well as hopefully other applications in Natural Language Processing and Digital Humanities.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards a Cleaner Document-Oriented Multilingual Crawled Corpus</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.lrec-1.463" target="_blank">https://aclanthology.org/2022.lrec-1.463</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In populous countries pending legal cases have been growing exponentially. There is a need for developing techniques for processing and organizing legal documents. In this paper we introduce a new corpus for structuring legal documents. In particular we introduce a corpus of legal judgment documents in English that are segmented into topical and coherent parts. Each of these parts is annotated with a label coming from a list of pre-defined Rhetorical Roles. We develop baseline models for automatically predicting rhetorical roles in a legal document based on the annotated corpus. Further we show the application of rhetorical roles to improve performance on the tasks of summarization and legal judgment prediction. We release the corpus and baseline model code along with the paper.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Corpus for Automatic Structuring of Legal Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.lrec-1.470" target="_blank">https://aclanthology.org/2022.lrec-1.470</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>A law practitioner has to go through numerous lengthy legal case proceedings for their practices of various categories such as land dispute corruption etc. Hence it is important to summarize these documents and ensure that summaries contain phrases with intent matching the category of the case. To the best of our knowledge there is no evaluation metric that evaluates a summary based on its intent. We propose an automated intent-based summarization metric which shows a better agreement with human evaluation as compared to other automated metrics like BLEU ROUGE-L etc. in terms of human satisfaction. We also curate a dataset by annotating intent phrases in legal documents and show a proof of concept as to how this system can be automated.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Evaluation Framework for Legal Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.lrec-1.508" target="_blank">https://aclanthology.org/2022.lrec-1.508</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Established cross-document coreference resolution (CDCR) datasets contain event-centric coreference chains of events and entities with identity relations. These datasets establish strict definitions of the coreference relations across related tests but typically ignore anaphora with more vague context-dependent loose coreference relations. In this paper we qualitatively and quantitatively compare the annotation schemes of ECB+ a CDCR dataset with identity coreference relations and NewsWCL50 a CDCR dataset with a mix of loose context-dependent and strict coreference relations. We propose a phrasing diversity metric (PD) that encounters for the diversity of full phrases unlike the previously proposed metrics and allows to evaluate lexical diversity of the CDCR datasets in a higher precision. The analysis shows that coreference chains of NewsWCL50 are more lexically diverse than those of ECB+ but annotating of NewsWCL50 leads to the lower inter-coder reliability. We discuss the different tasks that both CDCR datasets create for the CDCR models i.e. lexical disambiguation and lexical diversity. Finally to ensure generalizability of the CDCR models we propose a direction for CDCR evaluation that combines CDCR datasets with multiple annotation schemes that focus of various properties of the coreference chains.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards Evaluation of Cross-document Coreference Resolution Models Using Datasets with Diverse Annotation Schemes</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.lrec-1.522" target="_blank">https://aclanthology.org/2022.lrec-1.522</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Medical Subject Heading (MeSH) indexing refers to the problem of assigning a given biomedical document with the most relevant labels from an extremely large set of MeSH terms. Currently the vast number of biomedical articles in the PubMed database are manually annotated by human curators which is time consuming and costly; therefore a computational system that can assist the indexing is highly valuable. When developing supervised MeSH indexing systems the availability of a large-scale annotated text corpus is desirable. A publicly available large corpus that permits robust evaluation and comparison of various systems is important to the research community. We release a large scale annotated MeSH indexing corpus MeSHup which contains 1342667 full text articles together with the associated MeSH labels and metadata authors and publication venues that are collected from the MEDLINE database. We train an end-to-end model that combines features from documents and their associated labels on our corpus and report the new baseline.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MeSHup: Corpus for Full Text Biomedical Document Indexing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.lrec-1.586" target="_blank">https://aclanthology.org/2022.lrec-1.586</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document authoring involves a lengthy revision process marked by individual edits that are frequently linked to comments. Modeling the relationship between edits and comments leads to a better understanding of document evolution potentially benefiting applications such as content summarization and task triaging. Prior work on understanding revisions has primarily focused on classifying edit intents but falling short of a deeper understanding of the nature of these edits. In this paper we present explore the challenge of describing an edit at two levels: identifying the edit intent and describing the edit using free-form text. We begin by defining a taxonomy of general edit intents and introduce a new dataset of full revision histories of Wikipedia pages annotated with each revision&#39;s edit intent. Using this dataset we train a classifier that achieves a 90% accuracy in identifying edit intent. We use this classifier to train a distantly-supervised model that generates a high-level description of a revision in free-form text. Our experimental results show that incorporating edit intent information aids in generating better edit descriptions. We establish a set of baselines for the edit description task achieving a best score of 28 ROUGE thus demonstrating the effectiveness of our layered approach to edit understanding.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>One Document Many Revisions: A Dataset for Classification and Description of Edit Intents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.lrec-1.591" target="_blank">https://aclanthology.org/2022.lrec-1.591</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>To augment datasets used for scientific-document writing support research we extract texts from ``Related Work&#39;&#39; sections and citation information in PDF-formatted papers published in English. The previous dataset was constructed entirely with Tex-formatted papers from which it is easy to extract citation information. However since many publicly available papers in various fields are provided only in PDF format a dataset constructed using only Tex papers has limited utility. To resolve this problem we augment the existing dataset by extracting the titles of sections using the visual features of PDF documents and extracting the Related Work section text using the explicit title information. Since text generated from the figures and footnotes appearing in the extraction target areas is considered noise we remove instances of such text. Moreover we map the cited paper&#39;s information obtained using existing tools to citation marks detected by regular expression rules resulting in pairs of cited paper information and text of the Related Work section. By evaluating body text extraction and citation mapping in the constructed dataset the accuracy of the proposed dataset was found to be close to that of the previous dataset. Accordingly we demonstrated the possibility of building a significantly augmented dataset.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Dataset Construction for Scientific-Document Writing Support by Extracting Related Work Section and Citations from PDF Papers</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.lrec-1.609" target="_blank">https://aclanthology.org/2022.lrec-1.609</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Transformer models have achieved great success across many NLP problems. However previous studies in automated ICD coding concluded that these models fail to outperform some of the earlier solutions such as CNN-based models. In this paper we challenge this conclusion. We present a simple and scalable method to process long text with the existing transformer models such as BERT. We show that this method significantly improves the previous results reported for transformer models in ICD coding and is able to outperform one of the prominent CNN-based methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>BERT for Long Documents: A Case Study of Automated ICD Coding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.louhi-1.12" target="_blank">https://aclanthology.org/2022.louhi-1.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents the outcomes of the MAPA project a set of annotated corpora for 24 languages of the European Union and an open-source customisable toolkit able to detect and substitute sensitive information in text documents from any domain using state-of-the art deep learning-based named entity recognition techniques. In the context of the project the toolkit has been developed and tested on administrative legal and medical documents obtaining state-of-the-art results. As a result of the project 24 dataset packages have been released and the de-identification toolkit is available as open source.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MAPA Project: Ready-to-Go Open-Source Datasets and Deep Learning Technology to Remove Identifying Information from Text Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.legal-1.12" target="_blank">https://aclanthology.org/2022.legal-1.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Archive collections are nowadays mostly available through search engines interfaces which allow a user to retrieve documents by issuing queries. The study of these collections may be however impaired by some aspects of search engines such as the overwhelming number of documents returned or the lack of contextual knowledge provided. New methods that could work independently or in combination with search engines are then required to access these collections. In this position paper we propose to extend TimeLine Summarization (TLS) methods on archive collections to assist in their studies. We provide an overview of existing TLS methods and we describe a conceptual framework for an Archive TimeLine Summarization (ATLS) system which aims to generate informative readable and interpretable timelines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Archive TimeLine Summarization (ATLS): Conceptual Framework for Timeline Generation over Historical Document Collections</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.latechclfl-1.3" target="_blank">https://aclanthology.org/2022.latechclfl-1.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>La g&#39;en&#39;eration automatique de questions `a partir de textes peut permettre d&#39;obtenir des corpus d&#39;apprentissage pour des mod`eles de compr&#39;ehension de documents de type question/r&#39;eponse sur des textes. Si cette t^ache de g&#39;en&#39;eration est d&#39;esormais appr&#39;ehend&#39;ee par des mod`eles de type s&#39;equence-`as&#39;equence bas&#39;es sur de grands mod`eles de langage pr&#39;e-entra^in&#39;es le choix des segments r&#39;eponses `a partir desquels seront g&#39;en&#39;er&#39;ees les questions est l&#39;un des principaux aspects diff&#39;erenciant les m&#39;ethodes de g&#39;en&#39;eration de corpus de question/r&#39;eponse. Nous proposons dans cette &#39;etude d&#39;exploiter l&#39;analyse s&#39;emantique de textes pour s&#39;electionner des r&#39;eponses plausibles et enrichir le processus de g&#39;en&#39;eration par des traits s&#39;emantiques g&#39;en&#39;eriques. Les questions g&#39;en&#39;er&#39;ees sont &#39;evalu&#39;ees dans leur capacit&#39;e `a ^etre utilis&#39;ees pour entra^iner un mod`ele de question-r&#39;eponse sur un nouveau corpus d&#39;archives num&#39;eris&#39;ees.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>G&#39;en&#39;eration de question `a partir d&#39;analyse s&#39;emantique pour l&#39;adaptation non supervis&#39;ee de mod`eles de compr&#39;ehension de documents (Question generation from semantic analysis for unsupervised adaptation of document understanding models)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.jeptalnrecital-taln.10" target="_blank">https://aclanthology.org/2022.jeptalnrecital-taln.10</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Identification de mots et passages difficiles dans les documents m&#39;edicaux en franccais. L&#39;objectif de la simplification automatique des textes consiste `a fournir une nouvelle version de documents qui devient plus facile `a comprendre pour une population donn&#39;ee ou plus facile `a traiter par d&#39;autres applications du TAL. Cependant avant d&#39;effectuer la simplification il est important de savoir ce qu&#39;il faut simplifier exactement dans les documents. En effet m^eme dans les documents techniques et sp&#39;ecialis&#39;es il n&#39;est pas n&#39;ecessaire de tout simplifier mais juste les segments qui pr&#39;esentent des difficult&#39;es de compr&#39;ehension. Il s&#39;agit typiquement de la t^ache d&#39;identification de mots complexes : effectuer le diagnostic de difficult&#39;e d&#39;un document donn&#39;e pour y d&#39;etecter les mots et passages complexes. Nous proposons de travail sur l&#39;identification de mots et passages complexes dans les documents biom&#39;edicaux en franccais.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Identification of complex words and passages in medical documents in French</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.jeptalnrecital-taln.11" target="_blank">https://aclanthology.org/2022.jeptalnrecital-taln.11</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The Document Vectors Using Cosine Similarity Revisited</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.insights-1.17" target="_blank">https://aclanthology.org/2022.insights-1.17</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Recent Advances in Long Documents Classification Using Deep-Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.icnlsp-1.12" target="_blank">https://aclanthology.org/2022.icnlsp-1.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Optimizing singular value based similarity measures for document similarity comparisons</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.icnlsp-1.13" target="_blank">https://aclanthology.org/2022.icnlsp-1.13</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Second-order Document Similarity Metrics for Transformers</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.icnlsp-1.15" target="_blank">https://aclanthology.org/2022.icnlsp-1.15</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recent advances in automatic text summarization have contemporaneously been accompanied by a great deal of new metrics of automatic evaluation. This in turn has inspired recent research to re-assess these evaluation metrics to see how well they correlate with each other as well as with human evaluation mostly focusing on single-document summarization (SDS) tasks. Although many of these metrics are typically also used for evaluating multi-document summarization (MDS) tasks so far little attention has been paid to studying them under such a distinct scenario. To address this gap we present a systematic analysis of the inter-metric correlations for MDS tasks while comparing and contrasting the results with SDS models. Using datasets from a wide range of domains (news peer reviews tweets dialogues) we thus study a unified set of metrics under both the task setups. Our empirical analysis suggests that while most reference-based metrics show fairly similar trends across both multi- and single-document summarization there is a notable lack of correlation between reference-free metrics in multi-document summarization tasks.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Assessing Inter-metric Correlation for Multi-document Summarization Evaluation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.gem-1.40" target="_blank">https://aclanthology.org/2022.gem-1.40</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper proposes a multilingual Automated Text Summarization (ATS) method targeting the Financial Narrative Summarization Task (FNS-2022). We developed two systems; the first uses a pre-trained abstractive summarization model that was fine-tuned on the downstream objective the second approaches the problem as an extractive approach in which a similarity search is performed on the trained span representations. Both models aim to identify the beginning of the continuous narrative section of the document. The language models were fine-tuned on a financial document collection of three languages (English Spanish and Greek) and aim to identify the beginning of the summary narrative part of the document. The proposed systems achieve high performance in the given task with the sequence-to-sequence variant ranked 1st on ROUGE-2 F1 score on the test set for each of the three languages.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilingual Text Summarization on Financial Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.fnp-1.7" target="_blank">https://aclanthology.org/2022.fnp-1.7</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Summarisation of long financial documents is a challenging task due to the lack of large-scale datasets and the need for domain knowledge experts to create human-written summaries. Traditional summarisation approaches that generate a summary based on the content cannot produce summaries comparable to human-written ones and thus are rarely used in practice. In this work we use the Longformer-Encoder-Decoder (LED) model to handle long financial reports. We describe our experiments and participating systems in the financial narrative summarisation shared task. Multi-stage fine-tuning helps the model generalise better on niche domains and avoids the problem of catastrophic forgetting. We further investigate the effect of the staged fine-tuning approach on the FNS dataset. Our systems achieved promising results in terms of ROUGE scores on the validation dataset.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Transformer-based Models for Long Document Summarisation in Financial Domain</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.fnp-1.10" target="_blank">https://aclanthology.org/2022.fnp-1.10</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes the FinTOC-2022 Shared Task on the structure extraction from financial documents its participants results and their findings. This shared task was organized as part of The 4th Financial Narrative Processing Workshop (FNP 2022) held jointly at The 13th Edition of the Language Resources and Evaluation Conference (LREC 2022) Marseille France (El-Haj et al. 2022). This shared task aimed to stimulate research in systems for extracting table-of-contents (TOC) from investment documents (such as financial prospectuses) by detecting the document titles and organizing them hierarchically into a TOC. For the forth edition of this shared task three subtasks were presented to the participants: one with English documents one with French documents and the other one with Spanish documents. This year we proposed a different and revised dataset for English and French compared to the previous editions of FinTOC and a new dataset for Spanish documents was added. The task attracted 6 submissions for each language from 4 teams and the most successful methods make use of textual structural and visual features extracted from the documents and propose classification models for detecting titles and TOCs for all of the subtasks.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The Financial Document Structure Extraction Shared Task (FinTOC 2022)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.fnp-1.12" target="_blank">https://aclanthology.org/2022.fnp-1.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we introduce the results of our submitted system to the FinTOC 2022 task. We address the task using a two-stage process: first we detect titles using Document Image Analysis then we train a supervised model for the hierarchical level prediction. We perform Document Image Analysis using a pre-trained Faster R-CNN on the PublyaNet dataset. We fine-tuned the model on the FinTOC 2022 training set. We extract orthographic and layout features from detected titles and use them to train a Random Forest model to predict the title level. The proposed system ranked #1 on both Title Detection and the Table of Content extraction tasks for Spanish. The system ranked #3 on both the two subtasks for English and French.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>swapUNIBA@FinTOC2022: Fine-tuning Pre-trained Document Image Analysis Model for Title Detection on the Financial Domain</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.fnp-1.14" target="_blank">https://aclanthology.org/2022.fnp-1.14</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>n this paper we present our contribution to the FinTOC-2022 Shared Task ``Financial Document Structure Extraction&#39;&#39;. We participated in the three tracks dedicated to English French and Spanish document processing. Our main contribution consists in considering financial prospectus as a bundle of documents i.e. a set of merged documents each with their own layout and structure. Therefore Document Layout and Structure Analysis (DLSA) first starts with the boundary detection of each document using general layout features. Then the process applies inside each single document taking advantage of the local properties. DLSA is achieved considering simultaneously text content vectorial shapes and images embedded in the native PDF document. For the Title Detection task in English and French we observed a significant improvement of the F-measures for Title Detection compared with those obtained during our previous participation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>GREYC@FinTOC-2022: Handling Document Layout and Structure in Native PDF Bundle of Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.fnp-1.15" target="_blank">https://aclanthology.org/2022.fnp-1.15</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes multi-lingual long document summarization systems submitted to the Financial Narrative Summarization Shared Task (FNS 2022 ) by Team-Tredence. We developed task-specific summarization methods for 3 languages -- English Spanish and Greek. The solution is divided into two parts where a RoBERTa model was finetuned to identify/extract summarizing segments from English documents and T5 based models were used for summarizing Spanish and Greek documents. A purely extractive approach was applied to summarize English documents using data-specific heuristics. An mT5 model was fine-tuned to identify potential narrative sections for Greek and Spanish followed by finetuning mT5 and T5(Spanish version) for abstractive summarization task. This system also features a novel approach for generating summarization training dataset using long document segmentation and the semantic similarity across segments. We also introduce an N-gram variability score to select sub-segments for generating more diverse and informative summaries from long documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilingual Financial Documentation Summarization by Team_Tredence for FNS2022</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.fnp-1.18" target="_blank">https://aclanthology.org/2022.fnp-1.18</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we describe our DCU-Lorcan system for the FinCausal 2022 shared task: span-based cause and effect extraction from financial documents. We frame the FinCausal 2022 causality extraction task as a span extraction/sequence labeling task our submitted systems are based on the contextualized word representations produced by pre-trained language models and linear layers predicting the label for each word followed by post-processing heuristics. In experiments we employ pre-trained language models including DistilBERT BERT and SpanBERT. Our best performed system achieves F-1 Recall Precision and Exact Match scores of 92.76 92.77 92.76 and 68.60 respectively. Additionally we conduct experiments investigating the effect of data size to the performance of causality extraction model and an error analysis investigating the outputs in predictions.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DCU-Lorcan at FinCausal 2022: Span-based Causality Extraction from Financial Documents using Pre-trained Language Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.fnp-1.19" target="_blank">https://aclanthology.org/2022.fnp-1.19</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper describes the approach which we have built for causality extraction from the financial documents that we have submitted for FinCausal 2022 task 2. We proving a solution with intelligent pre-processing and post-processing to detect the number of cause and effect in a financial document and extract them. Our given approach achieved 90% as F1 score(weighted-average) for the official blind evaluation dataset.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ExpertNeurons at FinCausal 2022 Task 2: Causality Extraction for Financial Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.fnp-1.22" target="_blank">https://aclanthology.org/2022.fnp-1.22</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The scientific claim verification task requires an NLP system to label scientific documents which Support or Refute an input claim and to select evidentiary sentences (or rationales) justifying each predicted label. In this work we present MultiVerS which predicts a fact-checking label and identifies rationales in a multitask fashion based on a shared encoding of the claim and full document context. This approach accomplishes two key modeling goals. First it ensures that all relevant contextual information is incorporated into each labeling decision. Second it enables the model to learn from instances annotated with a document-level fact-checking label but lacking sentence-level rationales. This allows MultiVerS to perform weakly-supervised domain adaptation by training on scientific documents labeled using high-precision heuristics. Our approach outperforms two competitive baselines on three scientific claim verification datasets with particularly strong performance in zero / few-shot domain adaptation experiments. Our code and data are available at https://github.com/dwadden/multivers.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MultiVerS: Improving scientific claim verification with weak supervision and full-document context</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-naacl.6" target="_blank">https://aclanthology.org/2022.findings-naacl.6</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a self-supervised pre-training approach for learning rich visual language representations for both handwritten and printed historical document transcription. After supervised fine-tuning of our pre-trained encoder representations for low-resource document transcription on two languages (1) a heterogeneous set of handwritten Islamicate manuscript images and (2) early modern English printed documents we show a meaningful improvement in recognition accuracy over the same supervised model trained from scratch with as few as 30 line image transcriptions for training. Our masked language model-style pre-training strategy where the model is trained to be able to identify the true masked visual representation from distractors sampled from within the same line encourages learning robust contextualized language representations invariant to scribal writing style and printing noise present across documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Lacuna Reconstruction: Self-Supervised Pre-Training for Low-Resource Historical Document Transcription</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-naacl.15" target="_blank">https://aclanthology.org/2022.findings-naacl.15</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we introduce DOCmT5 a multilingual sequence-to-sequence language model pretrained with large-scale parallel documents. While previous approaches have focused on leveraging sentence-level parallel data we try to build a general-purpose pretrained model that can understand and generate long documents. We propose a simple and effective pretraining objective - Document reordering Machine Translation (DrMT) in which the input documents that are shuffled and masked need to be translated. DrMT brings consistent improvements over strong baselines on a variety of document-level generation tasks including over 12 BLEU points for seen-language pair document-level MT over 7 BLEU points for unseen-language-pair document-level MT and over 3 ROUGE-1 points for seen-language pair cross-lingual summarization. We achieve state-of-the-art (SOTA) on WMT20 De-En and IWSLT15 Zh-En document translation tasks. We also conduct extensive analysis on various factors for document pretraining including (1) the effects of pretraining data quality and (2) The effects of combining mono-lingual and cross-lingual pretraining. We plan to make our model checkpoints publicly available.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DOCmT5: Document-Level Pretraining of Multilingual Language Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-naacl.32" target="_blank">https://aclanthology.org/2022.findings-naacl.32</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Pretrained language models such as BERT have been successfully applied to a wide range of natural language processing tasks and also achieved impressive performance in document reranking tasks. Recent works indicate that further pretraining the language models on the task-specific datasets before fine-tuning helps improve reranking performance. However the pre-training tasks like masked language model and next sentence prediction were based on the context of documents instead of encouraging the model to understand the content of queries in document reranking task. In this paper we propose a new self-supervised joint training framework (SJTF) with a self-supervised method called Masked Query Prediction (MQP) to establish semantic relations between given queries and positive documents. The framework randomly masks a token of query and encodes the masked query paired with positive documents and uses a linear layer as a decoder to predict the masked token. In addition the MQP is used to jointly optimize the models with supervised ranking objective during fine-tuning stage without an extra further pre-training stage. Extensive experiments on the MS MARCO passage ranking and TREC Robust datasets show that models trained with our framework obtain significant improvements compared to original models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Self-supervised Joint Training Framework for Document Reranking</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-naacl.79" target="_blank">https://aclanthology.org/2022.findings-naacl.79</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The Transformer architecture has led to significant gains in machine translation. However most studies focus on only sentence-level translation without considering the context dependency within documents leading to the inadequacy of document-level coherence. Some recent research tried to mitigate this issue by introducing an additional context encoder or translating with multiple sentences or even the entire document. Such methods may lose the information on the target side or have an increasing computational complexity as documents get longer. To address such problems we introduce a recurrent memory unit to the vanilla Transformer which supports the information exchange between the sentence and previous context. The memory unit is recurrently updated by acquiring information from sentences and passing the aggregated knowledge back to subsequent sentence states. We follow a two-stage training strategy in which the model is first trained at the sentence level and then finetuned for document-level translation. We conduct experiments on three popular datasets for document-level machine translation and our model has an average improvement of 0.91 s-BLEU over the sentence-level baseline. We also achieve state-of-the-art results on TED and News outperforming the previous work by 0.36 s-BLEU and 1.49 d-BLEU on average.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learn To Remember: Transformer with Recurrent Memory for Document-Level Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-naacl.105" target="_blank">https://aclanthology.org/2022.findings-naacl.105</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Historical records in Korea before the 20th century were primarily written in Hanja an extinct language based on Chinese characters and not understood by modern Korean or Chinese speakers. Historians with expertise in this time period have been analyzing the documents but that process is very difficult and time-consuming and language models would significantly speed up the process. Toward building and evaluating language models for Hanja we release the Hanja Understanding Evaluation dataset consisting of chronological attribution topic classification named entity recognition and summary retrieval tasks. We also present BERT-based models continued training on the two major corpora from the 14th to the 19th centuries: the Annals of the Joseon Dynasty and Diaries of the Royal Secretariats. We compare the models with several baselines on all tasks and show there are significant improvements gained by training on the two corpora. Additionally we run zero-shot experiments on the Daily Records of the Royal Court and Important Officials (DRRI). The DRRI dataset has not been studied much by the historians and not at all by the NLP community.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>HUE: Pretrained Model and Dataset for Understanding Hanja Documents of Ancient Korea</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-naacl.140" target="_blank">https://aclanthology.org/2022.findings-naacl.140</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Legal document classification is an essential task in law intelligence to automate the labor-intensive law case filing process. Unlike traditional document classification problems legal documents should be classified by reasons and facts instead of topics. We propose a Document-to-Graph Classifier (D2GCLF) which extracts facts as relations between key participants in the law case and represents a legal document with four relation graphs. Each graph is responsible for capturing different relations between the litigation participants. We further develop a graph attention network on top of the four relation graphs to classify the legal documents. Experiments on a real-world legal document dataset show that D2GCLF outperforms the state-of-the-art methods in terms of accuracy.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>D2GCLF: Document-to-Graph Classifier for Legal Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-naacl.170" target="_blank">https://aclanthology.org/2022.findings-naacl.170</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Events are inter-related in documents. Motivated by the one-sense-per-discourse theory we hypothesize that a participant tends to play consistent roles across multiple events in the same document. However recent work on document-level event argument extraction models each individual event in isolation and therefore causes inconsistency among extracted arguments across events which will further cause discrepancy for downstream applications such as event knowledge base population question answering and hypothesis generation. In this work we formulate event argument consistency as the constraints from event-event relations under the document-level setting. To improve consistency we introduce the Event-Aware Argument Extraction (EA^2E) model with augmented context for training and inference. Experiment results on WIKIEVENTS and ACE2005 datasets demonstrate the effectiveness of EA^2E compared to baseline methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>EA^2E: Improving Consistency with Event Awareness for Document-Level Argument Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-naacl.202" target="_blank">https://aclanthology.org/2022.findings-naacl.202</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Natural Language Inference (NLI) has been extensively studied by the NLP community as a framework for estimating the semantic relation between sentence pairs. While early work identified certain biases in NLI models recent advancements in modeling and datasets demonstrated promising performance.In this work we further explore the direct zero-shot applicability of NLI models to real applications beyond the sentence-pair setting they were trained on. First we analyze the robustness of these models to longer and out-of-domain inputs. Then we develop new aggregation methods to allow operating over full documents reaching state-of-the-art performance on the ContractNLI dataset. Interestingly we find NLI scores to provide strong retrieval signals leading to more relevant evidence extractions compared to common similarity-based methods. Finally we go further and investigate whole document clusters to identify both discrepancies and consensus among sources. In a test case we find real inconsistencies between Wikipedia pages in different languages about the same topic.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Stretching Sentence-pair NLI Models to Reason over Long Documents and Clusters</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-emnlp.28" target="_blank">https://aclanthology.org/2022.findings-emnlp.28</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The surge of pre-training has witnessed the rapid development of document understanding recently. Pre-training and fine-tuning framework has been effectively used to tackle texts in various formats including plain texts document texts and web texts. Despite achieving promising performance existing pre-trained models usually target one specific document format at one time making it difficult to combine knowledge from multiple document formats. To address this we propose XDoc a unified pre-trained model which deals with different document formats in a single model. For parameter efficiency we share backbone parameters for different formats such as the word embedding layer and the Transformer layers. Meanwhile we introduce adaptive layers with lightweight parameters to enhance the distinction across different formats. Experimental results have demonstrated that with only 36.7% parameters XDoc achieves comparable or even better performance on a variety of downstream tasks compared with the individual pre-trained models which is cost effective for real-world deployment. The code and pre-trained models are publicly available at urlhttps://aka.ms/xdoc.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>XDoc: Unified Pre-training for Cross-Format Document Understanding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-emnlp.71" target="_blank">https://aclanthology.org/2022.findings-emnlp.71</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Existing abstractive summarization systems are hampered by content hallucinations in which models generate text that is not directly inferable from the source alone. Annotations from prior work have shown that some of these hallucinations while being `unfaithful&#39; to the source are nonetheless factual. Our analysis in this paper suggests that these factual hallucinations occur as a result of the prevalence of factual yet unfaithful entities in summarization datasets. We find that these entities are not aberrations but instead examples of additional world knowledge being readily used to latently connect entities and concepts -- in this case connecting entities in the source document to those in the target summary. In our analysis and experiments we demonstrate that connecting entities to an external knowledge base can lend provenance to many of these unfaithful yet factual entities and further this knowledge can be used to improve the factuality of summaries without simply making them more extractive.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Faithful to the Document or to the World? Mitigating Hallucinations via Entity-Linked Knowledge in Abstractive Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-emnlp.76" target="_blank">https://aclanthology.org/2022.findings-emnlp.76</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The Annals of Joseon Dynasty (AJD) contain the daily records of the Kings of Joseon the 500-year kingdom preceding the modern nation of Korea.The Annals were originally written in an archaic Korean writing system `Hanja&#39; and were translated into Korean from 1968 to 1993.The resulting translation was however too literal and contained many archaic Korean words; thus a new expert translation effort began in 2012. Since then the records of only one king have been completed in a decade.In parallel expert translators are working on English translation also at a slow pace and produced only one king&#39;s records in English so far.Thus we propose H2KE a neural machine translation model that translates historical documents in Hanja to more easily understandable Korean and to English.Built on top of multilingual neural machine translation H2KE learns to translate a historical document written in Hanja from both a full dataset of outdated Korean translation and a small dataset of more recently translated contemporary Korean and English.We compare our method against two baselines:a recent model that simultaneously learns to restore and translate Hanja historical documentand a Transformer based model trained only on newly translated corpora.The experiments reveal that our method significantly outperforms the baselines in terms of BLEU scores for both contemporary Korean and English translations.We further conduct extensive human evaluation which shows that our translation is preferred over the original expert translations by both experts and non-expert Korean speakers.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Translating Hanja Historical Documents to Contemporary Korean and English</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-emnlp.91" target="_blank">https://aclanthology.org/2022.findings-emnlp.91</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this work we present a model for document-grounded response generation in dialog that is decomposed into two components according to Bayes&#39; theorem.One component is a traditional ungrounded response generation model and the other component models the reconstruction of the grounding document based on the dialog context and generated response.We propose different approximate decoding schemes and evaluate our approach on multiple open-domain and task-oriented document-grounded dialog datasets.Our experiments show that the model is more factual in terms of automatic factuality metrics than the baseline model.Furthermore we outline how introducing scaling factors between the components allows for controlling the tradeoff between factuality and fluency in the model output.Finally we compare our approach to a recently proposed method to control factuality in grounded dialog CTRL (Rashkin et al. 2021) and show that both approaches can be combined to achieve additional improvements.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Controllable Factuality in Document-Grounded Dialog Systems Using a Noisy Channel Model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-emnlp.98" target="_blank">https://aclanthology.org/2022.findings-emnlp.98</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper introduces Doc2Bot a novel dataset for building machines that help users seek information via conversations. This is of particular interest for companies and organizations that own a large number of manuals or instruction books. Despite its potential the nature of our task poses several challenges: (1) documents contain various structures that hinder the ability of machines to comprehend and (2) user information needs are often underspecified. Compared to prior datasets that either focus on a single structural type or overlook the role of questioning to uncover user needs the Doc2Bot dataset is developed to target such challenges systematically. Our dataset contains over 100000 turns based on Chinese documents from five domains larger than any prior document-grounded dialog dataset for information seeking. We propose three tasks in Doc2Bot: (1) dialog state tracking to track user intentions (2) dialog policy learning to plan system actions and contents and (3) response generation which generates responses based on the outputs of the dialog policy. Baseline methods based on the latest deep learning models are presented indicating that our proposed tasks are challenging and worthy of further research.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Doc2Bot: Accessing Heterogeneous Documents via Conversational Bots</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-emnlp.131" target="_blank">https://aclanthology.org/2022.findings-emnlp.131</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Financial prediction is complex due to the stochastic nature of the stock market. Semi-structured financial documents present comprehensive financial data in tabular formats such as earnings profit-loss statements and balance sheets and can often contain rich technical analysis along with a textual discussion of corporate history and management analysis compliance and risks. Existing research focuses on the textual and audio modalities of financial disclosures from company conference calls to forecast stock volatility and price movement but ignores the rich tabular data available in financial reports. Moreover the economic realm is still plagued with a severe under-representation of various communities spanning diverse demographics gender and native speakers. In this work we show that combining tabular data from financial semi-structured documents with text transcripts and audio recordings not only improves stock volatility and price movement prediction by 5-12% but also reduces gender bias caused due to audio-based neural networks by over 30%.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DocFin: Multimodal Financial Prediction and Bias Mitigation using Semi-structured Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-emnlp.139" target="_blank">https://aclanthology.org/2022.findings-emnlp.139</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level relation extraction (DocRE) aims to identify semantic labels among entities within a single document. One major challenge of DocRE is to dig decisive details regarding a specific entity pair from long text. However in many cases only a fraction of text carries required information even in the manually labeled supporting evidence. To better capture and exploit instructive information we propose a novel expLicit syntAx Refinement and Subsentence mOdeliNg based framework (LARSON). By introducing extra syntactic information LARSON can model subsentences of arbitrary granularity and efficiently screen instructive ones. Moreover we incorporate refined syntax into text representations which further improves the performance of LARSON. Experimental results on three benchmark datasets (DocRED CDR and GDA) demonstrate that LARSON significantly outperforms existing methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Not Just Plain Text! Fuel Document-Level Relation Extraction with Explicit Syntax Refinement and Subsentence Modeling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-emnlp.140" target="_blank">https://aclanthology.org/2022.findings-emnlp.140</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Given the recent proliferation of false claims online there has been a lot of manual fact-checking effort. As this is very time-consuming human fact-checkers can benefit from tools that can support them and make them more efficient. Here we focus on building a system that could provide such support. Given an input document it aims to detect all sentences that contain a claim that can be verified by some previously fact-checked claims (from a given database). The output is a re-ranked list of the document sentences so that those that can be verified are ranked as high as possible together with corresponding evidence. Unlike previous work which has looked into claim retrieval here we take a document-level perspective. We create a new manually annotated dataset for the task and we propose suitable evaluation measures. We further experiment with a learning-to-rank approach achieving sizable performance gains over several strong baselines. Our analysis demonstrates the importance of modeling text similarity and stance while also taking into account the veracity of the retrieved previously fact-checked claims. We believe that this research would be of interest to fact-checkers journalists media and regulatory authorities.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Assisting the Human Fact-Checkers: Detecting All Previously Fact-Checked Claims in a Document</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-emnlp.151" target="_blank">https://aclanthology.org/2022.findings-emnlp.151</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Developing models that can automatically generate detailed code explanation can greatly benefit software maintenance and programming education. However existing code-to-text generation models often produce only high-level summaries of code that do not capture implementation-level choices essential for these scenarios. To fill in this gap we propose the code explanation generation task. We first conducted a human study to identify the criteria for high-quality explanatory docstring for code. Based on that we collected and refined a large-scale code docstring corpus and formulated automatic evaluation metrics that best match human assessments. Finally we present a multi-stage fine-tuning strategy and baseline models for the task. Our experiments show that (1) our refined training dataset lets models achieve better performance in the explanation generation tasks compared to larger-scale unrefined data (15x larger) and (2) fine-tuned models can generate well-structured long docstrings comparable to human-written ones. We envision our training dataset human-evaluation protocol recommended metrics and fine-tuning strategy can boost future code explanation research. The code and annotated data are available at https://github.com/subercui/CodeExp.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CodeExp: Explanatory Code Document Generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-emnlp.174" target="_blank">https://aclanthology.org/2022.findings-emnlp.174</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In recent years there is a surge of generation-based information extraction work which allows a more direct use of pre-trained language models and efficiently captures output dependencies. However previous generative methods using lexical representation do not naturally fit document-level relation extraction (DocRE) where there are multiple entities and relational facts. In this paper we investigate the root cause of the underwhelming performance of the existing generative DocRE models and discover that the culprit is the inadequacy of the training paradigm instead of the capacities of the models. We propose to generate a symbolic and ordered sequence from the relation matrix which is deterministic and easier for model to learn. Moreover we design a parallel row generation method to process overlong target sequences. Besides we introduce several negative sampling strategies to improve the performance with balanced signals. Experimental results on four datasets show that our proposed method can improve the performance of the generative DocRE models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DORE: Document Ordered Relation Extraction based on Generative Framework</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-emnlp.253" target="_blank">https://aclanthology.org/2022.findings-emnlp.253</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recent years have witnessed the rise and success of pre-training techniques in visually-rich document understanding. However most existing methods lack the systematic mining and utilization of layout-centered knowledge leading to sub-optimal performances. In this paper we propose ERNIE-Layout a novel document pre-training solution with layout knowledge enhancement in the whole workflow to learn better representations that combine the features from text layout and image. Specifically we first rearrange input sequences in the serialization stage and then present a correlative pre-training task reading order prediction to learn the proper reading order of documents. To improve the layout awareness of the model we integrate a spatial-aware disentangled attention into the multi-modal transformer and a replaced regions prediction task into the pre-training phase. Experimental results show that ERNIE-Layout achieves superior performance on various downstream tasks setting new state-of-the-art on key information extraction document image classification and document question answering datasets. The code and models are publicly available at PaddleNLP.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-emnlp.274" target="_blank">https://aclanthology.org/2022.findings-emnlp.274</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present M3 (Multi-level dataset for Multi-document summarisation of Medical studies) a benchmark dataset for evaluating the quality of summarisation systems in the biomedical domain. The dataset contains sets of multiple input documents and target summaries of three levels of complexity: documents sentences and propositions. The dataset also includes several levels of annotation including biomedical entities direction and strength of relations between them and the discourse relationships between the input documents (``contradiction&#39;&#39; or ``agreement&#39;&#39;). We showcase usage scenarios of the dataset by testing 10 generic and domain-specific summarisation models in a zero-shot setting and introduce a probing task based on counterfactuals to test if models are aware of the direction and strength of the conclusions generated from input studies.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>M3: Multi-level dataset for Multi-document summarisation of Medical studies</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-emnlp.286" target="_blank">https://aclanthology.org/2022.findings-emnlp.286</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Most of the state-of-the-art methods for abstractive text summarization are under supervised learning settings while heavily relying on high-quality and large-scale parallel corpora. In this paper we remove the need for reference summaries and present an unsupervised learning method SCR (Summarize Contrast and Review) for abstractive summarization which leverages contrastive learning and is the first work to apply contrastive learning for unsupervised abstractive summarization. Particularly we use the true source documents as positive source document examples and strategically generated fake source documents as negative source document examples to train the model to generate good summaries. Furthermore we consider and improve the writing quality of the generated summaries by guiding them to be similar to human-written texts. The promising results on extensive experiments show that SCR outperforms other unsupervised abstractive summarization baselines which demonstrates its effectiveness.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learning From the Source Document: Unsupervised Abstractive Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-emnlp.309" target="_blank">https://aclanthology.org/2022.findings-emnlp.309</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Different from general documents it is recognised that the ease with which people can understand a biomedical text is eminently varied owing to the highly technical nature of biomedical documents and the variance of readers&#39; domain knowledge. However existing biomedical document summarization systems have paid little attention to readability control leaving users with summaries that are incompatible with their levels of expertise.In recognition of this urgent demand we introduce a new task of readability controllable summarization for biomedical documents which aims to recognise users&#39; readability demands and generate summaries that better suit their needs: technical summaries for experts and plain language summaries (PLS) for laymen.To establish this task we construct a corpus consisting of biomedical papers with technical summaries and PLSs written by the authors and benchmark multiple advanced controllable abstractive and extractive summarization models based on pre-trained language models (PLMs) with prevalent controlling and generation techniques.Moreover we propose a novel masked language model (MLM) based metric and its variant to effectively evaluate the readability discrepancy between lay and technical summaries.Experimental results from automated and human evaluations show that though current control techniques allow for a certain degree of readability adjustment during generation the performance of existing controllable summarization methods is far from desirable in this task.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Readability Controllable Biomedical Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-emnlp.343" target="_blank">https://aclanthology.org/2022.findings-emnlp.343</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Clinical trials are essential for drug development but are extremely expensive and time-consuming to conduct. It is beneficial to study similar historical trials when designing a clinical trial. However lengthy trial documents and lack of labeled data make trial similarity search difficult. We propose a zero-shotclinical trial retrieval method called Trial2Vec which learns through self-supervision without the need for annotating similar clinical trials. Specifically the meta-structure of trial documents (e.g. title eligibility criteria target disease) along with clinical knowledge (e.g. UMLS knowledge base) are leveraged to automatically generate contrastive samples. Besides encodes trial documents considering meta-structure thus producing compact embeddings aggregating multi-aspect information from the whole document. We show that our method yields medically interpretable embeddings by visualization and it gets 15% average improvement over the best baselines on precision/recall for trial retrieval which is evaluated on our labeled 1600 trial pairs. In addition we prove the pretrained embeddings benefit the downstream trial outcome prediction task over 240k trials. Software is available at https://github.com/RyanWangZf/Trial2Vec.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Trial2Vec: Zero-Shot Clinical Trial Document Similarity Search using Self-Supervision</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-emnlp.476" target="_blank">https://aclanthology.org/2022.findings-emnlp.476</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recent works in cyber deception study how to deter malicious intrusion by generating multiple fake versions of a critical document to impose costs on adversaries who need to identify the correct information. However existing approaches are context-agnostic resulting in sub-optimal and unvaried outputs. We propose a novel context-aware model Fake Document Infilling (FDI) by converting the problem to a controllable mask-then-infill procedure. FDI masks important concepts of varied lengths in the document then infills a realistic but fake alternative considering both the previous and future contexts. We conduct comprehensive evaluations on technical documents and news stories. Results show that FDI outperforms the baselines in generating highly believable fakes with moderate modification to protect critical information and deceive adversaries.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Controllable Fake Document Infilling for Cyber Deception</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-emnlp.486" target="_blank">https://aclanthology.org/2022.findings-emnlp.486</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level machine translation leverages inter-sentence dependencies to produce more coherent and consistent translations. However these models predominantly based on transformers are difficult to scale to long documents as their attention layers have quadratic complexity in the sequence length. Recent efforts on efficient attention improve scalability but their effect on document translation remains unexplored. In this work we investigate the efficacy of a recent linear attention model by Peng et al. (2021) on document translation and augment it with a sentential gate to promote a recency inductive bias. We evaluate the model on IWSLT 2015 and OpenSubtitles 2018 against the transformer demonstrating substantially increased decoding speed on long sequences with similar or better BLEU scores. We show that sentential gating further improves translation quality on IWSLT.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Modeling Context With Linear Attention for Scalable Document-Level Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-emnlp.515" target="_blank">https://aclanthology.org/2022.findings-emnlp.515</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The recent literature in text classification is biased towards short text sequences (e.g. sentences or paragraphs). In real-world applications multi-page multi-paragraph documents are common and they cannot be efficiently encoded by vanilla Transformer-based models. We compare different Transformer-based Long Document Classification (TrLDC) approaches that aim to mitigate the computational overhead of vanilla transformers to encode much longer text namely sparse attention and hierarchical encoding methods.We examine several aspects of sparse attention (e.g. size of local attention window use of global attention) and hierarchical (e.g. document splitting strategy) transformers on four document classification datasets covering different domains. We observe a clear benefit from being able to process longer text and based on our results we derive practical advice of applying Transformer-based models on long document classification tasks.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Revisiting Transformer-based Models for Long Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-emnlp.534" target="_blank">https://aclanthology.org/2022.findings-emnlp.534</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level relation extraction (DocRE) aims to extract semantic relations among entity pairs in a document. Typical DocRE methods blindly take the full document as input while a subset of the sentences in the document noted as the evidence are often sufficient for humans to predict the relation of an entity pair. In this paper we propose an evidence-enhanced framework Eider that empowers DocRE by efficiently extracting evidence and effectively fusing the extracted evidence in inference. We first jointly train an RE model with a lightweight evidence extraction model which is efficient in both memory and runtime. Empirically even training the evidence model on silver labels constructed by our heuristic rules can lead to better RE performance. We further design a simple yet effective inference process that makes RE predictions on both extracted evidence and the full document then fuses the predictions through a blending layer. This allows Eider to focus on important sentences while still having access to the complete information in the document. Extensive experiments show that Eider outperforms state-of-the-art methods on three benchmark datasets (e.g. by 1.37/1.26 Ign F1/F1 on DocRED).</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Eider: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-acl.23" target="_blank">https://aclanthology.org/2022.findings-acl.23</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Keyphrase extraction (KPE) automatically extracts phrases in a document that provide a concise summary of the core content which benefits downstream information retrieval and NLP tasks. Previous state-of-the-art methods select candidate keyphrases based on the similarity between learned representations of the candidates and the document. They suffer performance degradation on long documents due to discrepancy between sequence lengths which causes mismatch between representations of keyphrase candidates and the document. In this work we propose a novel unsupervised embedding-based KPE approach Masked Document Embedding Rank (MDERank) to address this problem by leveraging a mask strategy and ranking candidates by the similarity between embeddings of the source document and the masked document. We further develop a KPE-oriented BERT (KPEBERT) model by proposing a novel self-supervised contrastive learning method which is more compatible to MDERank than vanilla BERT. Comprehensive evaluations on six KPE benchmarks demonstrate that the proposed MDERank outperforms state-of-the-art unsupervised KPE approach by average 1.80 F1@15 improvement. MDERank further benefits from KPEBERT and overall achieves average 3.53 F1@15 improvement over SIFRank.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MDERank: A Masked Document Embedding Rank Approach for Unsupervised Keyphrase Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-acl.34" target="_blank">https://aclanthology.org/2022.findings-acl.34</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>A common method for extractive multi-document news summarization is to re-formulate it as a single-document summarization problem by concatenating all documents as a single meta-document. However this method neglects the relative importance of documents. We propose a simple approach to reorder the documents according to their relative importance before concatenating and summarizing them. The reordering makes the salient content easier to learn by the summarization model. Experiments show that our approach outperforms previous state-of-the-art methods with more complex architectures.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Read Top News First: A Document Reordering Approach for Multi-Document News Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-acl.51" target="_blank">https://aclanthology.org/2022.findings-acl.51</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Open-domain question answering has been used in a wide range of applications such as web search and enterprise search which usually takes clean texts extracted from various formats of documents (e.g. web pages PDFs or Word documents) as the information source. However designing different text extraction approaches is time-consuming and not scalable. In order to reduce human cost and improve the scalability of QA systems we propose and study an textbfOpen-domain textbfDocument textbfVisual textbfQuestion textbfAnswering (Open-domain DocVQA) task which requires answering questions based on a collection of document images directly instead of only document texts utilizing layouts and visual features additionally. Towards this end we introduce the first Chinese Open-domain DocVQA dataset called textrmDuReader_textrmvis containing about 15K question-answering pairs and 158K document images from the Baidu search engine. There are three main challenges in textrmDuReader_textrmvis: (1) long document understanding (2) noisy texts and (3) multi-span answer extraction. The extensive experiments demonstrate that the dataset is challenging. Additionally we propose a simple approach that incorporates the layout and visual features and the experimental results show the effectiveness of the proposed approach. The dataset and code will be publicly available at https://github.com/baidu/DuReader/tree/master/DuReader-vis.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>textrmDuReader_textrmvis: A Chinese Dataset for Open-domain Document Visual Question Answering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-acl.105" target="_blank">https://aclanthology.org/2022.findings-acl.105</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Event Argument Extraction (EAE) is one of the sub-tasks of event extraction aiming to recognize the role of each entity mention toward a specific event trigger. Despite the success of prior works in sentence-level EAE the document-level setting is less explored. In particular whereas syntactic structures of sentences have been shown to be effective for sentence-level EAE prior document-level EAE models totally ignore syntactic structures for documents. Hence in this work we study the importance of syntactic structures in document-level EAE. Specifically we propose to employ Optimal Transport (OT) to induce structures of documents based on sentence-level syntactic structures and tailored to EAE task. Furthermore we propose a novel regularization technique to explicitly constrain the contributions of unrelated context words in the final prediction for EAE. We perform extensive experiments on the benchmark document-level EAE dataset RAMS that leads to the state-of-the-art performance. Moreover our experiments on the ACE 2005 dataset reveals the effectiveness of the proposed model in the sentence-level EAE by establishing new state-of-the-art results.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Event Argument Extraction via Optimal Transport</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-acl.130" target="_blank">https://aclanthology.org/2022.findings-acl.130</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level Relation Extraction (DocRE) is a more challenging task compared to its sentence-level counterpart. It aims to extract relations from multiple sentences at once. In this paper we propose a semi-supervised framework for DocRE with three novel components. Firstly we use an axial attention module for learning the interdependency among entity-pairs which improves the performance on two-hop relations. Secondly we propose an adaptive focal loss to tackle the class imbalance problem of DocRE. Lastly we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. We conducted experiments on two DocRE datasets. Our model consistently outperforms strong baselines and its performance exceeds the previous SOTA by 1.36 F1 and 1.46 Ign_F1 score on the DocRED leaderboard.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-acl.132" target="_blank">https://aclanthology.org/2022.findings-acl.132</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recent progress in NLP is driven by pretrained models leveraging massive datasets and has predominantly benefited the world&#39;s political and economic superpowers. Technologically underserved languages are left behind because they lack such resources. Hundreds of underserved languages nevertheless have available data sources in the form of interlinear glossed text (IGT) from language documentation efforts. IGT remains underutilized in NLP work perhaps because its annotations are only semi-structured and often language-specific. With this paper we make the case that IGT data can be leveraged successfully provided that target language expertise is available. We specifically advocate for collaboration with documentary linguists. Our paper provides a roadmap for successful projects utilizing IGT data: (1) It is essential to define which NLP tasks can be accomplished with the given IGT data and how these will benefit the speech community. (2) Great care and target language expertise is required when converting the data into structured formats commonly employed in NLP. (3) Task-specific and user-specific evaluation can help to ascertain that the tools which are created benefit the target language speech community. We illustrate each step through a case study on developing a morphological reinflection system for the Tsimchianic language Gitksan.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Dim Wihl Gat Tun: The Case for Linguistic Expertise in NLP for Under-Documented Languages</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-acl.167" target="_blank">https://aclanthology.org/2022.findings-acl.167</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We investigate the exploitation of self-supervised models for two Creole languages with few resources: Gwadloup&#39;eyen and Morisien. Automatic language processing tools are almost non-existent for these two languages. We propose to use about one hour of annotated data to design an automatic speech recognition system for each language. We evaluate how much data is needed to obtain a query-by-example system that is usable by linguists. Moreover our experiments show that multilingual self-supervised models are not necessarily the most efficient for Creole languages.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Speech Recognition and Query By Example for Creole Languages Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-acl.197" target="_blank">https://aclanthology.org/2022.findings-acl.197</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Many populous countries including India are burdened with a considerable backlog of legal cases. Development of automated systems that could process legal documents and augment legal practitioners can mitigate this. However there is a dearth of high-quality corpora that is needed to develop such data-driven systems. The problem gets even more pronounced in the case of low resource languages such as Hindi. In this resource paper we introduce the Hindi Legal Documents Corpus (HLDC) a corpus of more than 900K legal documents in Hindi. Documents are cleaned and structured to enable the development of downstream applications. Further as a use-case for the corpus we introduce the task of bail prediction. We experiment with a battery of models and propose a Multi-Task Learning (MTL) based model for the same. MTL models use summarization as an auxiliary task along with bail prediction as the main task. Experiments with different models are indicative of the need for further research in this area.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>HLDC: Hindi Legal Documents Corpus</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-acl.278" target="_blank">https://aclanthology.org/2022.findings-acl.278</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper does not aim at introducing a novel model for document-level neural machine translation. Instead we head back to the original Transformer model and hope to answer the following question: Is the capacity of current models strong enough for document-level translation? Interestingly we observe that the original Transformer with appropriate training techniques can achieve strong results for document translation even with a length of 2000 words. We evaluate this model and several recent approaches on nine document-level datasets and two sentence-level datasets across six languages. Experiments show that document-level Transformer models outperforms sentence-level ones and many previous methods in a comprehensive set of metrics including BLEU four lexical indices three newly proposed assistant linguistic indicators and human evaluation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Rethinking Document-level Neural Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-acl.279" target="_blank">https://aclanthology.org/2022.findings-acl.279</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>State-of-the-art neural models typically encode document-query pairs using cross-attention for re-ranking. To this end models generally utilize an encoder-only (like BERT) paradigm or an encoder-decoder (like T5) approach. These paradigms however are not without flaws i.e. running the model on all query-document pairs at inference-time incurs a significant computational cost. This paper proposes a new training and inference paradigm for re-ranking. We propose to finetune a pretrained encoder-decoder model using in the form of document to query generation. Subsequently we show that this encoder-decoder architecture can be decomposed into a decoder-only language model during inference. This results in significant inference time speedups since the decoder-only architecture only needs to learn to interpret static encoder embeddings during inference. Our experiments show that this new paradigm achieves results that are comparable to the more expensive cross-attention ranking approaches while being up to 6.8X faster. We believe this work paves the way for more efficient neural rankers that leverage large pretrained models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ED2LM: Encoder-Decoder to Language Model for Faster Document Re-ranking Inference</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-acl.295" target="_blank">https://aclanthology.org/2022.findings-acl.295</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Entity recognition is a fundamental task in understanding document images. Traditional sequence labeling frameworks treat the entity types as class IDs and rely on extensive data and high-quality annotations to learn semantics which are typically expensive in practice. In this paper we aim to build an entity recognition model requiring only a few shots of annotated document images. To overcome the data limitation we propose to leverage the label surface names to better inform the model of the target entity type semantics and also embed the labels into the spatial embedding space to capture the spatial correspondence between regions and labels. Specifically we go beyond sequence labeling and develop a novel label-aware seq2seq framework LASER. The proposed model follows a new labeling scheme that generates the label surface names word-by-word explicitly after generating the entities. During training LASER refines the label semantics by updating the label surface name representations and also strengthens the label-region correlation. In this way LASER recognizes the entities from document images through both semantic and layout correspondence. Extensive experiments on two benchmark datasets demonstrate the superiority of LASER under the few-shot setting.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards Few-shot Entity Recognition in Document Images: A Label-aware Sequence-to-Sequence Framework</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-acl.329" target="_blank">https://aclanthology.org/2022.findings-acl.329</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Long document (e.g. scientific papers) summarization is obtaining more and more attention in recent years. Extractive approaches attempt to choose salient sentences via understanding the whole document but long documents cover numerous subjects with varying details and will not ease content understanding. Instead abstractive approaches elaborate to generate related tokens while suffering from truncating the source document due to their input sizes. To this end we propose a Simple yet Effective HYbrid approach which we call SEHY that exploits the discourse information of a document to select salient sections instead sentences for summary generation. On the one hand SEHY avoids the full-text understanding; on the other hand it retains salient information given the length limit. In particular we design two simple strategies for training the extractor: extracting sections incrementally and based on salience-analysis. Then we use strong abstractive models to generate the final summary. We evaluate our approach on a large-scale scientific paper dataset: arXiv. Further we discuss how the disciplinary class (e.g. computer science math or physics) of a scientific paper affects the performance of SEHY as its writing style indicates which is unexplored yet in existing works. Experimental results show the effectiveness of our approach and interesting findings on arXiv and its subsets generated in this paper.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SEHY: A Simple yet Effective Hybrid Model for Summarization of Long Scientific Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-aacl.9" target="_blank">https://aclanthology.org/2022.findings-aacl.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Translation of structured content is an important application of machine translation but the scarcity of evaluation data sets especially for Asian languages limits progress. In this paper we present a novel multilingual multiway evaluation data set for the translation of structured documents of the Asian languages Japanese Korean and Chinese. We describe the data set its creation process and important characteristics followed by establishing and evaluating baselines using the direct translation as well as detag-project approaches. Our data set is well suited for multilingual evaluation and it contains richer annotation tag sets than existing data sets. Our results show that massively multilingual translation models like M2M-100 and mBART-50 perform surprisingly well despite not being explicitly trained to handle structured content. The data set described in this paper and used in our experiments is released publicly.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Multilingual Multiway Evaluation Data Set for Structured Document Translation of Asian Languages</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-aacl.23" target="_blank">https://aclanthology.org/2022.findings-aacl.23</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Most of the existing discourse-level Information Extraction tasks have been modeled to be extractive in nature. However we argue that extracting information from larger bodies of discourse-like documents requires more natural language understanding and reasoning capabilities. In our work we propose the novel task of document-level event argument aggregation which generates consolidated event-arguments at a document-level with minimal loss of information. More specifically we focus on generating precise document-level information frames in a multilingual setting using prompt-based methods. In this paper we show the effectiveness of u prompt-based text generation approach to generate document-level argument spans in a low-resource and zero-shot setting. We also release the first of its kind multilingual event argument aggregation dataset that can be leveraged in other related multilingual text generation tasks as well: https://github.com/DebanjanaKar/ArgGen.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ArgGen: Prompting Text Generation Models for Document-Level Event-Argument Aggregation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-aacl.37" target="_blank">https://aclanthology.org/2022.findings-aacl.37</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Positional encoding plays a key role in Transformer-based architecture which is to indicate and embed token sequential order information. Understanding documents with unreliable reading order information is a real challenge for document Transformer models. This paper proposes a simple and effective positional encoding method learnable sinusoidal positional encoding (LSPE) by building a learnable sinusoidal positional encoding feed-forward network. We apply LSPE to document Transformer models and pretrain them on document datasets. Then we finetune and evaluate the model performance on document understanding tasks in form receipt and invoice domains. Experimental results show our proposed method not only outperforms other baselines but also demonstrates its robustness and stability on handling noisy data with incorrect order information.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Simple yet Effective Learnable Positional Encoding Method for Improving Document Transformer Model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.findings-aacl.42" target="_blank">https://aclanthology.org/2022.findings-aacl.42</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Language documentation encompasses translation typically into the dominant high-resource language in the region where the target language is spoken. To make data accessible to a broader audience additional translation into other high-resource languages might be needed. Working within a project documenting Kotiria we explore the extent to which state-of-the-art machine translation (MT) systems can support this second translation -- in our case from Portuguese to English. This translation task is challenging for multiple reasons: (1) the data is out-of-domain with respect to the MT system&#39;s training data (2) much of the data is conversational (3) existing translations include non-standard and uncommon expressions often reflecting properties of the documented language and (4) the data includes borrowings from other regional languages. Despite these challenges existing MT systems perform at a usable level though there is still room for improvement. We then conduct a qualitative analysis and suggest ways to improve MT between high-resource languages in a language documentation setting.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Machine Translation Between High-resource Languages in a Language Documentation Setting</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.fieldmatters-1.3" target="_blank">https://aclanthology.org/2022.fieldmatters-1.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Nepalese historical legal documents contain a plethora of valuable information on the history of what is today Nepal. An empirical study based on such documents enables a deep understanding of religion and ritual legal practice rulership and many other aspects of the society through time. The aim of the research project `Documents on the History of Religion and Law of Pre-modern Nepal&#39; is to make accessible a text corpus with 18 th to 20 th century documents both through cataloging and digital text editions building a database called Documenta Nepalica. However the lack of interoperability with other resources hampers its seamless integration into broader research contexts. To address this problem we target the modeling of the Documenta Nepalica as Linked Data. This paper presents one module of this larger endeavour: It describes a proof of concept for an ontology for Nepalese toponyms that provides the means to classify toponyms attested in the documents and to model their entanglement with other toponyms persons events and time. The ontology integrates and extends standard ontologies and increases interoperability through aligning the ontology individuals to the respective entries of geographic authority files such as GeoNames. Also we establish a mapping of the individuals to DBpedia entities.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards an Ontology for Toponyms in Nepalese Historical Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.eurali-1.2" target="_blank">https://aclanthology.org/2022.eurali-1.2</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Text segmentation is important for signaling a document&#39;s structure. Without segmenting a long document into topically coherent sections it is difficult for readers to comprehend the text let alone find important information. The problem is only exacerbated by a lack of segmentation in transcripts of audio/video recordings. In this paper we explore the role that section segmentation plays in extractive summarization of written and spoken documents. Our approach learns robust sentence representations by performing summarization and segmentation simultaneously which is further enhanced by an optimization-based regularizer to promote selection of diverse summary sentences. We conduct experiments on multiple datasets ranging from scientific articles to spoken transcripts to evaluate the model&#39;s performance. Our findings suggest that the model can not only achieve state-of-the-art performance on publicly available benchmarks but demonstrate better cross-genre transferability when equipped with text segmentation. We perform a series of analyses to quantify the impact of section segmentation on summarizing written and spoken documents of substantial length and complexity.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Toward Unifying Text Segmentation and Long Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.8" target="_blank">https://aclanthology.org/2022.emnlp-main.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present DocInfer - a novel end-to-end Document-level Natural Language Inference model that builds a hierarchical document graph enriched through inter-sentence relations (topical entity-based concept-based) performs paragraph pruning using the novel SubGraph Pooling layer followed by optimal evidence selection based on REINFORCE algorithm to identify the most important context sentences for a given hypothesis. Our evidence selection mechanism allows it to transcend the input length limitation of modern BERT-like Transformer models while presenting the entire evidence together for inferential reasoning. We show this is an important property needed to reason on large documents where the evidence may be fragmented and located arbitrarily far from each other. Extensive experiments on popular corpora - DocNLI ContractNLI and ConTRoL datasets and our new proposed dataset called CaseHoldNLI on the task of legal judicial reasoning demonstrate significant performance gains of 8-12% over SOTA methods. Our ablation studies validate the impact of our model. Performance improvement of 3-6% on annotation-scarce downstream tasks of fact verification multiple-choice QA and contract clause retrieval demonstrates the usefulness of DocInfer beyond primary NLI tasks.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DocInfer: Document-level Natural Language Inference using Optimal Evidence Selection</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.51" target="_blank">https://aclanthology.org/2022.emnlp-main.51</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Efficient document retrieval heavily relies on the technique of semantic hashing which learns a binary code for every document and employs Hamming distance to evaluate document distances. However existing semantic hashing methods are mostly established on outdated TFIDF features which obviously do not contain lots of important semantic information about documents. Furthermore the Hamming distance can only be equal to one of several integer values significantly limiting its representational ability for document distances. To address these issues in this paper we propose to leverage BERT embeddings to perform efficient retrieval based on the product quantization technique which will assign for every document a real-valued codeword from the codebook instead of a binary code as in semantic hashing. Specifically we first transform the original BERT embeddings via a learnable mapping and feed the transformed embedding into a probabilistic product quantization module to output the assigned codeword. The refining and quantizing modules can be optimized in an end-to-end manner by minimizing the probabilistic contrastive loss. A mutual information maximization based method is further proposed to improve the representativeness of codewords so that documents can be quantized more accurately. Extensive experiments conducted on three benchmarks demonstrate that our proposed method significantly outperforms current state-of-the-art baselines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Efficient Document Retrieval by End-to-End Refining and Quantizing BERT Embedding with Contrastive Product Quantization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.54" target="_blank">https://aclanthology.org/2022.emnlp-main.54</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The task of Cross-document Coreference Resolution has been traditionally formulated as requiring to identify all coreference links across a given set of documents. We propose an appealing and often more applicable complementary set up for the task -- Cross-document Coreference Search focusing in this paper on event coreference. Concretely given a mention in context of an event of interest considered as a query the task is to find all coreferring mentions for the query event in a large document collection. To support research on this task we create a corresponding dataset which is derived from Wikipedia while leveraging annotations in the available Wikipedia Event Coreferecene dataset (WEC-Eng). Observing that the coreference search setup is largely analogous to the setting of Open Domain Question Answering we adapt the prominent Deep Passage Retrieval (DPR) model to our setting as an appealing baseline. Finally we present a novel model that integrates a powerful coreference scoring scheme into the DPR architecture yielding improved performance.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-document Event Coreference Search: Task Dataset and Modeling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.58" target="_blank">https://aclanthology.org/2022.emnlp-main.58</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Summarization datasets are often assembled either by scraping naturally occurring public-domain summaries---which are nearly always in difficult-to-work-with technical domains---or by using approximate heuristics to extract them from everyday text---which frequently yields unfaithful summaries. In this work we turn to a slower but more straightforward approach to developing summarization benchmark data: We hire highly-qualified contractors to read stories and write original summaries from scratch. To amortize reading time we collect five summaries per document with the first giving an overview and the subsequent four addressing specific questions. We use this protocol to collect SQuALITY a dataset of question-focused summaries built on the same public-domain short stories as the multiple-choice dataset QuALITY (Pang et al. 2021). Experiments with state-of-the-art summarization systems show that our dataset is challenging and that existing automatic evaluation metrics are weak indicators of quality.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SQuALITY: Building a Long-Document Summarization Dataset the Hard Way</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.75" target="_blank">https://aclanthology.org/2022.emnlp-main.75</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Multi-document reading comprehension task requires collecting evidences from different documents for answering questions. Previous research works either use the extractive modeling method to naively integrate the scores from different documents on the encoder side or use the generative modeling method to collect the clues from different documents on the decoder side individually. However any single modeling method cannot make full of the advantages of both. In this work we propose a novel method that tries to employ a multi-view fusion and multi-decoding mechanism to achieve it. For one thing our approach leverages question-centered fusion mechanism and cross-attention mechanism to gather fine-grained fusion of evidence clues from different documents in the encoder and decoder concurrently. For another our method simultaneously employs both the extractive decoding approach and the generative decoding method to effectively guide the training process. Compared with existing methods our method can perform both extractive decoding and generative decoding independently and optionally. Our experiments on two mainstream multi-document reading comprehension datasets (Natural Questions and TriviaQA) demonstrate that our method can provide consistent improvements over previous state-of-the-art methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>M3: A Multi-View Fusion and Multi-Decoding Network for Multi-Document Reading Comprehension</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.94" target="_blank">https://aclanthology.org/2022.emnlp-main.94</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Synthesizing datasets for conversational question answering (CQA) from unlabeled documents remains challenging due to its interactive nature.Moreover while modeling information needs is an essential key only few studies have discussed it.In this paper we introduce a novel framework **SimSeek** (**Sim**ulating information-**Seek**ing conversation from unlabeled documents) and compare its two variants.In our baseline **SimSeek-sym** a questioner generates follow-up questions upon the predetermined answer by an answerer.On the contrary **SimSeek-asym** first generates the question and then finds its corresponding answer under the conversational context.Our experiments show that they can synthesize effective training resources for CQA and conversational search tasks.As a result conversations from **SimSeek-asym** not only make more improvements in our experiments but also are favorably reviewed in a human evaluation.We finally release a large-scale resource of synthetic conversations **Wiki-SimSeek** containing 2 million CQA pairs built upon Wikipedia documents.With the dataset our CQA model achieves the state-of-the-art performance on a recent CQA benchmark QuAC.The code and dataset are available at https://github.com/naver-ai/simseek</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Generating Information-Seeking Conversations from Unlabeled Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.151" target="_blank">https://aclanthology.org/2022.emnlp-main.151</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level natural language inference (DOCNLI) is a new challenging task in natural language processing aiming at judging the entailment relationship between a pair of hypothesis and premise documents. Current datasets and baselines largely follow sentence-level settings but fail to address the issues raised by longer documents. In this paper we establish a general solution named Retrieval Reading and Fusion (R2F) framework and a new setting by analyzing the main challenges of DOCNLI: interpretability long-range dependency and cross-sentence inference. The basic idea of the framework is to simplify document-level task into a set of sentence-level tasks and improve both performance and interpretability with the power of evidence. For each hypothesis sentence the framework retrieves evidence sentences from the premise and reads to estimate its credibility. Then the sentence-level results are fused to judge the relationship between the documents. For the setting we contribute complementary evidence and entailment label annotation on hypothesis sentences for interpretability study. Our experimental results show that R2F framework can obtain state-of-the-art performance and is robust for diverse evidence retrieval methods. Moreover it can give more interpretable prediction results. Our model and code are released at https://github.com/phoenixsecularbird/R2F.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>R2F: A General Retrieval Reading and Fusion Framework for Document-level Natural Language Inference</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.204" target="_blank">https://aclanthology.org/2022.emnlp-main.204</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document images are a ubiquitous source of data where the text is organized in a complex hierarchical structure ranging from fine granularity (e.g. words) medium granularity (e.g. regions such as paragraphs or figures) to coarse granularity (e.g. the whole page). The spatial hierarchical relationships between content at different levels of granularity are crucial for document image understanding tasks. Existing methods learn features from either word-level or region-level but fail to consider both simultaneously. Word-level models are restricted by the fact that they originate from pure-text language models which only encode the word-level context. In contrast region-level models attempt to encode regions corresponding to paragraphs or text blocks into a single embedding but they perform worse with additional word-level features. To deal with these issues we propose MGDoc a new multi-modal multi-granular pre-training framework that encodes page-level region-level and word-level information at the same time. MGDoc uses a unified text-visual encoder to obtain multi-modal features across different granularities which makes it possible to project the multi-granular features into the same hyperspace. To model the region-word correlation we design a cross-granular attention mechanism and specific pre-training tasks for our model to reinforce the model of learning the hierarchy between regions and words. Experiments demonstrate that our proposed model can learn better features that perform well across granularities and lead to improvements in downstream tasks.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MGDoc: Pre-training with Multi-granular Hierarchy for Document Image Understanding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.265" target="_blank">https://aclanthology.org/2022.emnlp-main.265</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level relation extraction (RE) aims to identify relations between entities across multiple sentences. Most previous methods focused on document-level RE under full supervision. However in real-world scenario it is expensive and difficult to completely label all relations in a document because the number of entity pairs in document-level RE grows quadratically with the number of entities. To solve the common incomplete labeling problem we propose a unified positive-unlabeled learning framework - shift and squared ranking loss positive-unlabeled (SSR-PU) learning. We use positive-unlabeled (PU) learning on document-level RE for the first time. Considering that labeled data of a dataset may lead to prior shift of unlabeled data we introduce a PU learning under prior shift of training data. Also using none-class score as an adaptive threshold we propose squared ranking loss and prove its Bayesian consistency with multi-label ranking metrics. Extensive experiments demonstrate that our method achieves an improvement of about 14 F1 points relative to the previous baseline with incomplete labeling. In addition it outperforms previous state-of-the-art results under both fully supervised and extremely unlabeled settings as well.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Unified Positive-Unlabeled Learning Framework for Document-Level Relation Extraction with Different Levels of Labeling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.276" target="_blank">https://aclanthology.org/2022.emnlp-main.276</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Long document question answering is a challenging task due to its demands for complex reasoning over long text. Previous works usually take long documents as non-structured flat texts or only consider the local structure in long documents. However these methods usually ignore the global structure of the long document which is essential for long-range understanding. To tackle this problem we propose Compressive Graph Selector Network (CGSN) to capture the global structure in a compressive and iterative manner. The proposed model mainly focuses on the evidence selection phase of long document question answering. Specifically it consists of three modules: local graph network global graph network and evidence memory network. Firstly the local graph network builds the graph structure of the chunked segment in token sentence paragraph and segment levels to capture the short-term dependency of the text. Secondly the global graph network selectively receives the information of each level from the local graph compresses them into the global graph nodes and applies graph attention to the global graph nodes to build the long-range reasoning over the entire text in an iterative way. Thirdly the evidence memory network is designed to alleviate the redundancy problem in the evidence selection by saving the selected result in the previous steps. Extensive experiments show that the proposed model outperforms previous methods on two datasets.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Capturing Global Structural Information in Long Document Question Answering with Compressive Graph Selector Network</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.336" target="_blank">https://aclanthology.org/2022.emnlp-main.336</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Sequential abstractive neural summarizers often do not use the underlying structure in the input article or dependencies between the input sentences. This structure is essential to integrate and consolidate information from different parts of the text. To address this shortcoming we propose a hierarchy-aware graph neural network (HierGNN) which captures such dependencies through three main steps: 1) learning a hierarchical document structure through a latent structure tree learned by a sparse matrix-tree computation; 2) propagating sentence information over this structure using a novel message-passing node propagation mechanism to identify salient information; 3) using graph-level attention to concentrate the decoder on salient information. Experiments confirm HierGNN improves strong sequence models such as BART with a 0.55 and 0.75 margin in average ROUGE-1/2/L for CNN/DM and XSum. Further human evaluation demonstrates that summaries produced by our model are more relevant and less redundant than the baselines into which HierGNN is incorporated. We also find HierGNN synthesizes summaries by fusing multiple source sentences more rather than compressing a single source sentence and that it processes long inputs more effectively.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Abstractive Summarization Guided by Latent Hierarchical Document Structure</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.355" target="_blank">https://aclanthology.org/2022.emnlp-main.355</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The task of multi-document summarization (MDS) aims at models that given multiple documents as input are able to generate a summary that combines disperse information originally spread __across__ these documents. Accordingly it is expected that both reference summaries in MDS datasets as well as system summaries would indeed be based on such dispersed information. In this paper we argue for quantifying and assessing this expectation. To that end we propose an automated measure for evaluating the degree to which a summary is ``disperse&#39;&#39; in the sense of the number of source documents needed to cover its content. We apply our measure to empirically analyze several popular MDS datasets with respect to their reference summaries as well as the output of state-of-the-art systems. Our results show that certain MDS datasets barely require combining information from multiple documents where a single document often covers the full summary content. Overall we advocate using our metric for assessing and improving the degree to which summarization datasets require combining multi-document information and similarly how summarization models actually meet this challenge.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>How ``Multi&#39;&#39; is Multi-Document Summarization?</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.389" target="_blank">https://aclanthology.org/2022.emnlp-main.389</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>A key component of fact verification is the evidence retrieval often from multiple documents. Recent approaches use dense representations and condition the retrieval of each document on the previously retrieved ones. The latter step is performed over all the documents in the collection requiring storing their dense representations in an index thus incurring a high memory footprint. An alternative paradigm is retrieve-and-rerank where documents are retrieved using methods such as BM25 their sentences are reranked and further documents are retrieved conditioned on these sentences reducing the memory requirements. However such approaches can be brittle as they rely on heuristics and assume hyperlinks between documents.We propose a novel retrieve-and-rerank method for multi-hop retrieval that consists of a retriever that jointly scores documents in the knowledge source and sentences from previously retrieved documents using an autoregressive formulation and is guided by a proof system based on natural logic that dynamically terminates the retrieval process if the evidence is deemed sufficient.This method exceeds or is on par with the current state-of-the-art on FEVER HoVer and FEVEROUS-S while using 5 to 10 times less memory than competing systems. Evaluation on an adversarial dataset indicates improved stability of our approach compared to commonly deployed threshold-based methods. Finally the proof system helps humans predict model decisions correctly more often than using the evidence alone.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Natural Logic-guided Autoregressive Multi-hop Document Retrieval for Fact Verification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.411" target="_blank">https://aclanthology.org/2022.emnlp-main.411</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we aim to relieve the issue of lexical translation inconsistency for document-level neural machine translation (NMT) by modeling consistency preference for lexical chains which consist of repeated words in a source-side document and provide a representation of the lexical consistency structure of the document. Specifically we first propose lexical-consistency attention to capture consistency context among words in the same lexical chains. Then for each lexical chain we define and learn a consistency-tailored latent variable which will guide the translation of corresponding sentences to enhance lexical translation consistency. Experimental results on Chinese→English and French→English document-level translation tasks show that our approach not only significantly improves translation performance in BLEU but also substantially alleviates the problem of the lexical translation inconsistency.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Modeling Consistency Preference via Lexical Chains for Document-level Neural Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.424" target="_blank">https://aclanthology.org/2022.emnlp-main.424</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We argue that disentangling content selection from the budget used to cover salient content improves the performance and applicability of abstractive summarizers. Our method FactorSum does this disentanglement by factorizing summarization into two steps through an energy function: (1) generation of abstractive summary views covering salient information in subsets of the input document (document views); (2) combination of these views into a final summary following a budget and content guidance. This guidance may come from different sources including from an advisor model such as BART or BigBird or in oracle mode -- from the reference. This factorization achieves significantly higher ROUGE scores on multiple benchmarks for long document summarization namely PubMed arXiv and GovReport. Most notably our model is effective for domain adaptation. When trained only on PubMed samples it achieves a 46.29 ROUGE-1 score on arXiv outperforming PEGASUS trained in domain by a large margin. Our experimental results indicate that the performance gains are due to more flexible budget adaptation and processing of shorter contexts provided by partial document views.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Factorizing Content and Budget Decisions in Abstractive Summarization of Long Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.426" target="_blank">https://aclanthology.org/2022.emnlp-main.426</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Event coreference resolution (ECR) aims to cluster event mentions that refer to the same real-world events. Deep learning methods have achieved SOTA results on the ECR task. However due to the encoding length limitation previous methods either adopt classical pairwise models based on sentence-level context or split each document into multiple chunks and encode them separately. They failed to capture the interactions and contextual cues among those long-distance event mentions. Besides high-level information such as event topics is rarely considered to enhance representation learning for ECR. To address the above two issues we first apply a Longformer-based encoder to obtain the document-level embeddings and an encoder with a trigger-mask mechanism to learn sentence-level embeddings based on local context. In addition we propose an event topic generator to infer the latent topic-level representations. Finally using the above event embeddings we employ a multiple tensor matching method to capture their interactions at the document sentence and topic levels. Experimental results on the KBP 2017 dataset show that our model outperforms the SOTA baselines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Event Coreference Resolution Using Document-level and Topic-level Information</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.454" target="_blank">https://aclanthology.org/2022.emnlp-main.454</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level relation extraction (RE) aims to extract the relations between entities from the input document that usually containing many difficultly-predicted entity pairs whose relations can only be predicted through relational inference. Existing methods usually directly predict the relations of all entity pairs of input document in a one-pass manner ignoring the fact that predictions of some entity pairs heavily depend on the predicted results of other pairs. To deal with this issue in this paper we propose a novel document-level RE model with iterative inference. Our model is mainly composed of two modules: 1) a base module expected to provide preliminary relation predictions on entity pairs; 2) an inference module introduced to refine these preliminary predictions by iteratively dealing with difficultly-predicted entity pairs depending on other pairs in an easy-to-hard manner. Unlike previous methods which only consider feature information of entity pairs our inference module is equipped with two Extended Cross Attention units allowing it to exploit both feature information and previous predictions of entity pairs during relational inference. Furthermore we adopt a two-stage strategy to train our model. At the first stage we only train our base module. During the second stage we train the whole model where contrastive learning is introduced to enhance the training of inference module. Experimental results on three commonly-used datasets show that our model consistently outperforms other competitive baselines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards Better Document-level Relation Extraction via Iterative Inference</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.568" target="_blank">https://aclanthology.org/2022.emnlp-main.568</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Automatic topic classification has been studied extensively to assist managing and indexing scientific documents in a digital collection. With the large number of topics being available in recent years it has become necessary to arrange them in a hierarchy. Therefore the automatic classification systems need to be able to classify the documents hierarchically. In addition each paper is often assigned to more than one relevant topic. For example a paper can be assigned to several topics in a hierarchy tree. In this paper we introduce a new dataset for hierarchical multi-label text classification (HMLTC) of scientific papers called SciHTC which contains 186160 papers and 1234 categories from the ACM CCS tree. We establish strong baselines for HMLTC and propose a multi-task learning approach for topic classification with keyword labeling as an auxiliary task. Our best model achieves a Macro-F1 score of 34.57% which shows that this dataset provides significant research opportunities on hierarchical scientific topic classification. We make our dataset and code for all experiments publicly available.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Hierarchical Multi-Label Classification of Scientific Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.610" target="_blank">https://aclanthology.org/2022.emnlp-main.610</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Pairing a lexical retriever with a neural re-ranking model has set state-of-the-art performance on large-scale information retrieval datasets. This pipeline covers scenarios like question answering or navigational queries however for information-seeking scenarios users often provide information on whether a document is relevant to their query in form of clicks or explicit feedback. Therefore in this work we explore how relevance feedback can be directly integrated into neural re-ranking models by adopting few-shot and parameter-efficient learning techniques. Specifically we introduce a kNN approach that re-ranks documents based on their similarity with the query and the documents the user considers relevant. Further we explore Cross-Encoder models that we pre-train using meta-learning and subsequently fine-tune for each query training only on the feedback documents. To evaluate our different integration strategies we transform four existing information retrieval datasets into the relevance feedback scenario. Extensive experiments demonstrate that integrating relevance feedback directly in neural re-ranking models improves their performance and fusing lexical ranking with our best performing neural re-ranker outperforms all other methods by 5.2% nDCG@20.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Incorporating Relevance Feedback for Information-Seeking Retrieval using Few-Shot Document Re-Ranking</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.614" target="_blank">https://aclanthology.org/2022.emnlp-main.614</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This work describes the first thorough analysis of ``header&#39;&#39; signs in proto-Elamite an undeciphered script from 3100-2900 BCE. Headers are a category of signs which have been provisionally identified through painstaking manual analysis of this script by domain experts. We use unsupervised neural and statistical sequence modeling techniques to provide new and independent evidence for the existence of headers without supervision from domain experts. Having affirmed the existence of headers as a legitimate structural feature we next arrive at a richer understanding of their possible meaning and purpose by (i) examining which features predict their presence; (ii) identifying correlations between these features and other document properties; and (iii) examining cases where these features predict the presence of a header in texts where domain experts do not expect one (or vice versa). We provide more concrete processes for labeling headers in this corpus and a clearer justification for existing intuitions about document structure in proto-Elamite.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Sequence Models for Document Structure Identification in an Undeciphered Script</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.620" target="_blank">https://aclanthology.org/2022.emnlp-main.620</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Building pretrained language models is considered expensive and data-intensive but must we increase dataset size to achieve better performance? We propose an alternative to larger training sets by automatically identifying smaller yet domain-representative subsets. We extend Cynical Data Selection a statistical sentence scoring method that conditions on a representative target domain corpus. As an example we treat the OntoNotes corpus as a target domain and pretrain a RoBERTa-like encoder from a cynically selected subset of the Pile. On both perplexity and across several downstream tasks in the target domain it consistently outperforms random selection with 20x less data 3x fewer training iterations and 2x less estimated cloud compute cost validating the recipe of automatic document selection for LM pretraining.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Document Selection for Efficient Encoder Pretraining</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.647" target="_blank">https://aclanthology.org/2022.emnlp-main.647</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Relation Extraction (RE) is a fundamental task of information extraction which has attracted a large amount of research attention. Previous studies focus on extracting the relations within a sentence or document while currently researchers begin to explore cross-document RE. However current cross-document RE methods directly utilize text snippets surrounding target entities in multiple given documents which brings considerable noisy and non-relevant sentences. Moreover they utilize all the text paths in a document bag in a coarse-grained way without considering the connections between these text paths.In this paper we aim to address both of these shortages and push the state-of-the-art for cross-document RE. First we focus on input construction for our RE model and propose an entity-based document-context filter to retain useful information in the given documents by using the bridge entities in the text paths. Second we propose a cross-document RE model based on cross-path entity relation attention which allow the entity relations across text paths to interact with each other. We compare our cross-document RE method with the state-of-the-art methods in the dataset CodRED. Our method outperforms them by at least 10% in F1 thus demonstrating its effectiveness.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Entity-centered Cross-document Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.671" target="_blank">https://aclanthology.org/2022.emnlp-main.671</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Literary translation is a culturally significant task but it is bottlenecked by the small number of qualified literary translators relative to the many untranslated works published around the world. Machine translation (MT) holds potential to complement the work of human translators by improving both training procedures and their overall efficiency. Literary translation is less constrained than more traditional MT settings since translators must balance meaning equivalence readability and critical interpretability in the target language. This property along with the complex discourse-level context present in literary texts also makes literary MT more challenging to computationally model and evaluate. To explore this task we collect a dataset (Par3) of non-English language novels in the public domain each aligned at the paragraph level to both human and automatic English translations. Using Par3 we discover that expert literary translators prefer reference human translations over machine-translated paragraphs at a rate of 84% while state-of-the-art automatic MT metrics do not correlate with those preferences. The experts note that MT outputs contain not only mistranslations but also discourse-disrupting errors and stylistic inconsistencies. To address these problems we train a post-editing model whose output is preferred over normal MT output at a rate of 69% by experts. We publicly release Par3 to spur future research into literary MT.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploring Document-Level Literary Machine Translation with Parallel Paragraphs from World Literature</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.672" target="_blank">https://aclanthology.org/2022.emnlp-main.672</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Extractive summarization for long documents is challenging due to the extended structured input context. The long-distance sentence dependency hinders cross-sentence relations modeling the critical step of extractive summarization. This paper proposes HEGEL a hypergraph neural network for long document summarization by capturing high-order cross-sentence relations. HEGEL updates and learns effective sentence representations with hypergraph transformer layers and fuses different types of sentence dependencies including latent topics keywords coreference and section structure. We validate HEGEL by conducting extensive experiments on two benchmark datasets and experimental results demonstrate the effectiveness and efficiency of HEGEL.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>HEGEL: Hypergraph Transformer for Long Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.692" target="_blank">https://aclanthology.org/2022.emnlp-main.692</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level relation extraction (DocRE) aims at extracting relations of all entity pairs in a document. A key challenge to DocRE lies in the complex interdependency between the relations of entity pairs. Unlike most prior efforts focusing on implicitly powerful representations the recently proposed LogiRE (Ru et al. 2021) explicitly captures the interdependency by learning logical rules. However LogiRE requires extra parameterized modules to reason merely after training backbones and this disjointed optimization of backbones and extra modules may lead to sub-optimal results. In this paper we propose MILR a logic enhanced framework that boosts DocRE by Mining and Injecting Logical Rules. MILR first mines logical rules from annotations based on frequencies. Then in training consistency regularizationis leveraged as an auxiliary loss to penalize instances that violate mined rules. Finally MILR infers from a global perspective based on integer programming. Compared with LogiRE MILR does not introduce extra parameters and injects logical rules during both training and inference. Extensive experiments on two benchmarks demonstrate that MILR not only improves the relation extraction performance (1.1%-3.8% F1) but also makes predictions more logically consistent (over 4.5% Logic). More importantly MILR also consistently outperforms LogiRE on both counts. Code is available at https://github.com/XingYing-stack/MILR.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Boosting Document-Level Relation Extraction by Mining and Injecting Logical Rules</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.704" target="_blank">https://aclanthology.org/2022.emnlp-main.704</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Contrastive learning has been the dominant approach to training dense retrieval models. In this work we investigate the impact of ranking context - an often overlooked aspect of learning dense retrieval models. In particular we examine the effect of its constituent parts: jointly scoring a large number of negatives per query using retrieved (query-specific) instead of random negatives and a fully list-wise loss.To incorporate these factors into training we introduce Contextual Document Embedding Reranking (CODER) a highly efficient retrieval framework. When reranking it incurs only a negligible computational overhead on top of a first-stage method at run time (approx. 5 ms delay per query) allowing it to be easily combined with any state-of-the-art dual encoder method. Models trained through CODER can also be used as stand-alone retrievers.Evaluating CODER in a large set of experiments on the MS MARCO and TripClick collections we show that the contextual reranking of precomputed document embeddings leads to a significant improvement in retrieval performance. This improvement becomes even more pronounced when more relevance information per query is available shown in the TripClick collection where we establish new state-of-the-art results by a large margin.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CODER: An efficient framework for improving retrieval through COntextual Document Embedding Reranking</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.727" target="_blank">https://aclanthology.org/2022.emnlp-main.727</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper studies how to enhance the document representation for the bi-encoder approach in dense document retrieval. The bi-encoder separately encoding a query and a document as a single vector is favored for high efficiency in large-scale information retrieval compared to more effective but complex architectures. To combine the strength of the two the multi-vector representation of documents for bi-encoder such as ColBERT preserving all token embeddings has been widely adopted. Our contribution is to reduce the size of the multi-vector representation without compromising the effectiveness supervised by query logs. Our proposed solution decreases the latency and the memory footprint up to 8- and 3-fold validated on MSMARCO and real-world search query logs.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Pseudo-Relevance for Enhancing Document Representation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.800" target="_blank">https://aclanthology.org/2022.emnlp-main.800</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Learning scientific document representations can be substantially improved through contrastive learning objectives where the challenge lies in creating positive and negative training samples that encode the desired similarity semantics. Prior work relies on discrete citation relations to generate contrast samples. However discrete citations enforce a hard cut-off to similarity. This is counter-intuitive to similarity-based learning and ignores that scientific papers can be very similar despite lacking a direct citation - a core problem of finding related research. Instead we use controlled nearest neighbor sampling over citation graph embeddings for contrastive learning. This control allows us to learn continuous similarity to sample hard-to-learn negatives and positives and also to avoid collisions between negative and positive samples by controlling the sampling margin between them. The resulting method SciNCL outperforms the state-of-the-art on the SciDocs benchmark. Furthermore we demonstrate that it can train (or tune) language models sample-efficiently and that it can be combined with recent training-efficient methods. Perhaps surprisingly even training a general-domain language model this way outperforms baselines pretrained in-domain.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.802" target="_blank">https://aclanthology.org/2022.emnlp-main.802</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Current abstractive summarization systems tend to hallucinate content that is unfaithful to the source document posing a risk of misinformation. To mitigate hallucination we must teach the model to distinguish hallucinated summaries from faithful ones. However the commonly used maximum likelihood training does not disentangle factual errors from other model errors. To address this issuewe propose a back-translation-style approach to augment negative samples that mimic factual errors made by the model. Specifically we train an elaboration model that generates hallucinated documents given the reference summaries and then generates negative summaries from the fake documents. We incorporate the negative samples into training through a controlled generator which produces faithful/unfaithful summaries conditioned on the control codes. Additionally we find that adding textual entailment data through multitasking further boosts the performance. Experiments on three datasets (XSum Gigaword and WikiHow) show that our method consistently improves faithfulness without sacrificing informativeness according to both human and automatic evaluation</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Faithfulness by Augmenting Negative Summaries from Fake Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-main.816" target="_blank">https://aclanthology.org/2022.emnlp-main.816</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Although contextualized word embeddings have led to great improvements in automatic language understanding their potential for practical applications in document exploration and visualization has been little explored. Common visualization techniques used for e.g. model analysis usually provide simple scatter plots of token-level embeddings that do not provide insight into their contextual use. In this work we propose KeywordScape a visual exploration tool that allows to overview summarize and explore the semantic content of documents based on their keywords. While existing keyword-based exploration tools assume that keywords have static meanings our tool represents keywords in terms of their contextualized embeddings. Our application visualizes these embeddings in a semantic landscape that represents keywords as islands on a spherical map. This keeps keywords with similar context close to each other allowing for a more precise search and comparison of documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>KeywordScape: Visual Document Exploration using Contextualized Keyword Embeddings</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-demos.14" target="_blank">https://aclanthology.org/2022.emnlp-demos.14</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Systems that automatically define unfamiliar terms hold the promise of improving the accessibility of scientific texts especially for readers who may lack prerequisite background knowledge. However current systems assume a single ``best&#39;&#39; description per concept which fails to account for the many ways a concept can be described. We present ACCoRD an end-to-end system tackling the novel task of generating sets of descriptions of scientific concepts. Our system takes advantage of the myriad ways a concept is mentioned across the scientific literature to produce distinct diverse descriptions oftarget concepts in terms of different reference concepts. In a user study we find that users prefer (1) descriptions produced by our end-to-end system and (2) multiple descriptions to a single ``best&#39;&#39; description. We release the ACCoRD corpus which includes 1275 labeled contexts and 1787 expert-authored concept descriptions to support research on our task.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ACCoRD: A Multi-Document Approach to Generating Diverse Descriptions of Scientific Concepts</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.emnlp-demos.20" target="_blank">https://aclanthology.org/2022.emnlp-demos.20</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents the results of the DELA Project. We describe the testing of context span for document-level evaluation construction of a document-level corpus and context position as well as the latest developments of the project when looking at human and automatic evaluation metrics for document-level evaluation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DELA Project: Document-level Machine Translation Evaluation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.eamt-1.50" target="_blank">https://aclanthology.org/2022.eamt-1.50</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.dialdoc-1.0" target="_blank">https://aclanthology.org/2022.dialdoc-1.0</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Goal-oriented dialogues generation grounded in multiple documents(MultiDoc2Dial) is a challenging and realistic task. Unlike previous works which treat document-grounded dialogue modeling as a machine reading comprehension task from single document MultiDoc2Dial task faces challenges of both seeking information from multiple documents and generating conversation response simultaneously. This paper summarizes our entries to agent response generation subtask in MultiDoc2Dial dataset. We propose a three-stage solution Grounding-guided goal-oriented dialogues generation(G4) which predicts groundings from retrieved passages to guide the generation of the final response. Our experiments show that G4 achieves SacreBLEU score of 31.24 and F1 score of 44.6 which is 60.7% higher than the baseline model.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>G4: Grounding-guided Goal-oriented Dialogues Generation with Multiple Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.dialdoc-1.11" target="_blank">https://aclanthology.org/2022.dialdoc-1.11</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This work presents the contribution from the Text-to-Knowledge team of Ghent University (UGent-T2K) to the MultiDoc2Dial shared task on modeling dialogs grounded in multiple documents. We propose a pipeline system comprising (1) document retrieval (2) passage retrieval and (3) response generation. We engineered these individual components mainly by for (1)-(2) combining multiple ranking models and adding a final LambdaMART reranker and for (3) by adopting a Fusion-in-Decoder (FiD) model. We thus significantly boost the baseline system&#39;s performance (over +10 points for both F1 and SacreBLEU). Further error analysis reveals two major failure cases to be addressed in future work: (i) in case of topic shift within the dialog retrieval often fails to select the correct grounding document(s) and (ii) generation sometimes fails to use the correctly retrieved grounding passage. Our code is released at this link.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>UGent-T2K at the 2nd DialDoc Shared Task: A Retrieval-Focused Dialog System Grounded in Multiple Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.dialdoc-1.12" target="_blank">https://aclanthology.org/2022.dialdoc-1.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Question Answering (QA) is a Natural Language Processing (NLP) task that can measure language and semantics understanding ability it requires a system not only to retrieve relevant documents from a large number of articles but also to answer corresponding questions according to documents. However various language styles and sources of human questions and evidence documents form the different embedding semantic spaces which may bring some errors to the downstream QA task. To alleviate these problems we propose a framework for enhancing downstream evidence retrieval by generating evidence aiming at improving the performance of response generation. Specifically we take the pre-training language model as a knowledge base storing documents&#39; information and knowledge into model parameters. With the Child-Tuning approach being designed the knowledge storage and evidence generation avoid catastrophic forgetting for response generation. Extensive experiments carried out on the multi-documents dataset show that the proposed method can improve the final performance which demonstrates the effectiveness of the proposed framework.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Knowledge storage and semantic space alignment Method for Multi-documents dialogue generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.dialdoc-1.14" target="_blank">https://aclanthology.org/2022.dialdoc-1.14</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we mainly discuss about our submission to MultiDoc2Dial task which aims to model the goal-oriented dialogues grounded in multiple documents. The proposed task is split into grounding span prediction and agent response generation. The baseline for the task is the retrieval augmented generation model which consists of a dense passage retrieval model for the retrieval part and the BART model for the generation part. The main challenge of this task is that the system requires a great amount of pre-trained knowledge to generate answers grounded in multiple documents. To overcome this challenge we adopt model pretraining fine-tuning and multi-task learning to enhance our model&#39;s coverage of pretrained knowledge. We experimented with various settings of our method to show the effectiveness of our approaches.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Multiple Documents Grounded Goal-Oriented Dialog Systems via Diverse Knowledge Enhanced Pretrained Language Model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.dialdoc-1.15" target="_blank">https://aclanthology.org/2022.dialdoc-1.15</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Information-seeking dialogue systems including knowledge identification and response generation aim to respond to users with fluent coherent and informative answers based on users&#39; needs. This paper discusses our proposed approach Docalog for the DialDoc-22 (MultiDoc2Dial) shared task. Docalog identifies the most relevant knowledge in the associated document in a multi-document setting. Docalog is a three-stage pipeline consisting of textit(1) a document retriever model (DR. TEIT) textit(2) an answer span prediction model and textit(3) an ultimate span picker deciding on the most likely answer span out of all predicted spans. In the test phase of MultiDoc2Dial 2022 Docalog achieved f1-scores of 36.07% and 28.44% and SacreBLEU scores of 23.70% and 20.52% respectively on the textitMDD-SEEN and textitMDD-UNSEEN folds.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Docalog: Multi-document Dialogue System using Transformer-based Span Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.dialdoc-1.16" target="_blank">https://aclanthology.org/2022.dialdoc-1.16</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The paper presents the results of the Shared Task hosted by the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering co-located at ACL 2022. The primary goal of this Shared Task is to build goal-oriented information-seeking conversation systems that are grounded in the domain documents where each dialogue could correspond to multiple subtasks that are based on different documents. The task is to generate agent responses in natural language given the dialogue and document contexts. There are two task settings and leaderboards based on (1) the same sets of domains (SEEN) and (2) one unseen domain (UNSEEN). There are over 20 teams participating in Dev Phase and 8 teams participating in both Dev and Test Phases. Multiple submissions significantly outperform the baseline. The best-performing system achieves 52.06 F1 and the total of 191.30 on the SEEN task; and 34.65 F1 and the total of 130.79 on the UNSEEN task.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DialDoc 2022 Shared Task: Open-Book Document-grounded Dialogue Modeling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.dialdoc-1.18" target="_blank">https://aclanthology.org/2022.dialdoc-1.18</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Comments are widely used by users in collaborative documents every day. The documents&#39; comments enable collaborative editing and review dynamics transforming each document into a context-sensitive communication channel. Understanding the role of comments in communication dynamics within documents is the first step towards automating their management. In this paper we propose the first ever taxonomy for different types of in-document comments based on analysis of a large scale dataset of public documents from the web. We envision that the next generation of intelligent collaborative document experiences allow interactive creation and consumption of content there We also introduce the components necessary for developing novel tools that automate the handling of comments through natural language interaction with the documents. We identify the commands that users would use to respond to various types of comments. We train machine learning algorithms to recognize the different types of comments and assess their feasibility. We conclude by discussing some of the implications for the design of automatic document management tools.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Handling Comments in Documents through Interactions</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.dialdoc-1.20" target="_blank">https://aclanthology.org/2022.dialdoc-1.20</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper proposes a novel task on commonsense-enhanced task-based dialogue grounded in documents and describes the Task2Dial dataset a novel dataset of document-grounded task-based dialogues where an Information Giver (IG) provides instructions (by consulting a document) to an Information Follower (IF) so that the latter can successfully complete the task. In this unique setting the IF can ask clarification questions which may not be grounded in the underlying document and require commonsense knowledge to be answered. The Task2Dial dataset poses new challenges: (1) its human reference texts show more lexical richness and variation than other document-grounded dialogue datasets; (2) generating from this set requires paraphrasing as instructional responses might have been modified from the underlying document; (3) requires commonsense knowledge since questions might not necessarily be grounded in the document; (4) generating requires planning based on context as task steps need to be provided in order. The Task2Dial dataset contains dialogues with an average 18.15 number of turns and 19.79 tokens per turn as compared to 12.94 and 12 respectively in existing datasets. As such learning from this dataset promises more natural varied and less template-like system utterances.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Task2Dial: A Novel Task and Dataset for Commonsense-enhanced Task-based Dialogue Grounded in Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.dialdoc-1.21" target="_blank">https://aclanthology.org/2022.dialdoc-1.21</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The distant supervision (DS) paradigm has been widely used for relation extraction (RE) to alleviate the need for expensive annotations. However it suffers from noisy labels which leads to worse performance than models trained on human-annotated data even when trained using hundreds of times more data. We present a systematic study on the use of natural language inference (NLI) to improve distantly supervised document-level RE. We apply NLI in three scenarios: (i) as a filter for denoising DS labels (ii) as a filter for model prediction and (iii) as a standalone RE model. Our results show that NLI filtering consistently improves performance reducing the performance gap with a model trained on human-annotated data by 2.3 F1.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Distantly Supervised Document-Level Relation Extraction Through Natural Language Inference</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.deeplo-1.2" target="_blank">https://aclanthology.org/2022.deeplo-1.2</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Text segmentation and extraction from unstructured documents can provide business researchers with a wealth of new information on firms and their behaviors. However the most valuable text is often difficult to extract consistently due to substantial variations in how content can appear from document to document. Thus the most successful way to extract this content has been through costly crowdsourcing and training of manual workers. We propose the Assisted Neural Text Segmentation (ANTS) framework to identify pertinent text in unstructured documents from a small set of labeled examples. ANTS leverages deep learning and transfer learning architectures to empower researchers to identify relevant text with minimal manual coding. Using a real world sample of accounting documents we identify targeted sections 96% of the time using only 5 training examples.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ANTS: A Framework for Retrieval of Text Segments in Unstructured Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.deeplo-1.5" target="_blank">https://aclanthology.org/2022.deeplo-1.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Business documents come in a variety of structures formats and information needs which makes information extraction a challenging task. Due to these variations having a document generic model which can work well across all types of documents for all the use cases seems far-fetched. For document-specific models we would need customized document-specific labels. We introduce DoSA (Document Specific Automated Annotations) which helps annotators in generating initial annotations automatically using our novel bootstrap approach by leveraging document generic datasets and models. These initial annotations can further be reviewed by a human for correctness. An initial document-specific model can be trained and its inference can be used as feedback for generating more automated annotations. These automated annotations can be reviewed by humanin-the-loop for the correctness and a new improved model can be trained using the current model as pre-trained model before going for the next iteration. In this paper our scope is limited to Form like documents due to limited availability of generic annotated datasets but this idea can be extended to a variety of other documents as more datasets are built. An opensource ready-to-use implementation is made available on GitHub.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DoSA : A System to Accelerate Annotations on Business Documents with Human-in-the-Loop</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.dash-1.4" target="_blank">https://aclanthology.org/2022.dash-1.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The Creative Summarization Shared Task at COLING 2022 aspires to generate summaries given long-form texts from creative writing. This paper presents the system architecture and the results of our participation in the Scriptbase track that focuses on generating movie plots given movie scripts. The core innovation in our model employs a two-stage hierarchical architecture for movie script summarization. In the first stage a heuristic extraction method is applied to extract actions and essential dialogues which reduces the average length of input movie scripts by 66% from about 24K to 8K tokens. In the second stage a state-of-the-art encoder-decoder model Longformer-Encoder-Decoder (LED) is trained with effective fine-tuning methods BitFit and NoisyTune. Evaluations on the unseen test set indicate that our system outperforms both zero-shot LED baselines as well as other participants on various automatic metrics and ranks 1st in the Scriptbase track.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Two-Stage Movie Script Summarization: An Efficient Method For Low-Resource Long Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.creativesumm-1.9" target="_blank">https://aclanthology.org/2022.creativesumm-1.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we present baseline results for Event Coreference Resolution (ECR) in Dutch using gold-standard (i.e non-predicted) event mentions. A newly developed benchmark dataset allows us to properly investigate the possibility of creating ECR systems for both within and cross-document coreference. We give an overview of the state of the art for ECR in other languages as well as a detailed overview of existing ECR resources. Afterwards we provide a comparative report on our own dataset. We apply a significant number of approaches that have been shown to attain good results for English ECR including feature-based models monolingual transformer language models and multilingual language models. The best results were obtained using the monolingual BERTje model. Finally results for all models are thoroughly analysed and visualised as to provide insight into the inner workings of ECR and long-distance semantic NLP tasks in general.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Investigating Cross-Document Event Coreference for Dutch</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.crac-1.9" target="_blank">https://aclanthology.org/2022.crac-1.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>During the COVID-19 pandemic the spread of misinformation on online social media has grown exponentially. Unverified bogus claims on these platforms regularly mislead people leading them to believe in half-baked truths. The current vogue is to employ manual fact-checkers to verify claims to combat this avalanche of misinformation. However establishing such claims&#39; veracity is becoming increasingly challenging partly due to the plethora of information available which is difficult to process manually. Thus it becomes imperative to verify claims automatically without human interventions. To cope up with this issue we propose an automated claim verification solution encompassing two steps -- document retrieval and veracity prediction. For the retrieval module we employ a hybrid search-based system with BM25 as a base retriever and experiment with recent state-of-the-art transformer-based models for re-ranking. Furthermore we use a BART-based textual entailment architecture to authenticate the retrieved documents in the later step. We report experimental findings demonstrating that our retrieval module outperforms the best baseline system by 10.32 NDCG@100 points. We escort a demonstration to assess the efficacy and impact of our suggested solution. As a byproduct of this study we present an open-source easily deployable and user-friendly Python API that the community can adopt.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Retrieval and Claim Verification to Mitigate COVID-19 Misinformation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.constraint-1.8" target="_blank">https://aclanthology.org/2022.constraint-1.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Language revitalisation should not be understood as a direct outcome of language documentation which is mainly focused on the creation of language repositories. Natural language processing (NLP) offers the potential to complement and exploit these repositories through the development of language technologies that may contribute to improving the vitality status of endangered languages. In this paper we discuss the current state of the interaction between language documentation and computational linguistics present a diagnosis of how the outputs of recent documentation projects for endangered languages are underutilised for the NLP community and discuss how the situation could change from both the documentary linguistics and NLP perspectives. All this is introduced as a bridging paradigm dubbed as Computational Language Documentation and Development (CLDmbox^2). CLDmbox^2 calls for (1) the inclusion of NLP-friendly annotated data as a deliverable of future language documentation projects; and (2) the exploitation of language documentation databases by the NLP community to promote the computerization of endangered languages as one way to contribute to their revitalization.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CLDmbox^2 Language Documentation Meets Natural Language Processing for Revitalising Endangered Languages</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.computel-1.4" target="_blank">https://aclanthology.org/2022.computel-1.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we present an approach to efficiently recover texts from corrupted documents of endangered languages. Textual resources for such languages are scarce and sometimes the few available resources are corrupted PDF documents. Endangered languages are not supported by standard tools and present even the additional difficulties of not possessing any corpus over which to train language models to assist with the recovery. The approach presented is able to fully recover born digital PDF documents with minimal effort thereby helping the preservation effort of endangered languages by extending the range of documents usable for corpus building.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Recovering Text from Endangered Languages Corrupted PDF documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.computel-1.10" target="_blank">https://aclanthology.org/2022.computel-1.10</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>For decades researchers in natural language processing and computational linguistics have been developing models and algorithms that aim to serve the needs of language documentation projects. However these models have seen little use in language documentation despite their great potential for making documentary linguistic artefacts better and easier to produce. In this work we argue that a major reason for this NLP gap is the lack of a strong foundation of application software which can on the one hand serve the complex needs of language documentation and on the other hand provide effortless integration with NLP models. We further present and describe a work-in-progress system we have developed to serve this need Glam.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Closing the NLP Gap: Documentary Linguistics and NLP Need a Shared Software Infrastructure</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.computel-1.15" target="_blank">https://aclanthology.org/2022.computel-1.15</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This study investigates applications of automatic speech recognition (ASR) techniques to Hupa a critically endangered Native American language from the Dene (Athabaskan) language family. Using around 9h12m of spoken data produced by one elder who is a first-language Hupa speaker we experimented with different evaluation schemes and training settings. On average a fully connected deep neural network reached a word error rate of 35.26%. Our overall results illustrate the utility of ASR for making Hupa language documentation more accessible and usable. In addition we found that when training acoustic models using recordings with transcripts that were not carefully verified did not necessarily have a negative effect on model performance. This shows promise for speech corpora of indigenous languages that commonly include transcriptions produced by second-language speakers or linguists who have advanced knowledge in the language of interest.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Enhancing Documentation of Hupa with Automatic Speech Recognition</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.computel-1.23" target="_blank">https://aclanthology.org/2022.computel-1.23</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Knowledge-grounded dialog systems need to incorporate smooth transitions among knowledge selected for generating responses to ensure that dialog flows naturally. For document-grounded dialog systems the inter- and intra-document knowledge relations can be used to model such conversational flows. We develop a novel Multi-Document Co-Referential Graph (Coref-MDG) to effectively capture the inter-document relationships based on commonsense and similarity and the intra-document co-referential structures of knowledge segments within the grounding documents. We propose CorefDiffs a Co-referential and Differential flow management method to linearize the static Coref-MDG into conversational sequence logic. CorefDiffs performs knowledge selection by accounting for contextual graph structures and the knowledge difference sequences. CorefDiffs significantly outperforms the state-of-the-art by 9.5% 7.4% and 8.2% on three public benchmarks. This demonstrates that the effective modeling of co-reference and knowledge difference for dialog flows are critical for transitions in document-grounded conversation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CorefDiffs: Co-referential and Differential Knowledge Flow in Document Grounded Conversations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.38" target="_blank">https://aclanthology.org/2022.coling-1.38</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Graph neural networks (GNNs) have been recently applied in natural language processing. Various GNN research studies are proposed to learn node interactions within the local graph of each document that contains words sentences or topics for inductive text classification. However most inductive GNNs that are built on a word graph generally take global word embeddings as node features without referring to document-wise contextual information. Consequently we find that BERT models can perform better than inductive GNNs. An intuitive follow-up approach is used to enrich GNNs with contextual embeddings from BERT yet there is a lack of related research. In this work we propose a simple yet effective unified model coined ConTextING with a joint training mechanism to learn from both document embeddings and contextual word interactions simultaneously. Our experiments show that ConTextING outperforms pure inductive GNNs and BERT-style models. The analyses also highlight the benefits of the sub-word graph and joint training with separated classifiers.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ConTextING: Granting Document-Wise Contextual Embeddings to Graph Neural Networks for Inductive Text Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.100" target="_blank">https://aclanthology.org/2022.coling-1.100</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Domain-specific documents cover terminologies and specialized knowledge. This has been the main challenge of domain-specific document retrieval systems. Previous approaches propose domain-adaptation and transfer learning methods to alleviate this problem. However these approaches still follow the same document representation method in previous approaches; a document is embedded into a single vector. In this study we propose VKGDR. VKGDR represents a given corpus into a graph of entities and their relations (known as a virtual knowledge graph) and computes the relevance between queries and documents based on the graph representation. We conduct three experiments 1) domain-specific document retrieval 2) comparison of our virtual knowledge graph construction method with previous approaches and 3) ablation study on each component of our virtual knowledge graph. From the results we see that unsupervised VKGDR outperforms baselines in a zero-shot setting and even outperforms fully-supervised bi-encoder. We also verify that our virtual knowledge graph construction method results in better retrieval performance than previous approaches.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Virtual Knowledge Graph Construction for Zero-Shot Domain-Specific Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.101" target="_blank">https://aclanthology.org/2022.coling-1.101</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level Relation Extraction (DocRE) aims at extracting relations between entities in a given document. Since different mention pairs may express different relations or even no relation it is crucial to identify key mention pairs responsible for the entity-level relation labels. However most recent studies treat different mentions equally while predicting the relations between entities leading to sub-optimal performance. To this end we propose a novel DocRE model called Key Mention pairs Guided Relation Extractor (KMGRE) to directly model mention-level relations containing two modules: a mention-level relation extractor and a key instance classifier. These two modules could be iteratively optimized with an EM-based algorithm to enhance each other. We also propose a new method to solve the multi-label problem in optimizing the mention-level relation extractor. Experimental results on two public DocRE datasets demonstrate that the proposed model is effective and outperforms previous state-of-the-art models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Key Mention Pairs Guided Document-Level Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.165" target="_blank">https://aclanthology.org/2022.coling-1.165</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The introduction of multimodal information and pretraining technique significantly improves entity recognition from visually-rich documents. However most of the existing methods pay unnecessary attention to irrelevant regions of the current document while ignoring the potentially valuable information in related documents. To deal with this problem this work proposes a cross-document semantic enhancement method which consists of two modules: 1) To prevent distractions from irrelevant regions in the current document we design a learnable attention mask mechanism which is used to adaptively filter redundant information in the current document. 2) To further enrich the entity-related context we propose a cross-document information awareness technique which enables the model to collect more evidence across documents to assist in prediction. The experimental results on two documents understanding benchmarks covering eight languages demonstrate that our method outperforms the SOTA methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Read Extensively Focus Smartly: A Cross-document Semantic Enhancement Method for Visual Documents NER</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.177" target="_blank">https://aclanthology.org/2022.coling-1.177</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level biomedical relation extraction (Bio-DocuRE) is an important branch of biomedical text mining that aims to automatically extract all relation facts from the biomedical text. Since there are a considerable number of relations in biomedical documents that need to be judged by other existing relations logical reasoning has become a research hotspot in the past two years. However current models with reasoning are single-granularity only based on one element information ignoring the complementary fact of different granularity reasoning information. In addition obtaining rich document information is a prerequisite for logical reasoning but most of the previous models cannot sufficiently utilize document information which limits the reasoning ability of the model. In this paper we propose a novel Bio-DocuRE model called FILR based on Multi-Dimensional Fusion Information and Multi-Granularity Logical Reasoning. Specifically FILR presents a multi-dimensional information fusion module MDIF to extract sufficient global document information. Then FILR proposes a multi-granularity reasoning module MGLR to obtain rich inference information through the reasoning of both entity-pairs and mention-pairs. We evaluate our FILR model on two widely used biomedical corpora CDR and GDA. Experimental results show that FILR achieves state-of-the-art performance.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-level Biomedical Relation Extraction Based on Multi-Dimensional Fusion Information and Multi-Granularity Logical Reasoning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.183" target="_blank">https://aclanthology.org/2022.coling-1.183</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level Event Causality Identification (DECI) aims to identify event-event causal relations in a document. Existing works usually build an event graph for global reasoning across multiple sentences. However the edges between events have to be carefully designed through heuristic rules or external tools. In this paper we propose a novel Event Relational Graph TransfOrmer (ERGO) framework for DECI to ease the graph construction and improve it over the noisy edge issue. Different from conventional event graphs we define a pair of events as a node and build a complete event relational graph without any prior knowledge or tools. This naturally formulates DECI as a node classification problem and thus we capture the causation transitivity among event pairs via a graph transformer. Furthermore we design a criss-cross constraint and an adaptive focal loss for the imbalanced classification to alleviate the issues of false positives and false negatives. Extensive experiments on two benchmark datasets show that ERGO greatly outperforms previous state-of-the-art (SOTA) methods (12.8% F1 gains on average).</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ERGO: Event Relational Graph Transformer for Document-level Event Causality Identification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.185" target="_blank">https://aclanthology.org/2022.coling-1.185</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We propose DocQueryNet a value retrieval method with arbitrary queries for form-like documents to reduce human effort of processing forms. Unlike previous methods that only address a fixed set of field items our method predicts target value for an arbitrary query based on the understanding of the layout and semantics of a form. To further boost model performance we propose a simple document language modeling (SimpleDLM) strategy to improve document understanding on large-scale model pre-training. Experimental results show that DocQueryNet outperforms previous designs significantly and the SimpleDLM further improves our performance on value retrieval by around 17% F1 score compared with the state-of-the-art pre-training method. Code is available here https://github.com/salesforce/QVR-SimpleDLM.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DocQueryNet: Value Retrieval with Arbitrary Queries for Form-like Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.187" target="_blank">https://aclanthology.org/2022.coling-1.187</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>One typical approach to long-form document matching is first conducting alignment between cross-document sentence pairs and then aggregating all of the sentence-level matching signals. However this approach could be problematic because the alignment between documents is partial --- despite two documents as a whole are well-matched most of the sentences could still be dissimilar. Those dissimilar sentences lead to spurious sentence-level matching signals which may overwhelm the real ones increasing the difficulties of learning the matching function. Therefore accurately selecting the key sentences for document matching is becoming a challenging issue. To address the issue we propose a novel matching approach that equips existing document matching models with an Optimal Partial Transport (OPT) based component namely OPT-Match which selects the sentences that play a major role in matching. Enjoying the partial transport properties of OPT the selected key sentences can not only effectively enhance the matching accuracy but also be explained as the rationales for the matching results. Extensive experiments on four publicly available datasets demonstrated that existing methods equipped with OPT-Match consistently outperformed the corresponding underlying methods. Evaluations also showed that the key sentences selected by OPT-Match were consistent with human-provided rationales.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Optimal Partial Transport Based Sentence Selection for Long-form Document Matching</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.208" target="_blank">https://aclanthology.org/2022.coling-1.208</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level relation extraction aims to recognize relations among multiple entity pairs from a whole piece of article. Recent methods achieve considerable performance but still suffer from two challenges: a) the relational entity pairs are sparse b) the representation of entity pairs is insufficient. In this paper we propose Pair-Aware and Entity-Enhanced(PAEE) model to solve the aforementioned two challenges. For the first challenge we design a Pair-Aware Representation module to predict potential relational entity pairs which constrains the relation extraction to the predicted entity pairs subset rather than all pairs; For the second we introduce a Entity-Enhanced Representation module to assemble directional entity pairs and obtain a holistic understanding of the entire document. Experimental results show that our approach can obtain state-of-the-art performance on four benchmark datasets DocRED DWIE CDR and GDA.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Relation Extraction via Pair-Aware and Entity-Enhanced Representation Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.213" target="_blank">https://aclanthology.org/2022.coling-1.213</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Transforming the large amounts of unstructured text on the Internet into structured event knowledge is a critical yet unsolved goal of NLP especially when addressing document-level text. Existing methods struggle in Document-level Event Extraction (DEE) due to its two intrinsic challenges: (a) Nested arguments which means one argument is the sub-string of another one. (b) Multiple events which indicates we should identify multiple events and assemble the arguments for them. In this paper we propose a role-interactive multi-event head attention network (CLIO) to solve these two challenges jointly. The key idea is to map different events to multiple subspaces (i.e. multi-event head). In each event subspace we draw the semantic representation of each role closer to its corresponding arguments then we determine whether the current event exists. To further optimize event representation we propose an event representation enhancing strategy to regularize pre-trained embedding space to be more isotropic. Our experiments on two widely used DEE datasets show that CLIO achieves consistent improvements over previous methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CLIO: Role-interactive Multi-event Head Attention Network for Document-level Event Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.221" target="_blank">https://aclanthology.org/2022.coling-1.221</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level Event Factuality Identification (DEFI) predicts the factuality of a specific event based on a document from which the event can be derived which is a fundamental and crucial task in Natural Language Processing (NLP). However most previous studies only considered sentence-level task and did not adopt document-level knowledge. Moreover they modelled DEFI as a typical text classification task depending on annotated information heavily and limited to the task-specific corpus only which resulted in data scarcity. To tackle these issues we propose a new framework formulating DEFI as Machine Reading Comprehension (MRC) tasks considering both Span-Extraction (Ext) and Multiple-Choice (Mch). Our model does not employ any other explicit annotated information and utilizes Transfer Learning (TL) to extract knowledge from universal large-scale MRC corpora for cross-domain data augmentation. The empirical results on DLEFM corpus demonstrate that the proposed model outperforms several state-of-the-arts.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-level Event Factuality Identification via Machine Reading Comprehension Frameworks with Transfer Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.231" target="_blank">https://aclanthology.org/2022.coling-1.231</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recognizing the layout of unstructured digital documents is crucial when parsing the documents into the structured machine-readable format for downstream applications. Recent studies in Document Layout Analysis usually rely on visual cues to understand documents while ignoring other information such as contextual information or the relationships between document layout components which are vital to boost better layout analysis performance. Our Doc-GCN presents an effective way to harmonize and integrate heterogeneous aspects for Document Layout Analysis. We construct different graphs to capture the four main features aspects of document layout components including syntactic semantic density and appearance features. Then we apply graph convolutional networks to enhance each aspect of features and apply the node-level pooling for integration. Finally we concatenate features of all aspects and feed them into the 2-layer MLPs for document layout component classification. Our Doc-GCN achieves state-of-the-art results on three widely used DLA datasets: PubLayNet FUNSD and DocBank. The code will be released at https://github.com/adlnlp/doc_gcn</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Doc-GCN: Heterogeneous Graph Convolutional Networks for Document Layout Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.256" target="_blank">https://aclanthology.org/2022.coling-1.256</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Cohesion devices e.g. reiteration coreference are crucial for building cohesion links across sentences. In this paper we propose a document-level neural machine translation framework CoDoNMT which models cohesion devices from two perspectives: Cohesion Device Masking (CoDM) and Cohesion Attention Focusing (CoAF). In CoDM we mask cohesion devices in the current sentence and force NMT to predict them with inter-sentential context information. A prediction task is also introduced to be jointly trained with NMT. In CoAF we attempt to guide the model to pay exclusive attention to relevant cohesion devices in the context when translating cohesion devices in the current sentence. Such a cohesion attention focusing strategy is softly applied to the self-attention layer. Experiments on three benchmark datasets demonstrate that our approach outperforms state-of-the-art document-level neural machine translation baselines. Further linguistic evaluation validates the effectiveness of the proposed model in producing cohesive translations.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CoDoNMT: Modeling Cohesion Devices for Document-Level Neural Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.462" target="_blank">https://aclanthology.org/2022.coling-1.462</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Heterogeneous Graph Neural Networks (HeterGNN) have been recently introduced as an emergent approach for extracting document summarization (EDS) by exploiting the cross-relations between words and sentences. However applying HeterGNN for long documents is still an open research issue. One of the main majors is the lacking of inter-sentence connections. In this regard this paper exploits how to apply HeterGNN for long documents by building a graph on sentence-level nodes (homogeneous graph) and combine with HeterGNN for capturing the semantic information in terms of both inter and intra-sentence connections. Experiments on two benchmark datasets of long documents such as PubMed and ArXiv show that our method is able to achieve state-of-the-art results in this research field.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi Graph Neural Network for Extractive Long Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.512" target="_blank">https://aclanthology.org/2022.coling-1.512</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents a novel multi-perspective document revision task. In conventional studies on document revision tasks such as grammatical error correction sentence reordering and discourse relation classification have been performed individually; however these tasks simultaneously should be revised to improve the readability and clarity of a whole document. Thus our study defines multi-perspective document revision as a task that simultaneously revises multiple perspectives. To model the task we design a novel Japanese multi-perspective document revision dataset that simultaneously handles seven perspectives to improve the readability and clarity of a document. Although a large amount of data that simultaneously handles multiple perspectives is needed to model multi-perspective document revision elaborately it is difficult to prepare such a large amount of this data. Therefore our study offers a multi-perspective document revision modeling method that can use a limited amount of matched data (i.e. data for the multi-perspective document revision task) and external partially-matched data (e.g. data for the grammatical error correction task). Experiments using our created dataset demonstrate the effectiveness of using multiple partially-matched datasets to model the multi-perspective document revision task.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Perspective Document Revision</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.535" target="_blank">https://aclanthology.org/2022.coling-1.535</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>A challenging task when generating summaries of legal documents is the ability to address their argumentative nature. We introduce a simple technique to capture the argumentative structure of legal documents by integrating argument role labeling into the summarization process. Experiments with pretrained language models show that our proposed approach improves performance over strong baselines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ArgLegalSumm: Improving Abstractive Summarization of Legal Documents with Argument Mining</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.540" target="_blank">https://aclanthology.org/2022.coling-1.540</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Social media posts provide a compelling yet challenging source of data of diverse perspectives from many socially salient groups. Automatic text summarization algorithms make this data accessible at scale by compressing large collections of documents into short summaries that preserve salient information from the source text. In this work we take a complementary approach to analyzing and improving the quality of summaries generated from social media data in terms of their ability to represent salient as well as diverse perspectives. We introduce a novel dataset DivSumm of dialect diverse tweets and human-written extractive and abstractive summaries. Then we study the extent of dialect diversity reflected in human-written reference summaries as well as system-generated summaries. The results of our extensive experiments suggest that humans annotate fairly well-balanced dialect diverse summaries and that cluster-based pre-processing approaches seem beneficial in improving the overall quality of the system-generated summaries without loss in diversity.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Analyzing the Dialect Diversity in Multi-document Summaries</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.542" target="_blank">https://aclanthology.org/2022.coling-1.542</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Multi-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents which makes them ideal for content modeling and relationship modeling. In this paper we present textbfKGSum an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically in the encoding process two graph-based modules are proposed to incorporate knowledge graph information into paper encoding while in the decoding process we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Document Scientific Summarization from a Knowledge Graph-Centric View</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.543" target="_blank">https://aclanthology.org/2022.coling-1.543</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Graph Neural Network (GNN)-based models have proven effective in various Natural Language Processing (NLP) tasks in recent years. Specifically in the case of the Extractive Document Summarization (EDS) task modeling documents under graph structure is able to analyze the complex relations between semantic units (e.g. word-to-word word-to-sentence sentence-to-sentence) and enrich sentence representations via valuable information from their neighbors. However long-form document summarization using graph-based methods is still an open research issue. The main challenge is to represent long documents in a graph structure in an effective way. In this regard this paper proposes a new heterogeneous graph neural network (HeterGNN) model to improve the performance of long document summarization (HeterGraphLongSum). Specifically the main idea is to add the passage nodes into the heterogeneous graph structure of word and sentence nodes for enriching the final representation of sentences. In this regard HeterGraphLongSum is designed with three types of semantic units such as word sentence and passage. Experiments on two benchmark datasets for long documents such as Pubmed and Arxiv indicate promising results of the proposed model for the extractive long document summarization problem. Especially HeterGraphLongSum is able to achieve state-of-the-art performance without relying on any pre-trained language models (e.g. BERT). The source code is available for further exploitation on the Github.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>HeterGraphLongSum: Heterogeneous Graph Neural Network with Passage Aggregation for Extractive Long Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.545" target="_blank">https://aclanthology.org/2022.coling-1.545</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recently neural topic models (NTMs) have been incorporated into pre-trained language models (PLMs) to capture the global semantic information for text summarization. However in these methods there remain limitations in the way they capture and integrate the global semantic information. In this paper we propose a novel model the graph contrastive topic enhanced language model (GRETEL) that incorporates the graph contrastive topic model with the pre-trained language model to fully leverage both the global and local contextual semantics for long document extractive summarization. To better capture and incorporate the global semantic information into PLMs the graph contrastive topic model integrates the hierarchical transformer encoder and the graph contrastive learning to fuse the semantic information from the global document context and the gold summary. To this end GRETEL encourages the model to efficiently extract salient sentences that are topically related to the gold summary rather than redundant sentences that cover sub-optimal topics. Experimental results on both general domain and biomedical datasets demonstrate that our proposed method outperforms SOTA methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>GRETEL: Graph Contrastive Topic Enhanced Language Model for Long Document Extractive Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.546" target="_blank">https://aclanthology.org/2022.coling-1.546</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Multi-Document Summarization (MDS) commonly employs the 2-stage extract-then-abstract paradigm which first extracts a relatively short meta-document then feeds it into the deep neural networks to generate an abstract. Previous work usually takes the ROUGE score as the label for training a scoring model to evaluate source documents. However the trained scoring model is prone to under-fitting for low-resource settings as it relies on the training data. To extract documents effectively we construct prompting templates that invoke the underlying knowledge in Pre-trained Language Model (PLM) to calculate the document and keyword&#39;s perplexity which can assess the document&#39;s semantic salience. Our unsupervised approach can be applied as a plug-in to boost other metrics for evaluating a document&#39;s salience thus improving the subsequent abstract generation. We get positive results on 2 MDS datasets 2 data settings and 2 abstractive backbone models showing our method&#39;s effectiveness. Our code is available at https://github.com/THU-KEG/UPER</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>UPER: Boosting Multi-Document Summarization with an Unsupervised Prompt-based Extractor</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.coling-1.550" target="_blank">https://aclanthology.org/2022.coling-1.550</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>``神经网络模型的快速发展使得多文档摘要可以获得人类可读的流畅的摘要对大规模的数据进行预训练可以更好的从自然语言文本中捕捉更丰富的语义信息并更好的作用于下游任务。目前很多的多文档摘要的工作也应用了预训练模型(如BERT)并取得了一定的效果但是这些预训练模型不能更好的从文本中捕获事实性知识没有考虑到多文档文本的结构化的实体-关系信息本文提出了基于实体信息增强和多粒度融合的多文档摘要模型MGNIE将实体关系信息融入预训练模型ERNIE中增强知识事实以获得多层语义信息解决摘要生成的事实一致性问题。进而从多种粒度进行多文档层次结构的融合建模以词信息、实体信息以及句子信息捕捉长文本信息摘要生成所需的关键信息点。本文设计的模型在国际标准评测数据集MultiNews上对比强基线模型效果和竞争力获得较大提升。&#39;&#39;</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>基于实体信息增强及多粒度融合的多文档摘要(Multi-Document Summarization Based on Entity Information Enhancement and Multi-Granularity Fusion)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.ccl-1.15" target="_blank">https://aclanthology.org/2022.ccl-1.15</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>``篇章事件抽取是从给定的文本中识别其事件类型和事件论元。目前篇章事件普遍存在数据稀疏和多值论元耦合的问题。基于此本文将汉语框架网(CFN)与中文篇章事件建立映射同时引入滑窗机制和触发词释义改善了事件检测的数据稀疏问题;使用基于类型感知标签的多事件分离策略缓解了论元耦合问题。为了提升模型的鲁棒性进一步引入对抗训练。本文提出的方法在DuEE-Fin和CCKS2021数据集上实验结果显著优于现有方法。&#39;&#39;</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>基于框架语义映射和类型感知的篇章事件抽取(Document-Level Event Extraction Based on Frame Semantic Mapping and Type Awareness)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.ccl-1.22" target="_blank">https://aclanthology.org/2022.ccl-1.22</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>PRINCIPLE was a Connecting Europe Facility (CEF)-funded project that focused on the identification collection and processing of language resources (LRs) for four European under-resourced languages (Croatian Icelandic Irish and Norwegian) in order to improve translation quality of eTranslation an online machine translation (MT) tool provided by the European Commission. The collected LRs were used for the development of neural MT engines in order to verify the quality of the resources. For all four languages a total of 66 LRs were collected and made available on the ELRC-SHARE repository under various licenses. For Croatian we have collected and published 20 LRs: 19 parallel corpora and 1 glossary. The majority of data is in the general domain (72 % of translation units) while the rest is in the eJustice (23 %) eHealth (3 %) and eProcurement (2 %) Digital Service Infrastructures (DSI) domains. The majority of the resources were for the Croatian-English language pair. The data was donated by six data contributors from the public as well as private sector. In this paper we present a subset of 13 Croatian LRs developed based on public administration documents which are all made freely available as well as challenges associated with the data collection cleaning and processing.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Challenges of Building Domain-Specific Parallel Corpora from Public Administration Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.bucc-1.7" target="_blank">https://aclanthology.org/2022.bucc-1.7</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Motivated by the fact that many relations cross the sentence boundary there has been increasing interest in document-level relation extraction (DocRE). DocRE requires integrating information within and across sentences capturing complex interactions between mentions of entities. Most existing methods are pipeline-based requiring entities as input. However jointly learning to extract entities and relations can improve performance and be more efficient due to shared parameters and training steps. In this paper we develop a sequence-to-sequence approach seq2rel that can learn the subtasks of DocRE (entity extraction coreference resolution and relation extraction) end-to-end replacing a pipeline of task-specific components. Using a simple strategy we call entity hinting we compare our approach to existing pipeline-based methods on several popular biomedical datasets in some cases exceeding their performance. We also report the first end-to-end results on these datasets for future comparison. Finally we demonstrate that under our model an end-to-end approach outperforms a pipeline-based approach. Our code data and trained models are available at https://github.com/johngiorgi/seq2rel. An online demo is available at https://share.streamlit.io/johngiorgi/seq2rel/main/demo.py.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A sequence-to-sequence approach for document-level relation extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.bionlp-1.2" target="_blank">https://aclanthology.org/2022.bionlp-1.2</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We study the zero-shot setting for the aspect-based scientific document summarization task. Summarizing scientific documents with respect to an aspect can remarkably improve document assistance systems and readers experience. However existing large-scale datasets contain a limited variety of aspects causing summarization models to over-fit to a small set of aspects and a specific domain. We establish baseline results in zero-shot performance (over unseen aspects and the presence of domain shift) paraphrasing leave-one-out and limited supervised samples experimental setups. We propose a self-supervised pre-training approach to enhance the zero-shot performance. We leverage the PubMed structured abstracts to create a biomedical aspect-based summarization dataset. Experimental results on the PubMed and FacetSum aspect-based datasets show promising performance when the model is pre-trained using unlabelled in-domain data.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Zero-Shot Aspect-Based Scientific Document Summarization using Self-Supervised Pre-training</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.bionlp-1.5" target="_blank">https://aclanthology.org/2022.bionlp-1.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>To unlock the value of high-quality bilingual translated documents we need parallel data. With sentence-aligned translation pairs we can fuel our neural machine translation customize MT or create translation memories for our clients. To automate this process automatic segmentation and alignment are required. Despite Arabic being the fifth biggest language in the world language technology for Arabic is many times way behind other languages. We will show how we struggled to find a proper sentence segmentation for Arabic and instead explored different frameworks from statistical to deep learning to end up fine-tuning our own Arabic DL segmentation model. We will highlight our learnings and challenges with segmenting and aligning Arabic and English bilingual data. Finally we will show the impact on our proprietary NMT engine as we started to unlock the value and could leverage data that had been translated offline outside CAT tools as well as comparable corpora to feed our NMT.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unlocking the value of bilingual translated documents with Deep Learning Segmentation and Alignment for Arabic</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.amta-upg.24" target="_blank">https://aclanthology.org/2022.amta-upg.24</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A DistilBERTopic Model for Short Text Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.alta-1.11" target="_blank">https://aclanthology.org/2022.alta-1.11</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Dense retrieval models which aim at retrieving the most relevant document for an input query on a dense representation space have gained considerable attention for their remarkable success. Yet dense models require a vast amount of labeled training data for notable performance whereas it is often challenging to acquire query-document pairs annotated by humans. To tackle this problem we propose a simple but effective Document Augmentation for dense Retrieval (DAR) framework which augments the representations of documents with their interpolation and perturbation. We validate the performance of DAR on retrieval tasks with two benchmark datasets showing that the proposed DAR significantly outperforms relevant baselines on the dense retrieval of both the labeled and unlabeled documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-short.48" target="_blank">https://aclanthology.org/2022.acl-short.48</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The goal-oriented document-grounded dialogue aims at responding to the user query based on the dialogue context and supporting document. Existing studies tackle this problem by decomposing it into two sub-tasks: knowledge identification and response generation. However such pipeline methods would unavoidably suffer from the error propagation issue. This paper proposes to unify these two sub-tasks via sequentially generating the grounding knowledge and the response. We further develop a prompt-connected multi-task learning strategy to model the characteristics and connections of different tasks and introduce linear temperature scheduling to reduce the negative effect of irrelevant document information. Experimental results demonstrate the effectiveness of our framework.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>UniGDD: A Unified Generative Framework for Goal-Oriented Document-Grounded Dialogue</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-short.66" target="_blank">https://aclanthology.org/2022.acl-short.66</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Several methods have been proposed for classifying long textual documents using Transformers. However there is a lack of consensus on a benchmark to enable a fair comparison among different approaches. In this paper we provide a comprehensive evaluation of the relative efficacy measured against various baselines and diverse datasets --- both in terms of accuracy as well as time and space overheads. Our datasets cover binary multi-class and multi-label classification tasks and represent various ways information is organized in a long text (e.g. information that is critical to making the classification decision is at the beginning or towards the end of the document). Our results show that more complex models often fail to outperform simple baselines and yield inconsistent performance across datasets. These findings emphasize the need for future studies to consider comprehensive baselines and datasets that better represent the task of long document classification to develop robust models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Efficient Classification of Long Documents Using Transformers</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-short.79" target="_blank">https://aclanthology.org/2022.acl-short.79</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We consider the task of document-level entity linking (EL) where it is important to make consistent decisions for entity mentions over the full document jointly. We aim to leverage explicit ``connections&#39;&#39; among mentions within the document itself: we propose to join EL and coreference resolution (coref) in a single structured prediction task over directed trees and use a globally normalized model to solve it. This contrasts with related works where two separate models are trained for each of the tasks and additional logic is required to merge the outputs. Experimental results on two datasets show a boost of up to +5% F1-score on both coref and EL tasks compared to their standalone counterparts. For a subset of hard cases with individual mentions lacking the correct EL in their candidate entity list we obtain a +50% increase in accuracy.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards Consistent Document-level Entity Linking: Joint Models for Entity Linking and Coreference Resolution</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-short.88" target="_blank">https://aclanthology.org/2022.acl-short.88</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Although current state-of-the-art Transformer-based solutions succeeded in a wide range for single-document NLP tasks they still struggle to address multi-input tasks such as multi-document summarization. Many solutions truncate the inputs thus ignoring potential summary-relevant contents which is unacceptable in the medical domain where each information can be vital. Others leverage linear model approximations to apply multi-input concatenation worsening the results because all information is considered even if it is conflicting or noisy with respect to a shared background. Despite the importance and social impact of medicine there are no ad-hoc solutions for multi-document summarization. For this reason we propose a novel discriminative marginalized probabilistic method (DAMEN) trained to discriminate critical information from a cluster of topic-related medical documents and generate a multi-document summary via token probability marginalization. Results prove we outperform the previous state-of-the-art on a biomedical dataset for multi-document summarization of systematic literature reviews. Moreover we perform extensive ablation studies to motivate the design choices and prove the importance of each module of our method.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Discriminative Marginalized Probabilistic Neural Method for Multi-Document Summarization of Medical Literature</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.15" target="_blank">https://aclanthology.org/2022.acl-long.15</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Predicting the approval chance of a patent application is a challenging problem involving multiple facets. The most crucial facet is arguably the novelty --- textit35 U.S. Code S 102 rejects more recent applications that have very similar prior arts. Such novelty evaluations differ the patent approval prediction from conventional document classification --- Successful patent applications may share similar writing patterns; however too-similar newer applications would receive the opposite label thus confusing standard document classifiers (e.g. BERT). To address this issue we propose a novel framework that unifies the document classifier with handcrafted features particularly time-dependent novelty scores. Specifically we formulate the novelty scores by comparing each application with millions of prior arts using a hybrid of efficient filters and a neural bi-encoder. Moreover we impose a new regularization term into the classification objective to enforce the monotonic change of approval prediction w.r.t. novelty scores. From extensive experiments on a large-scale USPTO dataset we find that standard BERT fine-tuning can partially learn the correct relationship between novelty and approvals from inconsistent data. However our time-dependent novelty features offer a boost on top of it. Also our monotonic regularization while shrinking the search space can drive the optimizer to better local optima yielding a further small performance gain.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards Comprehensive Patent Approval Predictions:Beyond Traditional Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.28" target="_blank">https://aclanthology.org/2022.acl-long.28</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Transformer based re-ranking models can achieve high search relevance through context- aware soft matching of query tokens with document tokens. To alleviate runtime complexity of such inference previous work has adopted a late interaction architecture with pre-computed contextual token representations at the cost of a large online storage. This paper proposes contextual quantization of token embeddings by decoupling document-specific and document-independent ranking contributions during codebook-based compression. This allows effective online decompression and embedding composition for better search relevance. This paper presents an evaluation of the above compact token representation model in terms of relevance and space efficiency.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Compact Token Representations with Contextual Quantization for Efficient Document Re-ranking</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.51" target="_blank">https://aclanthology.org/2022.acl-long.51</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document structure is critical for efficient information consumption. However it is challenging to encode it efficiently into the modern Transformer architecture. In this work we present HIBRIDS which injects Hierarchical Biases foR Incorporating Document Structure into attention score calculation. We further present a new task hierarchical question-summary generation for summarizing salient content in the source document into a hierarchy of questions and summaries where each follow-up question inquires about the content of its parent question-summary pair. We also annotate a new dataset with 6153 question-summary hierarchies labeled on government reports. Experiment results show that our model produces better question-summary hierarchies than comparisons on both hierarchy quality and content coverage a finding also echoed by human judges. Additionally our model improves the generation of long-form summaries from long government reports and Wikipedia articles as measured by ROUGE scores.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>HIBRIDS: Attention with Hierarchical Biases for Structure-aware Long Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.58" target="_blank">https://aclanthology.org/2022.acl-long.58</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Text summarization helps readers capture salient information from documents news interviews and meetings. However most state-of-the-art pretrained language models (LM) are unable to efficiently process long text for many summarization tasks. In this paper we propose Summ^N a simple flexible and effective multi-stage framework for input texts that are longer than the maximum context length of typical pretrained LMs. Summ^N first splits the data samples and generates a coarse summary in multiple stages and then produces the final fine-grained summary based on it. Our framework can process input text of arbitrary length by adjusting the number of stages while keeping the LM input size fixed. Moreover it can deal with both single-source documents and dialogues and it can be used on top of different backbone abstractive summarization models. To the best of our knowledge Summ^N is the first multi-stage split-then-summarize framework for long input summarization. Our experiments demonstrate that Summ^N outperforms previous state-of-the-art methods by improving ROUGE scores on three long meeting summarization datasets AMI ICSI and QMSum two long TV series datasets from SummScreen and a long document summarization dataset GovReport. Our data and code are available at https://github.com/psunlpgroup/Summ-N.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Summ^N: A Multi-Stage Summarization Framework for Long Input Dialogues and Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.112" target="_blank">https://aclanthology.org/2022.acl-long.112</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Clinical trials offer a fundamental opportunity to discover new treatments and advance the medical knowledge. However the uncertainty of the outcome of a trial can lead to unforeseen costs and setbacks. In this study we propose a new method to predict the effectiveness of an intervention in a clinical trial. Our method relies on generating an informative summary from multiple documents available in the literature about the intervention under study. Specifically our method first gathers all the abstracts of PubMed articles related to the intervention. Then an evidence sentence which conveys information about the effectiveness of the intervention is extracted automatically from each abstract. Based on the set of evidence sentences extracted from the abstracts a short summary about the intervention is constructed. Finally the produced summaries are used to train a BERT-based classifier in order to infer the effectiveness of an intervention. To evaluate our proposed method we introduce a new dataset which is a collection of clinical trials together with their associated PubMed articles. Our experiments demonstrate the effectiveness of producing short informative summaries and using them to predict the effectiveness of an intervention.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Predicting Intervention Approval in Clinical Trials through Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.137" target="_blank">https://aclanthology.org/2022.acl-long.137</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>User language data can contain highly sensitive personal content. As such it is imperative to offer users a strong and interpretable privacy guarantee when learning from their data. In this work we propose SentDP pure local differential privacy at the sentence level for a single user document. We propose a novel technique DeepCandidate that combines concepts from robust statistics and language modeling to produce high (768) dimensional general epsilon-SentDP document embeddings. This guarantees that any single sentence in a document can be substituted with any other sentence while keeping the embedding epsilon-indistinguishable. Our experiments indicate that these private document embeddings are useful for downstream tasks like sentiment analysis and topic classification and even outperform baseline methods with weaker guarantees like word-level Metric DP.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Sentence-level Privacy for Document Embeddings</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.238" target="_blank">https://aclanthology.org/2022.acl-long.238</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Sequence modeling has demonstrated state-of-the-art performance on natural language and document understanding tasks. However it is challenging to correctly serialize tokens in form-like documents in practice due to their variety of layout patterns. We propose FormNet a structure-aware sequence model to mitigate the suboptimal serialization of forms. First we design Rich Attention that leverages the spatial relationship between tokens in a form for more precise attention score calculation. Second we construct Super-Tokens for each word by embedding representations from their neighboring tokens through graph convolutions. FormNet therefore explicitly recovers local syntactic information that may have been lost during serialization. In experiments FormNet outperforms existing methods with a more compact model size and less pre-training data establishing new state-of-the-art performance on CORD FUNSD and Payment benchmarks.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.260" target="_blank">https://aclanthology.org/2022.acl-long.260</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level information extraction (IE) tasks have recently begun to be revisited in earnest using the end-to-end neural network techniques that have been successful on their sentence-level IE counterparts. Evaluation of the approaches however has been limited in a number of dimensions. In particular the precision/recall/F1 scores typically reported provide few insights on the range of errors the models make. We build on the work of Kummerfeld and Klein (2013) to propose a transformation-based framework for automating error analysis in document-level event and (N-ary) relation extraction. We employ our framework to compare two state-of-the-art document-level template-filling approaches on datasets from three domains; and then to gauge progress in IE since its inception 30 years ago vs. four systems from the MUC-4 (1992) evaluation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Error Analysis for Document-level Information Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.274" target="_blank">https://aclanthology.org/2022.acl-long.274</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level neural machine translation (DocNMT) achieves coherent translations by incorporating cross-sentence context. However for most language pairs there&#39;s a shortage of parallel documents although parallel sentences are readily available. In this paper we study whether and how contextual modeling in DocNMT is transferable via multilingual modeling. We focus on the scenario of zero-shot transfer from teacher languages with document level data to student languages with no documents but sentence level data and for the first time treat document-level translation as a transfer learning problem. Using simple concatenation-based DocNMT we explore the effect of 3 factors on the transfer: the number of teacher languages with document level data the balance between document and sentence level data at training and the data condition of parallel documents (genuine vs. back-translated). Our experiments on Europarl-7 and IWSLT-10 show the feasibility of multilingual transfer for DocNMT particularly on document-specific metrics. We observe that more teacher languages and adequate data balance both contribute to better transfer quality. Surprisingly the transfer is less sensitive to the data condition where multilingual DocNMT delivers decent performance with either back-translated or genuine document pairs.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilingual Document-Level Translation Enables Zero-Shot Transfer From Sentences to Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.287" target="_blank">https://aclanthology.org/2022.acl-long.287</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Although multi-document summarisation (MDS) of the biomedical literature is a highly valuable task that has recently attracted substantial interest evaluation of the quality of biomedical summaries lacks consistency and transparency. In this paper we examine the summaries generated by two current models in order to understand the deficiencies of existing evaluation approaches in the context of the challenges that arise in the MDS task. Based on this analysis we propose a new approach to human evaluation and identify several challenges that must be overcome to develop effective biomedical MDS systems.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The patient is more dead than alive: exploring the current state of the multi-document summarisation of the biomedical literature</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.350" target="_blank">https://aclanthology.org/2022.acl-long.350</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Multi-document summarization (MDS) has made significant progress in recent years in part facilitated by the availability of new dedicated datasets and capacious language models. However a standing limitation of these models is that they are trained against limited references and with plain maximum-likelihood objectives. As for many other generative tasks reinforcement learning (RL) offers the potential to improve the training of MDS models; yet it requires a carefully-designed reward that can ensure appropriate leverage of both the reference summaries and the input documents. For this reason in this paper we propose fine-tuning an MDS baseline with a reward that balances a reference-based metric such as ROUGE with coverage of the input documents. To implement the approach we utilize RELAX (Grathwohl et al. 2018) a contemporary gradient estimator which is both low-variance and unbiased and we fine-tune the baseline in a few-shot style for both stability and computational efficiency. Experimental results over the Multi-News and WCEP MDS datasets show significant improvements of up to +0.95 pp average ROUGE score and +3.17 pp METEOR score over the baseline and competitive results with the literature. In addition they show that the coverage of the input documents is increased and evenly across all documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Multi-Document Coverage Reward for RELAXed Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.351" target="_blank">https://aclanthology.org/2022.acl-long.351</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We introduce PRIMERA a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data. PRIMERA uses our newly proposed pre-training objective designed to teach the model to connect and aggregate information across documents. It also uses efficient encoder-decoder transformers to simplify the processing of concatenated input documents. With extensive experiments on 6 multi-document summarization datasets from 3 different domains on zero-shot few-shot and full-supervised settings PRIMERA outperforms current state-of-the-art dataset-specific and pre-trained models on most of these settings with large margins.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.360" target="_blank">https://aclanthology.org/2022.acl-long.360</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Extracting informative arguments of events from news articles is a challenging problem in information extraction which requires a global contextual understanding of each document. While recent work on document-level extraction has gone beyond single-sentence and increased the cross-sentence inference capability of end-to-end models they are still restricted by certain input sequence length constraints and usually ignore the global context between events. To tackle this issue we introduce a new global neural generation-based framework for document-level event argument extraction by constructing a document memory store to record the contextual event information and leveraging it to implicitly and explicitly help with decoding of arguments for later events. Empirical results show that our framework outperforms prior methods substantially and it is more robust to adversarially annotated examples with our constrained decoding design.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Dynamic Global Memory for Document-level Argument Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.361" target="_blank">https://aclanthology.org/2022.acl-long.361</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Dense retrieval has achieved impressive advances in first-stage retrieval from a large-scale document collection which is built on bi-encoder architecture to produce single vector representation of query and document. However a document can usually answer multiple potential queries from different views. So the single vector representation of a document is hard to match with multi-view queries and faces a semantic mismatch problem. This paper proposes a multi-view document representation learning framework aiming to produce multi-view embeddings to represent documents and enforce them to align with different queries. First we propose a simple yet effective method of generating multiple embeddings through viewers. Second to prevent multi-view embeddings from collapsing to the same one we further propose a global-local loss with annealed temperature to encourage the multiple viewers to better align with different potential queries. Experiments show our method outperforms recent works and achieves state-of-the-art results.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-View Document Representation Learning for Open-Domain Dense Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.414" target="_blank">https://aclanthology.org/2022.acl-long.414</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Multimodal pre-training with text layout and image has made significant progress for Visually Rich Document Understanding (VRDU) especially the fixed-layout documents such as scanned document images. While there are still a large number of digital documents where the layout information is not fixed and needs to be interactively and dynamically rendered for visualization making existing layout-based pre-training approaches not easy to apply. In this paper we propose MarkupLM for document understanding tasks with markup languages as the backbone such as HTML/XML-based documents where text and markup information is jointly pre-trained. Experiment results show that the pre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding tasks. The pre-trained model and code will be publicly available at https://aka.ms/markuplm.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MarkupLM: Pre-training of Text and Markup Language for Visually Rich Document Understanding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.420" target="_blank">https://aclanthology.org/2022.acl-long.420</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Generic summaries try to cover an entire document and query-based summaries try to answer document-specific questions. But real users&#39; needs often fall in between these extremes and correspond to aspects high-level topics discussed among similar types of documents. In this paper we collect a dataset of realistic aspect-oriented summaries AspectNews which covers different subtopics about articles in news sub-domains. We annotate data across two domains of articles earthquakes and fraud investigations where each article is annotated with two distinct summaries focusing on different aspects for each domain. A system producing a single generic summary cannot concisely satisfy both aspects. Our focus in evaluation is how well existing techniques can generalize to these domains without seeing in-domain training data so we turn to techniques to construct synthetic training data that have been used in query-focused summarization work. We compare several training schemes that differ in how strongly keywords are used and how oracle summaries are extracted. Our evaluation shows that our final approach yields (a) focused summaries better than those from a generic summarization system or from keyword matching; (b) a system sensitive to the choice of keywords.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ASPECTNEWS: Aspect-Oriented Summarization of News Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.449" target="_blank">https://aclanthology.org/2022.acl-long.449</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We introduce MemSum (Multi-step Episodic Markov decision process extractive SUMmarizer) a reinforcement-learning-based extractive summarizer enriched at each step with information on the current extraction history. When MemSum iteratively selects sentences into the summary it considers a broad information set that would intuitively also be used by humans in this task: 1) the text content of the sentence 2) the global text context of the rest of the document and 3) the extraction history consisting of the set of sentences that have already been extracted. With a lightweight architecture MemSum obtains state-of-the-art test-set performance (ROUGE) in summarizing long documents taken from PubMed arXiv and GovReport. Ablation studies demonstrate the importance of local global and history information. A human evaluation confirms the high quality and low redundancy of the generated summaries stemming from MemSum&#39;s awareness of extraction history.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MemSum: Extractive Summarization of Long Documents Using Multi-Step Episodic Markov Decision Processes</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.450" target="_blank">https://aclanthology.org/2022.acl-long.450</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>BERT based ranking models have achieved superior performance on various information retrieval tasks. However the large number of parameters and complex self-attention operations come at a significant latency overhead. To remedy this recent works propose late-interaction architectures which allow pre-computation of intermediate document representations thus reducing latency. Nonetheless having solved the immediate latency issue these methods now introduce storage costs and network fetching latency which limit their adoption in real-life production systems.In this work we propose the Succinct Document Representation (SDR) scheme that computes textithighly compressed intermediate document representations mitigating the storage/network issue. Our approach first reduces the dimension of token representations by encoding them using a novel autoencoder architecture that uses the document&#39;s textual content in both the encoding and decoding phases. After this token encoding step we further reduce the size of the document representations using modern quantization techniques. Evaluation on MSMARCO&#39;s passage re-reranking task show that compared to existing approaches using compressed document representations our method is highly efficient achieving 4x--11.6x higher compression rates for the same ranking quality. Similarly on the TREC CAR dataset we achieve 7.7x higher compression rate for the same ranking quality.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SDR: Efficient Neural Re-ranking using Succinct Document Representation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.457" target="_blank">https://aclanthology.org/2022.acl-long.457</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Word and morpheme segmentation are fundamental steps of language documentation as they allow to discover lexical units in a language for which the lexicon is unknown. However in most language documentation scenarios linguists do not start from a blank page: they may already have a pre-existing dictionary or have initiated manual segmentation of a small part of their data. This paper studies how such a weak supervision can be taken advantage of in Bayesian non-parametric models of segmentation. Our experiments on two very low resource languages (Mboshi and Japhug) whose documentation is still in progress show that weak supervision can be beneficial to the segmentation quality. In addition we investigate an incremental learning scenario where manual segmentations are provided in a sequential manner. This work opens the way for interactive annotation tools for documentary linguists.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Weakly Supervised Word Segmentation for Computational Language Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.510" target="_blank">https://aclanthology.org/2022.acl-long.510</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Structured document understanding has attracted considerable attention and made significant progress recently owing to its crucial role in intelligent document processing. However most existing related models can only deal with the document data of specific language(s) (typically English) included in the pre-training collection which is extremely limited. To address this issue we propose a simple yet effective Language-independent Layout Transformer (LiLT) for structured document understanding. LiLT can be pre-trained on the structured documents of a single language and then directly fine-tuned on other languages with the corresponding off-the-shelf monolingual/multilingual pre-trained textual models. Experimental results on eight languages have shown that LiLT can achieve competitive or even superior performance on diverse widely-used downstream benchmarks which enables language-independent benefit from the pre-training of document layout structure. Code and model are publicly available at https://github.com/jpWang/LiLT.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.534" target="_blank">https://aclanthology.org/2022.acl-long.534</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Language model (LM) pretraining captures various knowledge from text corpora helping downstream tasks. However existing methods such as BERT model a single document and do not capture dependencies or knowledge that span across documents. In this work we propose LinkBERT an LM pretraining method that leverages links between documents e.g. hyperlinks. Given a text corpus we view it as a graph of documents and create LM inputs by placing linked documents in the same context. We then pretrain the LM with two joint self-supervised objectives: masked language modeling and our new proposal document relation prediction. We show that LinkBERT outperforms BERT on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with citation links). LinkBERT is especially effective for multi-hop reasoning and few-shot QA (+5% absolute improvement on HotpotQA and TriviaQA) and our biomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on BioASQ and USMLE). We release our pretrained models LinkBERT and BioLinkBERT as well as code and data.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>LinkBERT: Pretraining Language Models with Document Links</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.acl-long.551" target="_blank">https://aclanthology.org/2022.acl-long.551</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Multi-document summarization (MDS) is a process of generating an informative and concise summary from multiple topic-related documents. Many studies have analyzed the quality of MDS dataset or models however no work has been done from the perspective of topic preservation. In this work we fill the gap by performing an empirical analysis on two MDS datasets and study topic preservation on generated summaries from 8 MDS models.Our key findings include i) Multi-News dataset has better gold summaries compared to Multi-XScience in terms of its topic distribution consistency and ii) Extractive approaches perform better than abstractive approaches in preserving topic information from source documents. We hope our findings could help develop a summarization model that can generate topic-focused summary and also give inspiration to researchers in creating dataset for such challenging task.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Empirical Study on Topic Preservation in Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.aacl-srw.9" target="_blank">https://aclanthology.org/2022.aacl-srw.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We propose to leverage news discourse profiling to model document-level temporal structures for building temporal dependency graphs. Our key observation is that the functional roles of sentences used for profiling news discourse signify different time frames relevant to a news story and can therefore help to recover the global temporal structure of a document. Our analyses and experiments with the widely used knowledge distillation technique show that discourse profiling effectively identifies distant inter-sentence event and (or) time expression pairs that are temporally related and otherwise difficult to locate.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Modeling Document-level Temporal Structures for Building Temporal Dependency Graphs</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.aacl-short.44" target="_blank">https://aclanthology.org/2022.aacl-short.44</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this work we study the importance of content frequency on abstractive summarization where we define the content as ``semantic units.&#39;&#39; We propose a two-stage training framework to let the model automatically learn the frequency of each semantic unit in the source text. Our model is trained in an unsupervised manner since the frequency information can be inferred from source text only. During inference our model identifies sentences with high-frequency semantic units and utilizes frequency information to generate summaries from the filtered sentences. Our model performance on the CNN/Daily Mail summarization task outperforms the other unsupervised methods under the same settings. Furthermore we achieve competitive ROUGE scores with far fewer model parameters compared to several large-scale pre-trained models. Our model can be trained under low-resource language settings and thus can serve as a potential solution for real-world applications where pre-trained models are not applicable.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Single Document Abstractive Summarization using Semantic Units</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.aacl-main.69" target="_blank">https://aclanthology.org/2022.aacl-main.69</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2022</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Summarization of legal case judgement documents is a challenging problem in Legal NLP. However not much analyses exist on how different families of summarization models (e.g. extractive vs. abstractive) perform when applied to legal case documents. This question is particularly important since many recent transformer-based abstractive summarization models have restrictions on the number of input tokens and legal documents are known to be very long. Also it is an open question on how best to evaluate legal case document summarization systems. In this paper we carry out extensive experiments with several extractive and abstractive summarization methods (both supervised and unsupervised) over three legal summarization datasets that we have developed. Our analyses that includes evaluation by law practitioners lead to several interesting insights on legal summarization in specific and long document summarization in general.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Legal Case Document Summarization: Extractive and Abstractive Methods and their Evaluation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2022.aacl-main.77" target="_blank">https://aclanthology.org/2022.aacl-main.77</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recently the Machine Translation (MT) community has become more interested in document-level evaluation especially in light of reactions to claims of ``human parity&#39;&#39; since examining the quality at the level of the document rather than at the sentence level allows for the assessment of suprasentential context providing a more reliable evaluation. This paper presents a document-level corpus annotated in English with context-aware issues that arise when translating from English into Brazilian Portuguese namely ellipsis gender lexical ambiguity number reference and terminology with six different domains. The corpus can be used as a challenge test set for evaluation and as a training/testing corpus for MT as well as for deep linguistic analysis of context issues. To the best of our knowledge this is the first corpus of its kind.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DELA Corpus - A Document-Level Corpus Annotated with Context-Related Issues</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.wmt-1.63" target="_blank">https://aclanthology.org/2021.wmt-1.63</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Text classification is a central tool in NLP. However when the target classes are strongly correlated with other textual attributes text classification models can pick up ``wrong&#39;&#39; features leading to bad generalization and biases. In social media analysis this problem surfaces for demographic user classes such as language topic or gender which influence the generate text to a substantial extent. Adversarial training has been claimed to mitigate this problem but thorough evaluation is missing. In this paper we experiment with text classification of the correlated attributes of document topic and author gender using a novel multilingual parallel corpus of TED talk transcripts. Our findings are: (a) individual classifiers for topic and author gender are indeed biased; (b) debiasing with adversarial training works for topic but breaks down for author gender; (c) gender debiasing results differ across languages. We interpret the result in terms of feature space overlap highlighting the role of linguistic surface realization of the target classes.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Disentangling Document Topic and Author Gender in Multiple Languages: Lessons for Adversarial Debiasing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.wassa-1.6" target="_blank">https://aclanthology.org/2021.wassa-1.6</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper studies the problem of cross-document event coreference resolution (CDECR) that seeks to determine if event mentions across multiple documents refer to the same real-world events. Prior work has demonstrated the benefits of the predicate-argument information and document context for resolving the coreference of event mentions. However such information has not been captured effectively in prior work for CDECR. To address these limitations we propose a novel deep learning model for CDECR that introduces hierarchical graph convolutional neural networks (GCN) to jointly resolve entity and event mentions. As such sentence-level GCNs enable the encoding of important context words for event mentions and their arguments while the document-level GCN leverages the interaction structures of event mentions and arguments to compute document representations to perform CDECR. Extensive experiments are conducted to demonstrate the effectiveness of the proposed model.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Hierarchical Graph Convolutional Networks for Jointly Resolving Cross-document Coreference of Entity and Event Mentions</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.textgraphs-1.4" target="_blank">https://aclanthology.org/2021.textgraphs-1.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This work revisits the information given by the graph-of-words and its typical utilization through graph-based ranking approaches in the context of keyword extraction. Recent well-known graph-based approaches typically employ the knowledge from word vector representations during the ranking process via popular centrality measures (e.g. PageRank) without giving the primary role to vectors&#39; distribution. We consider the adjacency matrix that corresponds to the graph-of-words of a target text document as the vector representation of its vocabulary. We propose the distribution-based modeling of this adjacency matrix using unsupervised (learning) algorithms. The efficacy of the distribution-based modeling approaches compared to state-of-the-art graph-based methods is confirmed by an extensive experimental study according to the F1 score. Our code is available on GitHub.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Keyword Extraction Using Unsupervised Learning on the Document&#39;s Adjacency Matrix</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.textgraphs-1.9" target="_blank">https://aclanthology.org/2021.textgraphs-1.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Readability or difficulty estimation of words and documents has been investigated independently in the literature often assuming the existence of extensive annotated resources for the other. Motivated by our analysis showing that there is a recursive relationship between word and document difficulty we propose to jointly estimate word and document difficulty through a graph convolutional network (GCN) in a semi-supervised fashion. Our experimental results reveal that the GCN-based method can achieve higher accuracy than strong baselines and stays robust even with a smaller amount of labeled data.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Semi-Supervised Joint Estimation of Word and Document Readability</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.textgraphs-1.16" target="_blank">https://aclanthology.org/2021.textgraphs-1.16</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Abstract While pretrained language models (LMs) have driven impressive gains over morpho-syntactic and semantic tasks their ability to model discourse and pragmatic phenomena is less clear. As a step towards a better understanding of their discourse modeling capabilities we propose a sentence intrusion detection task. We examine the performance of a broad range of pretrained LMs on this detection task for English. Lacking a dataset for the task we introduce INSteD a novel intruder sentence detection dataset containing 170000+ documents constructed from English Wikipedia and CNN news articles. Our experiments show that pretrained LMs perform impressively in in-domain evaluation but experience a substantial drop in the cross-domain setting indicating limited generalization capacity. Further results over a novel linguistic probe dataset show that there is substantial room for improvement especially in the cross- domain setting.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Evaluating Document Coherence Modeling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.tacl-1.38" target="_blank">https://aclanthology.org/2021.tacl-1.38</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Several NLP tasks need the effective repre-sentation of text documents.Arora et al.2017 demonstrate that simple weighted aver-aging of word vectors frequently outperformsneural models. SCDV (Mekala et al. 2017)further extends this from sentences to docu-ments by employing soft and sparse cluster-ing over pre-computed word vectors. How-ever both techniques ignore the polysemyand contextual character of words.In thispaper we address this issue by proposingSCDV+BERT(ctxd) a simple and effective un-supervised representation that combines con-textualized BERT (Devlin et al. 2019) basedword embedding for word sense disambigua-tion with SCDV soft clustering approach. Weshow that our embeddings outperform origi-nal SCDV pre-train BERT and several otherbaselines on many classification datasets. Wealso demonstrate our embeddings effective-ness on other tasks such as concept match-ing and sentence similarity.In additionwe show that SCDV+BERT(ctxd) outperformsfine-tune BERT and different embedding ap-proaches in scenarios with limited data andonly few shots examples.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Contextualized Document Representation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.sustainlp-1.17" target="_blank">https://aclanthology.org/2021.sustainlp-1.17</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We point out that common evaluation practices for cross-document coreference resolution have been unrealistically permissive in their assumed settings yielding inflated results. We propose addressing this issue via two evaluation methodology principles. First as in other tasks models should be evaluated on predicted mentions rather than on gold mentions. Doing this raises a subtle issue regarding singleton coreference clusters which we address by decoupling the evaluation of mention detection from that of coreference linking. Second we argue that models should not exploit the synthetic topic structure of the standard ECB+ dataset forcing models to confront the lexical ambiguity challenge as intended by the dataset creators. We demonstrate empirically the drastic impact of our more realistic evaluation principles on a competitive model yielding a score which is 33 F1 lower compared to evaluating by prior lenient practices.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Realistic Evaluation Principles for Cross-document Coreference Resolution</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.starsem-1.13" target="_blank">https://aclanthology.org/2021.starsem-1.13</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents a technical report of our submission to the 4th task of SemEval-2021 titled: Reading Comprehension of Abstract Meaning. In this task we want to predict the correct answer based on a question given a context. Usually contexts are very lengthy and require a large receptive field from the model. Thus common contextualized language models like BERT miss fine representation and performance due to the limited capacity of the input tokens. To tackle this problem we used the longformer model to better process the sequences. Furthermore we utilized the method proposed in the longformer benchmark on wikihop dataset which improved the accuracy on our task data from (23.01% and 22.95%) achieved by the baselines for subtask 1 and 2 respectively to (70.30% and 64.38%).</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>NLP-IIS@UT at SemEval-2021 Task 4: Machine Reading Comprehension using the Long Document Transformer</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.semeval-1.23" target="_blank">https://aclanthology.org/2021.semeval-1.23</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Understanding tables is an important and relevant task that involves understanding table structure as well as being able to compare and contrast information within cells. In this paper we address this challenge by presenting a new dataset and tasks that addresses this goal in a shared task in SemEval 2020 Task 9: Fact Verification and Evidence Finding for Tabular Data in Scientific Documents (SEM-TAB-FACTS). Our dataset contains 981 manually-generated tables and an auto-generated dataset of 1980 tables providing over 180K statement and over 16M evidence annotations. SEM-TAB-FACTS featured two sub-tasks. In sub-task A the goal was to determine if a statement is supported refuted or unknown in relation to a table. In sub-task B the focus was on identifying the specific cells of a table that provide evidence for the statement. 69 teams signed up to participate in the task with 19 successful submissions to subtask A and 12 successful submissions to subtask B. We present our results and main findings from the competition.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SemEval-2021 Task 9: Fact Verification and Evidence Finding for Tabular Data in Scientific Documents (SEM-TAB-FACTS)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.semeval-1.39" target="_blank">https://aclanthology.org/2021.semeval-1.39</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Proceedings of the Second Workshop on Scholarly Document Processing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.sdp-1.0" target="_blank">https://aclanthology.org/2021.sdp-1.0</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>One of the challenges in information retrieval (IR) is the vocabulary mismatch problem which happens when the terms between queries and documents are lexically different but semantically similar. While recent work has proposed to expand the queries or documents by enriching their representations with additional relevant terms to address this challenge they usually require a large volume of query-document pairs to train an expansion model. In this paper we propose an Unsupervised Document Expansion with Generation (UDEG) framework with a pre-trained language model which generates diverse supplementary sentences for the original document without using labels on query-document pairs for training. For generating sentences we further stochastically perturb their embeddings to generate more diverse sentences for document expansion. We validate our framework on two standard IR benchmark datasets. The results show that our framework significantly outperforms relevant expansion baselines for IR.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Document Expansion for Information Retrieval with Stochastic Text Generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.sdp-1.2" target="_blank">https://aclanthology.org/2021.sdp-1.2</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>With the increase in the number of published academic papers growing expectations have been placed on research related to supporting the writing process of scientific papers. Recently research has been conducted on various tasks such as citation worthiness (judging whether a sentence requires citation) citation recommendation and citation-text generation. However since each task has been studied and evaluated using data that has been independently developed it is currently impossible to verify whether such tasks can be successfully pipelined to effective use in scientific-document writing. In this paper we first define a series of tasks related to scientific-document writing that can be pipelined. Then we create a dataset of academic papers that can be used for the evaluation of each task as well as a series of these tasks. Finally using the dataset we evaluate the tasks of citation worthiness and citation recommendation as well as both of these tasks integrated. The results of our evaluations show that the proposed approach is promising.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Task Definition and Integration For Scientific-Document Writing Support</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.sdp-1.3" target="_blank">https://aclanthology.org/2021.sdp-1.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Argument mining targets structures in natural language related to interpretation and persuasion which are central to scientific communication. Most scholarly discourse involves interpreting experimental evidence and attempting to persuade other scientists to adopt the same conclusions. While various argument mining studies have addressed student essays and news articles those that target scientific discourse are still scarce. This paper surveys existing work in argument mining of scholarly discourse and provides an overview of current models data tasks and applications. We identify a number of key challenges confronting argument mining in the scientific domain and suggest some possible solutions and future directions.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Argument Mining for Scholarly Document Processing: Taking Stock and Looking Ahead</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.sdp-1.7" target="_blank">https://aclanthology.org/2021.sdp-1.7</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Large pretrained models have seen enormous success in extractive summarization tasks. In this work we investigate the influence of pretraining on a BERT-based extractive summarization system for scientific documents. We derive significant performance improvements using an intermediate pretraining step that leverages existing summarization datasets and report state-of-the-art results on a recently released scientific summarization dataset SciTLDR. We systematically analyze the intermediate pretraining step by varying the size and domain of the pretraining corpus changing the length of the input sequence in the target task and varying target tasks. We also investigate how intermediate pretraining interacts with contextualized word embeddings trained on different domains.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The Effect of Pretraining on Extractive Summarization for Scientific Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.sdp-1.9" target="_blank">https://aclanthology.org/2021.sdp-1.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Most summarization task focuses on generating relatively short summaries. Such a length constraint might not be appropriate when summarizing scientific work. The LongSumm task needs participants generate long summary for scientific document. This task usual can be solved by language model. But an important problem is that model like BERT is limit to memory and can not deal with a long input like a document. Also generate a long output is hard. In this paper we propose a session based automatic summarization model(SBAS) which using a session and ensemble mechanism to generate long summary. And our model achieves the best performance in the LongSumm task.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>LongSumm 2021: Session based automatic summarization model for scientific document</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.sdp-1.12" target="_blank">https://aclanthology.org/2021.sdp-1.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes our submission for the LongSumm task in SDP 2021. We propose a method for incorporating sentence embeddings produced by deep language models into extractive summarization techniques based on graph centrality in an unsupervised manner.The proposed method is simple fast can summarize any kind of document of any size and can satisfy any length constraints for the summaries produced. The method offers competitive performance to more sophisticated supervised methods and can serve as a proxy for abstractive summarization techniques</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised document summarization using pre-trained sentence embeddings and graph centrality</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.sdp-1.14" target="_blank">https://aclanthology.org/2021.sdp-1.14</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>With the ever-increasing pace of research and high volume of scholarly communication scholars face a daunting task. Not only must they keep up with the growing literature in their own and related fields scholars increasingly also need to rebut pseudo-science and disinformation. These needs have motivated an increasing focus on computational methods for enhancing search summarization and analysis of scholarly documents. However the various strands of research on scholarly document processing remain fragmented. To reach out to the broader NLP and AI/ML community pool distributed efforts in this area and enable shared access to published research we held the 2nd Workshop on Scholarly Document Processing (SDP) at NAACL 2021 as a virtual event (https://sdproc.org/2021/). The SDP workshop consisted of a research track three invited talks and three Shared Tasks (LongSumm 2021 SCIVER and 3C). The program was geared towards the application of NLP information retrieval and data mining for scholarly documents with an emphasis on identifying and providing solutions to open challenges.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Overview of the Second Workshop on Scholarly Document Processing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.sdp-1.22" target="_blank">https://aclanthology.org/2021.sdp-1.22</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Information overload has been one of the challenges regarding information from the Internet. It is not a matter of information access instead the focus had shifted towards the quality of the retrieved data. Particularly in the news domain multiple outlets report on the same news events but may differ in details. This work considers that different news outlets are more likely to differ in their writing styles and the choice of words and proposes a method to extract sentences based on their key information by focusing on the shared synonyms in each sentence. Our method also attempts to reduce redundancy through hierarchical clustering and arrange selected sentences on the proposed orderBERT. The results show that the proposed unsupervised framework successfully improves the coverage coherence and meanwhile reduces the redundancy for a generated summary. Moreover due to the process of obtaining the dataset we also propose a data refinement method to alleviate the problems of undesirable texts which result from the process of automatic scraping.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Multi-document Summarization for News Corpus with Key Synonyms and Contextual Embeddings</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.rocling-1.25" target="_blank">https://aclanthology.org/2021.rocling-1.25</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level relation extraction is a challenging task requiring reasoning over multiple sentences to predict a set of relations in a document. In this paper we propose a novel framework E2GRE (Entity and Evidence Guided Relation Extraction) that jointly extracts relations and the underlying evidence sentences by using large pretrained language model (LM) as input encoder. First we propose to guide the pretrained LM&#39;s attention mechanism to focus on relevant context by using attention probabilities as additional features for evidence prediction. Furthermore instead of feeding the whole document into pretrained LMs to obtain entity representation we concatenate document text with head entities to help LMs concentrate on parts of the document that are more related to the head entity. Our E2GRE jointly learns relation extraction and evidence prediction effectively showing large gains on both these tasks which we find are highly correlated.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Entity and Evidence Guided Document-Level Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.repl4nlp-1.30" target="_blank">https://aclanthology.org/2021.repl4nlp-1.30</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Information extraction from documents has become great use of novel natural language processing areas. Most of the entity extraction methodologies are variant in a context such as medical area financial area also come even limited to the given language. It is better to have one generic approach applicable for any document type to extract entity information regardless of language context and structure. Also another issue in such research is structural analysis while keeping the hierarchical semantic and heuristic features. Another problem identified is that usually it requires a massive training corpus. Therefore this research focus on mitigating such barriers. Several approaches have been identifying towards building document information extractors focusing on different disciplines. This research area involves natural language processing semantic analysis information extraction and conceptual modelling. This paper presents a review of the information extraction mechanism to construct a generic framework for document extraction with aim of providing a solid base for upcoming research.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Review on Document Information Extraction Approaches</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.ranlp-srw.24" target="_blank">https://aclanthology.org/2021.ranlp-srw.24</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>EuroVoc is a multilingual thesaurus that was built for organizing the legislative documentary of the European Union institutions. It contains thousands of categories at different levels of specificity and its descriptors are targeted by legal texts in almost thirty languages. In this work we propose a unified framework for EuroVoc classification on 22 languages by fine-tuning modern Transformer-based pretrained language models. We study extensively the performance of our trained models and show that they significantly improve the results obtained by a similar tool - JEX - on the same dataset. The code and the fine-tuned models were open sourced together with a programmatic interface that eases the process of loading the weights of a trained model and of classifying a new document.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>PyEuroVoc: A Tool for Multilingual Legal Document Classification with EuroVoc Descriptors</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.ranlp-1.12" target="_blank">https://aclanthology.org/2021.ranlp-1.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The mix-up method (Zhang et al. 2017) one of the methods for data augmentation is known to be easy to implement and highly effective. Although the mix-up method is intended for image identification it can also be applied to natural language processing. In this paper we attempt to apply the mix-up method to a document classification task using bidirectional encoder representations from transformers (BERT) (Devlin et al. 2018). Since BERT allows for two-sentence input we concatenated word sequences from two documents with different labels and used the multi-class output as the supervised data with a one-hot vector. In an experiment using the livedoor news corpus which is Japanese we compared the accuracy of document classification using two methods for selecting documents to be concatenated with that of ordinary document classification. As a result we found that the proposed method is better than the normal classification when the documents with labels shortages are mixed preferentially. This indicates that how to choose documents for mix-up has a significant impact on the results.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Application of Mix-Up Method in Document Classification Task Using BERT</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.ranlp-1.77" target="_blank">https://aclanthology.org/2021.ranlp-1.77</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Distantly supervised datasets for relation extraction mostly focus on sentence-level extraction and they cover very few relations. In this work we propose cross-document relation extraction where the two entities of a relation tuple appear in two different documents that are connected via a chain of common entities. Following this idea we create a dataset for two-hop relation extraction where each chain contains exactly two documents. Our proposed dataset covers a higher number of relations than the publicly available sentence-level datasets. We also propose a hierarchical entity graph convolutional network (HEGCN) model for this task that improves performance by 1.1% F1 score on our two-hop relation extraction dataset compared to some strong neural baselines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Hierarchical Entity Graph Convolutional Network for Relation Extraction across Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.ranlp-1.115" target="_blank">https://aclanthology.org/2021.ranlp-1.115</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document alignment techniques based on multilingual sentence representations have recently shown state of the art results. However these techniques rely on unsupervised distance measurement techniques which cannot be fined-tuned to the task at hand. In this paper instead of these unsupervised distance measurement techniques we employ Metric Learning to derive task-specific distance measurements. These measurements are supervised meaning that the distance measurement metric is trained using a parallel dataset. Using a dataset belonging to English Sinhala and Tamil which belong to three different language families we show that these task-specific supervised distance learning metrics outperform their unsupervised counterparts for document alignment.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Metric Learning in Multilingual Sentence Similarity Measurement for Document Alignment</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.ranlp-1.129" target="_blank">https://aclanthology.org/2021.ranlp-1.129</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Neural Topic Models are recent neural models that aim at extracting the main themes from a collection of documents. The comparison of these models is usually limited because the hyperparameters are held fixed. In this paper we present an empirical analysis and comparison of Neural Topic Models by finding the optimal hyperparameters of each model for four different performance measures adopting a single-objective Bayesian optimization. This allows us to determine the robustness of a topic model for several evaluation metrics. We also empirically show the effect of the length of the documents on different optimized metrics and discover which evaluation metrics are in conflict or agreement with each other.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Empirical Analysis of Topic Models: Uncovering the Relationships between Hyperparameters Document Length and Performance Measures</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.ranlp-1.157" target="_blank">https://aclanthology.org/2021.ranlp-1.157</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Neural sequence-to-sequence (Seq2Seq) models and BERT have achieved substantial improvements in abstractive document summarization (ADS) without and with pre-training respectively. However they sometimes repeatedly attend to unimportant source phrases while mistakenly ignore important ones. We present reconstruction mechanisms on two levels to alleviate this issue. The sequence-level reconstructor reconstructs the whole document from the hidden layer of the target summary while the word embedding-level one rebuilds the average of word embeddings of the source at the target side to guarantee that as much critical information is included in the summary as possible. Based on the assumption that inverse document frequency (IDF) measures how important a word is we further leverage the IDF weights in our embedding-level reconstructor. The proposed frameworks lead to promising improvements for ROUGE metrics and human rating on both the CNN/Daily Mail and Newsroom summarization datasets.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Abstractive Document Summarization with Word Embedding Reconstruction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.ranlp-1.178" target="_blank">https://aclanthology.org/2021.ranlp-1.178</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Extracting the most important part of legislation documents has great business value because the texts are usually very long and hard to understand. The aim of this article is to evaluate different algorithms for text summarization on EU legislation documents. The content contains domain-specific words. We collected a text summarization dataset of EU legal documents consisting of 1563 documents in which the mean length of summaries is 424 words. Experiments were conducted with different algorithms using the new dataset. A simple extractive algorithm was selected as a baseline. Advanced extractive algorithms which use encoders show better results than baseline. The best result measured by ROUGE scores was achieved by a fine-tuned abstractive T5 model which was adapted to work with long texts.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Comparative Study on Abstractive and Extractive Approaches in Summarization of European Legislation Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.ranlp-1.184" target="_blank">https://aclanthology.org/2021.ranlp-1.184</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Monolingual vs multilingual BERTology for Vietnamese extractive multi-document summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.paclic-1.73" target="_blank">https://aclanthology.org/2021.paclic-1.73</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Fully understanding narratives often requires identifying events in the context of whole documents and modeling the event relations. However document-level event extraction is a challenging task as it requires the extraction of event and entity coreference and capturing arguments that span across different sentences. Existing works on event extraction usually confine on extracting events from single sentences which fail to capture the relationships between the event mentions at the scale of a document as well as the event arguments that appear in a different sentence than the event trigger. In this paper we propose an end-to-end model leveraging Deep Value Networks (DVN) a structured prediction algorithm to efficiently capture cross-event dependencies for document-level event extraction. Experimental results show that our approach achieves comparable performance to CRF-based models on ACE05 while enjoys significantly higher computational efficiency.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-level Event Extraction with Efficient End-to-end Learning of Cross-event Dependencies</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.nuse-1.4" target="_blank">https://aclanthology.org/2021.nuse-1.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this work we present a method for content selection and document planning for automated news and report generation from structured statistical data such as that offered by the European Union&#39;s statistical agency EuroStat. The method is driven by the data and is highly topic-independent within the statistical dataset domain. As our approach is not based on machine learning it is suitable for introducing news automation to the wide variety of domains where no training data is available. As such it is suitable as a low-cost (in terms of implementation effort) baseline for document structuring prior to introduction of domain-specific knowledge.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Baseline Document Planning Method for Automated Journalism</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.nodalida-main.11" target="_blank">https://aclanthology.org/2021.nodalida-main.11</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recent research using pre-trained language models for multi-document summarization task lacks deep investigation of potential erroneous cases and their possible application on other languages. In this work we apply a pre-trained language model (BART) for multi-document summarization (MDS) task using both fine-tuning and without fine-tuning. We use two English datasets and one German dataset for this study. First we reproduce the multi-document summaries for English language by following one of the recent studies. Next we show the applicability of the model to German language by achieving state-of-the-art performance on German MDS. We perform an in-depth error analysis of the followed approach for both languages which leads us to identifying most notable errors from made-up facts and topic delimitation and quantifying the amount of extractiveness.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Error Analysis of using BART for Multi-Document Summarization: A Study for English and German Language</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.nodalida-main.43" target="_blank">https://aclanthology.org/2021.nodalida-main.43</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Measuring the semantic similarity of different texts has many important applications in Digital Humanities research such as information retrieval document clustering and text summarization. The performance of different methods depends on the length of the text the domain and the language. This study focuses on experimenting with some of the current approaches to Finnish which is a morphologically rich language. At the same time we propose a simple method TFW2V which shows high efficiency in handling both long text documents and limited amounts of data. Furthermore we design an objective evaluation method which can be used as a framework for benchmarking text similarity approaches.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>TFW2V: An Enhanced Document Similarity Method for the Morphologically Rich Finnish Language</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.nlp4dh-1.19" target="_blank">https://aclanthology.org/2021.nlp4dh-1.19</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Searching for legal documents is a specialized Information Retrieval task that is relevant for expert users (lawyers and their assistants) and for non-expert users. By searching previous court decisions (cases) a user can better prepare the legal reasoning of a new case. Being able to search using a natural language text snippet instead of a more artificial query could help to prevent query formulation issues. Also if semantic similarity could be modeled beyond exact lexical matches more relevant results can be found even if the query terms don&#39;t match exactly. For this domain we formulated a task to compare different ways of modeling semantic similarity at paragraph level using neural and non-neural systems. We compared systems that encode the query and the search collection paragraphs as vectors enabling the use of cosine similarity for results ranking. After building a German dataset for cases and statutes from Switzerland and extracting citations from cases to statutes we developed an algorithm for estimating semantic similarity at paragraph level using a link-based similarity method. When evaluating different systems in this way we find that semantic similarity modeling by neural systems can be boosted with an extended attention mask that quenches noise in the inputs.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Searching for Legal Documents at Paragraph Level: Automating Label Generation and Use of an Extended Attention Mask for Boosting Neural Models of Semantic Similarity</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.nllp-1.12" target="_blank">https://aclanthology.org/2021.nllp-1.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>While many NLP pipelines assume raw clean texts many texts we encounter in the wild including a vast majority of legal documents are not so clean with many of them being visually structured documents (VSDs) such as PDFs. Conventional preprocessing tools for VSDs mainly focused on word segmentation and coarse layout analysis whereas fine-grained logical structure analysis (such as identifying paragraph boundaries and their hierarchies) of VSDs is underexplored. To that end we proposed to formulate the task as prediction of ``transition labels&#39;&#39; between text fragments that maps the fragments to a tree and developed a feature-based machine learning system that fuses visual textual and semantic cues. Our system is easily customizable to different types of VSDs and it significantly outperformed baselines in identifying different structures in VSDs. For example our system obtained a paragraph boundary detection F1 score of 0.953 which is significantly better than a popular PDF-to-text tool with an F1 score of 0.739.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Capturing Logical Structure of Visually Structured Documents with Multimodal Transition Parser</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.nllp-1.15" target="_blank">https://aclanthology.org/2021.nllp-1.15</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Bidirectional Encoder Representations from Transformers (BERT) has achieved state-of-the-art performances on several text classification tasks such as GLUE and sentiment analysis. Recent work in the legal domain started to use BERT on tasks such as legal judgement prediction and violation prediction. A common practise in using BERT is to fine-tune a pre-trained model on a target task and truncate the input texts to the size of the BERT input (e.g. at most 512 tokens). However due to the unique characteristics of legal documents it is not clear how to effectively adapt BERT in the legal domain. In this work we investigate how to deal with long documents and how is the importance of pre-training on documents from the same domain as the target task. We conduct experiments on the two recent datasets: ECHR Violation Dataset and the Overruling Task Dataset which are multi-label and binary classification tasks respectively. Importantly on average the number of tokens in a document from the ECHR Violation Dataset is more than 1600. While the documents in the Overruling Task Dataset are shorter (the maximum number of tokens is 204). We thoroughly compare several techniques for adapting BERT on long documents and compare different models pre-trained on the legal and other domains. Our experimental results show that we need to explicitly adapt BERT to handle long documents as the truncation leads to less effective performance. We also found that pre-training on the documents that are similar to the target task would result in more effective performance on several scenario.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Effectively Leveraging BERT for Legal Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.nllp-1.22" target="_blank">https://aclanthology.org/2021.nllp-1.22</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>A crucial difference between single- and multi-document summarization is how salient content manifests itself in the document(s). While such content may appear at the beginning of a single document essential information is frequently reiterated in a set of documents related to a particular topic resulting in an endorsement effect that increases information salience. In this paper we model the cross-document endorsement effect and its utilization in multiple document summarization. Our method generates a synopsis from each document which serves as an endorser to identify salient content from other documents. Strongly endorsed text segments are used to enrich a neural encoder-decoder model to consolidate them into an abstractive summary. The method has a great potential to learn from fewer examples to identify salient content which alleviates the need for costly retraining when the set of documents is dynamically adjusted. Through extensive experiments on benchmark multi-document summarization datasets we demonstrate the effectiveness of our proposed method over strong published baselines. Finally we shed light on future research directions and discuss broader challenges of this task using a case study.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Modeling Endorsement for Multi-Document Abstractive Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.newsum-1.13" target="_blank">https://aclanthology.org/2021.newsum-1.13</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Many applications require generation of summaries tailored to the user&#39;s information needs i.e. their intent. Methods that express intent via explicit user queries fall short when query interpretation is subjective. Several datasets exist for summarization with objective intents where for each document and intent (e.g. ``weather&#39;&#39;) a single summary suffices for all users. No datasets exist however for subjective intents (e.g. ``interesting places&#39;&#39;) where different users will provide different summaries. We present SUBSUME the first dataset for evaluation of SUBjective SUMmary Extraction systems. SUBSUME contains 2200 (document intent summary) triplets over 48 Wikipedia pages with ten intents of varying subjectivity provided by 103 individuals over Mechanical Turk. We demonstrate statistically that the intents in SUBSUME vary systematically in subjectivity. To indicate SUBSUME&#39;s usefulness we explore a collection of baseline algorithms for subjective extractive summarization and show that (i) as expected example-based approaches better capture subjective intents than query-based ones and (ii) there is ample scope for improving upon the baseline algorithms thereby motivating further research on this challenging problem.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SUBSUME: A Dataset for Subjective Summary Extraction from Wikipedia Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.newsum-1.14" target="_blank">https://aclanthology.org/2021.newsum-1.14</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The task of document-level text simplification is very similar to summarization with the additional difficulty of reducing complexity. We introduce a newly collected data set of German texts collected from the Swiss news magazine 20 Minuten (`20 Minutes&#39;) that consists of full articles paired with simplified summaries. Furthermore we present experiments on automatic text simplification with the pretrained multilingual mBART and a modified version thereof that is more memory-friendly using both our new data set and existing simplification corpora. Our modifications of mBART let us train at a lower memory cost without much loss in performance in fact the smaller mBART even improves over the standard model in a setting with multiple simplification levels.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A New Dataset and Efficient Baselines for Document-level Text Simplification in German</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.newsum-1.16" target="_blank">https://aclanthology.org/2021.newsum-1.16</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this thesis proposal we explore the application of event extraction to literary texts. Considering the lengths of literary documents modeling events in different granularities may be more adequate to extract meaningful information as individual elements contribute little to the overall semantics. We adapt the concept of schemas as sequences of events all describing a single process connected through shared participants extending it to for multiple schemas in a document. Segmentation of event sequences into schemas is approached by modeling event sequences on such task as the narrative cloze task the prediction of missing events in sequences. We propose building on sequences of event embeddings to form schema embeddings thereby summarizing sections of documents using a single representation. This approach will allow for the comparisons of different sections of documents and entire literary works. Literature is a challenging domain based on its variety of genres yet the representation of literary content has received relatively little attention.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards Layered Events and Schema Representations in Long Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-srw.5" target="_blank">https://aclanthology.org/2021.naacl-srw.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper proposes a new abstractive document summarization model hierarchical BART (Hie-BART) which captures hierarchical structures of a document (i.e. sentence-word structures) in the BART model. Although the existing BART model has achieved a state-of-the-art performance on document summarization tasks the model does not have the interactions between sentence-level information and word-level information. In machine translation tasks the performance of neural machine translation models has been improved by incorporating multi-granularity self-attention (MG-SA) which captures the relationships between words and phrases. Inspired by the previous work the proposed Hie-BART model incorporates MG-SA into the encoder of the BART model for capturing sentence-word structures. Evaluations on the CNN/Daily Mail dataset show that the proposed Hie-BART model outperforms some strong baselines and improves the performance of a non-hierarchical BART model (+0.23 ROUGE-L).</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Hie-BART: Document Summarization with Hierarchical BART</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-srw.20" target="_blank">https://aclanthology.org/2021.naacl-srw.20</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Allowing users to interact with multi-document summarizers is a promising direction towards improving and customizing summary results. Different ideas for interactive summarization have been proposed in previous work but these solutions are highly divergent and incomparable. In this paper we develop an end-to-end evaluation framework for interactive summarization focusing on expansion-based interaction which considers the accumulating information along a user session. Our framework includes a procedure of collecting real user sessions as well as evaluation measures relying on summarization standards but adapted to reflect interaction. All of our solutions and resources are available publicly as a benchmark allowing comparison of future developments in interactive summarization and spurring progress in its methodological evaluation. We demonstrate the use of our framework by evaluating and comparing baseline implementations that we developed for this purpose which will serve as part of our benchmark. Our extensive experimentation and analysis motivate the proposed evaluation framework design and support its viability.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extending Multi-Document Summarization Evaluation to the Interactive Setting</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-main.54" target="_blank">https://aclanthology.org/2021.naacl-main.54</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction which requires implicit coreference reasoning we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model&#39;s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Event Argument Extraction by Conditional Generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-main.69" target="_blank">https://aclanthology.org/2021.naacl-main.69</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Presentations are critical for communication in all areas of our lives yet the creation of slide decks is often tedious and time-consuming. There has been limited research aiming to automate the document-to-slides generation process and all face a critical challenge: no publicly available dataset for training and benchmarking. In this work we first contribute a new dataset SciDuet consisting of pairs of papers and their corresponding slides decks from recent years&#39; NLP and ML conferences (e.g. ACL). Secondly we present D2S a novel system that tackles the document-to-slides task with a two-step approach: 1) Use slide titles to retrieve relevant and engaging text figures and tables; 2) Summarize the retrieved context into bullet points with long-form question answering. Our evaluation suggests that long-form QA outperforms state-of-the-art summarization baselines on both automated ROUGE metrics and qualitative human evaluation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>D2S: Document-to-Slide Generation Via Query-Based Text Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-main.111" target="_blank">https://aclanthology.org/2021.naacl-main.111</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization. In this paper we propose Hepos a novel efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source. We further conduct a systematic study of existing efficient self-attentions. Combined with Hepos we are able to process ten times more tokens than existing models that use full attentions. For evaluation we present a new dataset GovReport with significantly longer documents and summaries. Results show that our models produce significantly higher ROUGE scores than competitive comparisons including new state-of-the-art results on PubMed. Human evaluation also shows that our models generate more informative summaries with fewer unfaithful errors.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Efficient Attentions for Long Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-main.112" target="_blank">https://aclanthology.org/2021.naacl-main.112</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Machine learning solutions are often criticized for the lack of explanation of their successes and failures. Understanding which instances are misclassified and why is essential to improve the learning process. This work helps to fill this gap by proposing a methodology to characterize quantify and measure the impact of hard instances in the task of polarity classification of movie reviews. We characterize such instances into two categories: neutrality where the text does not convey a clear polarity and discrepancy where the polarity of the text is the opposite of its true rating. We quantify the number of hard instances in polarity classification of movie reviews and provide empirical evidence about the need to pay attention to such problematic instances as they are much harder to classify for both machine and human classifiers. To the best of our knowledge this is the first systematic analysis of the impact of hard instances in polarity detection from well-formed textual reviews.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Why Do Document-Level Polarity Classifiers Fail?</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-main.143" target="_blank">https://aclanthology.org/2021.naacl-main.143</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Cross-document event coreference resolution is a foundational task for NLP applications involving multi-text processing. However existing corpora for this task are scarce and relatively small while annotating only modest-size clusters of documents belonging to the same topic. To complement these resources and enhance future research we present Wikipedia Event Coreference (WEC) an efficient methodology for gathering a large-scale dataset for cross-document event coreference from Wikipedia where coreference links are not restricted within predefined topics. We apply this methodology to the English Wikipedia and extract our large-scale WEC-Eng dataset. Notably our dataset creation method is generic and can be applied with relatively little effort to other Wikipedia languages. To set baseline results we develop an algorithm that adapts components of state-of-the-art models for within-document coreference resolution to the cross-document setting. Our model is suitably efficient and outperforms previously published state-of-the-art results for the task.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>WEC: Deriving a Large-scale Cross-document Event Coreference dataset from Wikipedia</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-main.198" target="_blank">https://aclanthology.org/2021.naacl-main.198</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Zero-shot learning aims to recognize unseen objects using their semantic representations. Most existing works use visual attributes labeled by humans not suitable for large-scale applications. In this paper we revisit the use of documents as semantic representations. We argue that documents like Wikipedia pages contain rich visual information which however can easily be buried by the vast amount of non-visual sentences. To address this issue we propose a semi-automatic mechanism for visual sentence extraction that leverages the document section headers and the clustering structure of visual sentences. The extracted visual sentences after a novel weighting scheme to distinguish similar classes essentially form semantic representations like visual attributes but need much less human effort. On the ImageNet dataset with over 10000 unseen classes our representations lead to a 64% relative improvement against the commonly used ones.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Revisiting Document Representations for Large-Scale Zero-Shot Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-main.250" target="_blank">https://aclanthology.org/2021.naacl-main.250</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We study the problem of Event Causality Identification (ECI) to detect causal relation between event mention pairs in text. Although deep learning models have recently shown state-of-the-art performance for ECI they are limited to the intra-sentence setting where event mention pairs are presented in the same sentences. This work addresses this issue by developing a novel deep learning model for document-level ECI (DECI) to accept inter-sentence event mention pairs. As such we propose a graph-based model that constructs interaction graphs to capture relevant connections between important objects for DECI in input documents. Such interaction graphs are then consumed by graph convolutional networks to learn document context-augmented representations for causality prediction between events. Various information sources are introduced to enrich the interaction graphs for DECI featuring discourse syntax and semantic information. Our extensive experiments show that the proposed model achieves state-of-the-art performance on two benchmark datasets.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Graph Convolutional Networks for Event Causality Identification with Rich Document-level Structures</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-main.273" target="_blank">https://aclanthology.org/2021.naacl-main.273</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document machine translation aims to translate the source sentence into the target language in the presence of additional contextual information. However it typically suffers from a lack of doc-level bilingual data. To remedy this here we propose a simple yet effective context-interactive pre-training approach which targets benefiting from external large-scale corpora. The proposed model performs inter sentence generation to capture the cross-sentence dependency within the target document and cross sentence translation to make better use of valuable contextual information. Comprehensive experiments illustrate that our approach can achieve state-of-the-art performance on three benchmark datasets which significantly outperforms a variety of baselines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Context-Interactive Pre-Training for Document Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-main.281" target="_blank">https://aclanthology.org/2021.naacl-main.281</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level neural machine translation (NMT) has proven to be of profound value for its effectiveness on capturing contextual information. Nevertheless existing approaches 1) simply introduce the representations of context sentences without explicitly characterizing the inter-sentence reasoning process; and 2) feed ground-truth target contexts as extra inputs at the training time thus facing the problem of exposure bias. We approach these problems with an inspiration from human behavior -- human translators ordinarily emerge a translation draft in their mind and progressively revise it according to the reasoning in discourse. To this end we propose a novel Multi-Hop Transformer (MHT) which offers NMT abilities to explicitly model the human-like draft-editing and reasoning process. Specifically our model serves the sentence-level translation as a draft and properly refines its representations by attending to multiple antecedent sentences iteratively. Experiments on four widely used document translation tasks demonstrate that our method can significantly improve document-level translation performance and can tackle discourse phenomena such as coreference error and the problem of polysemy.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Hop Transformer for Document-Level Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-main.309" target="_blank">https://aclanthology.org/2021.naacl-main.309</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document grounded generation is the task of using the information provided in a document to improve text generation. This work focuses on two different document grounded generation tasks: Wikipedia Update Generation task and Dialogue response generation. Our work introduces two novel adaptations of large scale pre-trained encoder-decoder models focusing on building context driven representation of the document and enabling specific attention to the information in the document. Additionally we provide a stronger BART baseline for these tasks. Our proposed techniques outperform existing methods on both automated (at least 48% increase in BLEU-4 points) and human evaluation for closeness to reference and relevance to the document. Furthermore we perform comprehensive manual inspection of the generated output and categorize errors to provide insights into future directions in modeling these tasks.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Focused Attention Improves Document-Grounded Generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-main.338" target="_blank">https://aclanthology.org/2021.naacl-main.338</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Machine reading comprehension is a challenging task especially for querying documents with deep and interconnected contexts. Transformer-based methods have shown advanced performances on this task; however most of them still treat documents as a flat sequence of tokens. This work proposes a new Transformer-based method that reads a document as tree slices. It contains two modules for identifying more relevant text passage and the best answer span respectively which are not only jointly trained but also jointly consulted at inference time. Our evaluation results show that our proposed method outperforms several competitive baseline approaches on two datasets from varied domains.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Does Structure Matter? Encoding Documents for Machine Reading Comprehension</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-main.367" target="_blank">https://aclanthology.org/2021.naacl-main.367</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al. 2019) and it incorporates an efficient encoding mechanism (Beltagy et al. 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al. 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-main.380" target="_blank">https://aclanthology.org/2021.naacl-main.380</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Knowledge-intensive tasks such as question answering often require assimilating information from different sections of large inputs such as books or article collections. We propose ReadTwice a simple and effective technique that combines several strengths of prior approaches to model long-range dependencies with Transformers. The main idea is to read text in small segments in parallel summarizing each segment into a memory table to be used in a second read of the text. We show that the method outperforms models of comparable size on several question answering (QA) datasets and sets a new state of the art on the challenging NarrativeQA task with questions about entire books.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ReadTwice: Reading Very Large Documents with Memories</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-main.408" target="_blank">https://aclanthology.org/2021.naacl-main.408</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a method for generating comparative summaries that highlight similarities and contradictions in input documents. The key challenge in creating such summaries is the lack of large parallel training data required for training typical summarization systems. To this end we introduce a hybrid generation approach inspired by traditional concept-to-text systems. To enable accurate comparison between different sources the model first learns to extract pertinent relations from input documents. The content planning component uses deterministic operators to aggregate these relations after identifying a subset for inclusion into a summary. The surface realization component lexicalizes this information using a text-infilling language model. By separately modeling content selection and realization we can effectively train them with limited annotations. We implemented and tested the model in the domain of nutrition and health -- rife with inconsistencies. Compared to conventional methods our framework leads to more faithful relevant and aggregation-sensitive summarization -- while being equally fluent.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Nutri-bullets Hybrid: Consensual Multi-document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-main.411" target="_blank">https://aclanthology.org/2021.naacl-main.411</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In many natural language processing applications identifying predictive text can be as important as the predictions themselves. When predicting medical diagnoses for example identifying predictive content in clinical notes not only enhances interpretability but also allows unknown descriptive (i.e. text-based) risk factors to be identified. We here formalize this problem as predictive extraction and address it using a simple mechanism based on linear attention. Our method preserves differentiability allowing scalable inference via stochastic gradient descent. Further the model decomposes predictions into a sum of contributions of distinct text spans. Importantly we require only document labels not ground-truth spans. Results show that our model identifies semantically-cohesive spans and assigns them scores that agree with human ratings while preserving classification performance.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SpanPredict: Extraction of Predictive Document Spans with Neural Attention</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-main.413" target="_blank">https://aclanthology.org/2021.naacl-main.413</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Although many end-to-end context-aware neural machine translation models have been proposed to incorporate inter-sentential contexts in translation these models can be trained only in domains where parallel documents with sentential alignments exist. We therefore present a simple method to perform context-aware decoding with any pre-trained sentence-level translation model by using a document-level language model. Our context-aware decoder is built upon sentence-level parallel data and target-side document-level monolingual data. From a theoretical viewpoint our core contribution is the novel representation of contextual information using point-wise mutual information between context and the current sentence. We demonstrate the effectiveness of our method on English to Russian translation by evaluating with BLEU and contrastive tests for context-aware translation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Context-aware Decoder for Neural Machine Translation using a Target-side Document-Level Language Model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-main.461" target="_blank">https://aclanthology.org/2021.naacl-main.461</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Neural-based summarization models suffer from the length limitation of text encoder. Long documents have to been truncated before they are sent to the model which results in huge loss of summary-relevant contents. To address this issue we propose the sliding selector network with dynamic memory for extractive summarization of long-form documents which employs a sliding window to extract summary sentences segment by segment. Moreover we adopt memory mechanism to preserve and update the history information dynamically allowing the semantic flow across different windows. Experimental results on two large-scale datasets that consist of scientific papers demonstrate that our model substantially outperforms previous state-of-the-art models. Besides we perform qualitative and quantitative investigations on how our model works and where the performance gain comes from.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Sliding Selector Network with Dynamic Memory for Extractive Summarization of Long Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-main.470" target="_blank">https://aclanthology.org/2021.naacl-main.470</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Current document embeddings require large training corpora but fail to learn high-quality representations when confronted with a small number of domain-specific documents and rare terms. Further they transform each document into a single embedding vector making it hard to capture different notions of document similarity or explain why two documents are considered similar. In this work we propose our Faceted Domain Encoder a novel approach to learn multifaceted embeddings for domain-specific documents. It is based on a Siamese neural network architecture and leverages knowledge graphs to further enhance the embeddings even if only a few training samples are available. The model identifies different types of domain knowledge and encodes them into separate dimensions of the embedding thereby enabling multiple ways of finding and comparing related documents in the vector space. We evaluate our approach on two benchmark datasets and find that it achieves the same embedding quality as state-of-the-art models while requiring only a tiny fraction of their training data. An interactive demo our source code and the evaluation datasets are available online: https://hpi.de/naumann/s/multifaceted-embeddings and a screencast is available on YouTube: https://youtu.be/HHcsX2clEwg</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multifaceted Domain-Specific Document Embeddings</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-demos.9" target="_blank">https://aclanthology.org/2021.naacl-demos.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>ActiveAnno is an annotation tool focused on document-level annotation tasks developed both for industry and research settings. It is designed to be a general-purpose tool with a wide variety of use cases. It features a modern and responsive web UI for creating annotation projects conducting annotations adjudicating disagreements and analyzing annotation results. ActiveAnno embeds a highly configurable and interactive user interface. The tool also integrates a RESTful API that enables integration into other software systems including an API for machine learning integration. ActiveAnno is built with extensible design and easy deployment in mind all to enable users to perform annotation tasks with high efficiency and high-quality annotation results.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ActiveAnno: General-Purpose Document-Level Annotation Tool with Active Learning Integration</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-demos.12" target="_blank">https://aclanthology.org/2021.naacl-demos.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a new information extraction system that can automatically construct temporal event graphs from a collection of news documents from multiple sources multiple languages (English and Spanish for our experiment) and multiple data modalities (speech text image and video). The system advances state-of-the-art from two aspects: (1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub with a demo video.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>RESIN: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.naacl-demos.16" target="_blank">https://aclanthology.org/2021.naacl-demos.16</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In a real-time simultaneous translation setting and neural machine translation (NMT) models start generating target language tokens from incomplete source language sentences and making them harder to translate and leading to poor translation quality. Previous research has shown that document-level NMT and comprising of sentence and context encoders and a decoder and leverages context from neighboring sentences and helps improve translation quality. In simultaneous translation settings and the context from previous sentences should be even more critical. To this end and in this paper and we propose wait-k simultaneous document-level NMT where we keep the context encoder as it is and replace the source sentence encoder and target language decoder with their wait-k equivalents. We experiment with low and high resource settings using the ALT and OpenSubtitles2018 corpora and where we observe minor improvements in translation quality. We then perform an analysis of the translations obtained using our models by focusing on sentences that should benefit from the context where we found out that the model does and in fact and benefit from context but is unable to effectively leverage it and especially in a low-resource setting. This shows that there is a need for further innovation in the way useful context is identified and leveraged.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Studying The Impact Of Document-level Context On Simultaneous Neural Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.mtsummit-research.17" target="_blank">https://aclanthology.org/2021.mtsummit-research.17</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Dense retrieval has shown great success for passage ranking in English. However its effectiveness for non-English languages remains unexplored due to limitation in training resources. In this work we explore different transfer techniques for document ranking from English annotations to non-English languages. Our experiments reveal that zero-shot model-based transfer using mBERT improves search quality. We find that weakly-supervised target language transfer is competitive compared to generation-based target language transfer which requires translation models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-Lingual Training of Dense Retrievers for Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.mrl-1.24" target="_blank">https://aclanthology.org/2021.mrl-1.24</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>La compression multi-phrases est utilis&#39;ee dans diff&#39;erentes t^aches de r&#39;esum&#39;e (microblogs opinions r&#39;eunions ou articles de presse). Leur objectif est de proposer une reformulation compress&#39;ee et grammaticalement correcte des phrases sources tout en gardant les faits principaux. Dans cet article nous pr&#39;esentons l&#39;&#39;etat de l&#39;art de la compression multi-phrases en mettant en avant les diff&#39;erents corpus et outils `a disposition. Nous axons notre analyse principalement sur la qualit&#39;e grammaticale et informative plus que sur le taux de compression.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Etat de l&#39;art en compression multi-phrases pour la synth`ese de documents (State-of-the-art of multi-sentence compression for document summarization)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.jeptalnrecital-recital.6" target="_blank">https://aclanthology.org/2021.jeptalnrecital-recital.6</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>L&#39;acc`es `a l&#39;information dans la documentation technique est une application particuli`ere et complexe du traitement du langage naturel et de la recherche d&#39;information. La difficult&#39;e tient aux contraintes propres des langages m&#39;etier sp&#39;ecialis&#39;es et semi-contr^ol&#39;es. Dans ce document nous proposons un outil d&#39;acc`es `a l&#39;information dans diff&#39;erents types de documents. Notre solution exploite conjointement la structure organisationnelle des documents et leur contenu informationnel pour extraire des informations m&#39;etier dans des diff&#39;erents corpus. Nous proposons un syst`eme bas&#39;e sur des interactions expert-machine dans un cycle d&#39;am&#39;elioration continu des mod`eles d&#39;extraction. Notre approche exploite des mod`eles d&#39;apprentissage `a faible supervision ne n&#39;ecessitant pas d&#39;expertise en ing&#39;enierie des langues. Notre syst`eme int`egre l&#39;utilisateur dans le processus de qualification de l&#39;information et permet de guider son apprentissage afin de rendre ses mod`eles plus performants au fil du temps.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Outil Interactif et &#39;Evolutif pour l&#39;Extraction d&#39;Information dans des Documents Techniques (Interactive and Evolutive Tool for Information Extraction in Technical Documents)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.jeptalnrecital-demo.4" target="_blank">https://aclanthology.org/2021.jeptalnrecital-demo.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Cet article pr&#39;esente notre participation `a l&#39;&#39;edition 2021 du D&#39;Efi Fouille de Textes (DEFT) et plus pr&#39;ecis&#39;ement `a la premi`ere t^ache li&#39;ee `a l&#39;identification du profil clinique du patient. Cette t^ache consiste `a s&#39;electionner pour un document d&#39;ecrivant l&#39;&#39;etat d&#39;un patient les diff&#39;erents types de maladies rencontr&#39;ees correspondant aux entr&#39;ees g&#39;en&#39;eriques des chapitres du MeSH (Medical Subject Headings). Dans notre travail nous nous sommes int&#39;eress&#39;es aux questions suivantes : (1) Comment am&#39;eliorer les repr&#39;esentations vectorielles de documents voire de classes ? (2) Comment apprendre des seuils de validation de classes ? Et (3) Une approche combinant apprentissage supervis&#39;e et similarit&#39;e s&#39;emantique peut-elle apporter une meilleure performance `a un syst`eme de classification multi-labels ?</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Participation de Berger-Levrault (BL.Research) `a DEFT 2021 : de l&#39;apprentissage des seuils de validation `a la classification multi-labels de documents (Berger-Levrault (BL)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.jeptalnrecital-deft.9" target="_blank">https://aclanthology.org/2021.jeptalnrecital-deft.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We argue that mainly due to technical innovation in the landscape of annotation tools a conceptual change in annotation models and processes is also on the horizon. It is diagnosed that these changes are bound up with multi-media and multi-perspective facilities of annotation tools in particular when considering virtual reality (VR) and augmented reality (AR) applications their potential ubiquitous use and the exploitation of externally trained natural language pre-processing methods. Such developments potentially lead to a dynamic and exploratory heuristic construction of the annotation process. With TextAnnotator an annotation suite is introduced which focuses on multi-mediality and multi-perspectivity with an interoperable set of task-specific annotation modules (e.g. for word classification rhetorical structures dependency trees semantic roles and more) and their linkage to VR and mobile implementations. The basic architecture and usage of TextAnnotator is described and related to the above mentioned shifts in the field.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unleashing annotations with TextAnnotator: Multimedia multi-perspective document views for ubiquitous annotation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.isa-1.7" target="_blank">https://aclanthology.org/2021.isa-1.7</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Data security and privacy is an issue of growing importance in the healthcare domain. In this paper we present an auditing system to detect privacy violations for unstructured text documents such as healthcare records. Given a sensitive document we present an anomaly detection algorithm that can find the top-k suspicious keyword queries that may have accessed the sensitive document. Since unstructured healthcare data such as medical reports and query logs are not easily available for public research in this paper we show how one can use the publicly available DBLP data to create an equivalent healthcare data and query log which can then be used for experimental evaluation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Auditing Keyword Queries Over Text Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.icon-main.46" target="_blank">https://aclanthology.org/2021.icon-main.46</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The exponential growth in the number of text documents produced daily on the web poses several difficulties to people who are responsible for collecting organizing and searching different textual content related to a particular topic. Automatic Text Summarization works well in this direction which can review many documents and pull out the relevant information. But the limitations associated with automatic text summarization need to be removed by finding efficient workarounds. Although current research works have focused on this direction for further improvements they still face many challenges. This paper proposes a combined semantic-based word and sentence similarity approach to summarize a corpus of text documents. To arrange the sentences in the final summary KL-divergence technique is used. The experimental work is conducted using DUC datasets and the obtained results are promising.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-document Text Summarization using Semantic Word and Sentence Similarity: A Combined Approach</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.icon-main.51" target="_blank">https://aclanthology.org/2021.icon-main.51</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Task2Dial Dataset: A Novel Dataset for Commonsense-enhanced Task-based Dialogue Grounded in Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.icnlsp-1.28" target="_blank">https://aclanthology.org/2021.icnlsp-1.28</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level human evaluation of machine translation (MT) has been raising interest in the community. However little is known about the issues of using document-level methodologies to assess MT quality. In this article we compare the inter-annotator agreement (IAA) scores the effort to assess the quality in different document-level methodologies and the issue of misevaluation when sentences are evaluated out of context.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards Document-Level Human MT Evaluation: On the Issues of Annotator Agreement Effort and Misevaluation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.humeval-1.4" target="_blank">https://aclanthology.org/2021.humeval-1.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recent studies emphasize the need of document context in human evaluation of machine translations but little research has been done on the impact of user interfaces on annotator productivity and the reliability of assessments. In this work we compare human assessment data from the last two WMT evaluation campaigns collected via two different methods for document-level evaluation. Our analysis shows that a document-centric approach to evaluation where the annotator is presented with the entire document context on a screen leads to higher quality segment and document level assessments. It improves the correlation between segment and document scores and increases inter-annotator agreement for document scores but is considerably more time consuming for annotators.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>On User Interfaces for Large-Scale Document-Level Human Evaluation of Machine Translation Outputs</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.humeval-1.11" target="_blank">https://aclanthology.org/2021.humeval-1.11</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Team Name: team-8 Embeddia Tool: Cross-Lingual Document Retrieval Zosa et al. Dataset: Estonian and Latvian news datasets abstract: Contemporary news media face increasing amounts of available data that can be of use when prioritizing selecting and discovering new news. In this work we propose a methodology for retrieving interesting articles in a cross-border news discovery setting. More specifically we explore how a set of seed documents in Estonian can be projected in Latvian document space and serve as a basis for discovery of novel interesting pieces of Latvian news that would interest Estonian readers. The proposed methodology was evaluated by Estonian journalist who confirmed that in the best setting from top 10 retrieved Latvian documents half of them represent news that are potentially interesting to be taken by the Estonian media house and presented to Estonian readers.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Interesting cross-border news discovery using cross-lingual article linking and document similarity</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.hackashop-1.16" target="_blank">https://aclanthology.org/2021.hackashop-1.16</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In the paper we deal with the problem of unsupervised text document clustering for the Polish language. Our goal is to compare the modern approaches based on language modeling (doc2vec and BERT) with the classical ones i.e. TF-IDF and wordnet-based. The experiments are conducted on three datasets containing qualification descriptions. The experiments&#39; results showed that wordnet-based similarity measures could compete and even outperform modern embedding-based approaches.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Text Document Clustering: Wordnet vs. TF-IDF vs. Word Embeddings</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.gwc-1.24" target="_blank">https://aclanthology.org/2021.gwc-1.24</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Developing documentation guidelines and easy-to-use templates for datasets and models is a challenging task especially given the variety of backgrounds skills and incentives of the people involved in the building of natural language processing (NLP) tools. Nevertheless the adoption of standard documentation practices across the field of NLP promotes more accessible and detailed descriptions of NLP datasets and models while supporting researchers and developers in reflecting on their work. To help with the standardization of documentation we present two case studies of efforts that aim to develop reusable documentation templates -- the HuggingFace data card a general purpose card for datasets in NLP and the GEM benchmark data and model cards with a focus on natural language generation. We describe our process for developing these templates including the identification of relevant stakeholder groups the definition of a set of guiding principles the use of existing templates as our foundation and iterative revisions based on feedback.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Reusable Templates and Guides For Documenting Datasets and Models for Natural Language Processing and Generation: A Case Study of the HuggingFace and GEM Data and Model Cards</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.gem-1.11" target="_blank">https://aclanthology.org/2021.gem-1.11</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DSC-IITISM at FinCausal 2021: Combining POS tagging with Attention-based Contextual Representations for Identifying Causal Relationships in Financial Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.fnp-1.8" target="_blank">https://aclanthology.org/2021.fnp-1.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The Financial Document Causality Detection Shared Task (FinCausal 2021)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.fnp-1.10" target="_blank">https://aclanthology.org/2021.fnp-1.10</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Daniel@FinTOC-2021: Taking Advantage of Images and Vectorial Shapes in Native PDF Document Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.fnp-1.13" target="_blank">https://aclanthology.org/2021.fnp-1.13</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Summarization of financial documents with TF-IDF weighting of multi-word terms</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.fnp-1.14" target="_blank">https://aclanthology.org/2021.fnp-1.14</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Not All Titles are Created Equal: Financial Document Structure Extraction Shared Task</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.fnp-1.16" target="_blank">https://aclanthology.org/2021.fnp-1.16</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>FINTOC 2021 - Document Structure Understanding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.fnp-1.17" target="_blank">https://aclanthology.org/2021.fnp-1.17</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Joint abstractive and extractive method for long financial document summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.fnp-1.19" target="_blank">https://aclanthology.org/2021.fnp-1.19</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CILAB@FinTOC-2021 Shared Task: Title Detection and Table of Content Extraction for Financial Document</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.fnp-1.20" target="_blank">https://aclanthology.org/2021.fnp-1.20</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The Financial Document Structure Extraction Shared Task (FinTOC2021)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.fnp-1.21" target="_blank">https://aclanthology.org/2021.fnp-1.21</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level event extraction is critical to various natural language processing tasks for providing structured information. Existing approaches by sequential modeling neglect the complex logic structures for long texts. In this paper we leverage the entity interactions and sentence interactions within long documents and transform each document into an undirected unweighted graph by exploiting the relationship between sentences. We introduce the Sentence Community to represent each event as a subgraph. Furthermore our framework SCDEE maintains the ability to extract multiple events by sentence community detection using graph attention networks and alleviate the role overlapping issue by predicting arguments in terms of roles. Experiments demonstrate that our framework achieves competitive results over state-of-the-art methods on the large-scale document-level event extraction dataset.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploring Sentence Community for Document-Level Event Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-emnlp.32" target="_blank">https://aclanthology.org/2021.findings-emnlp.32</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Emotion cause extraction (ECE) aims to extract the causes behind the certain emotion in text. Some works related to the ECE task have been published and attracted lots of attention in recent years. However these methods neglect two major issues: 1) pay few attentions to the effect of document-level context information on ECE and 2) lack of sufficient exploration for how to effectively use the annotated emotion clause. For the first issue we propose a bidirectional hierarchical attention network (BHA) corresponding to the specified candidate cause clause to capture the document-level context in a structured and dynamic manner. For the second issue we design an emotional filtering module (EF) for each layer of the graph attention network which calculates a gate score based on the emotion clause to filter the irrelevant information. Combining the BHA and EF the EF-BHA can dynamically aggregate the contextual information from two directions and filters irrelevant information. The experimental results demonstrate that EF-BHA achieves the competitive performances on two public datasets in different languages (Chinese and English). Moreover we quantify the effect of context on emotion cause extraction and provide the visualization of the interactions between candidate cause clauses and contexts.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Bidirectional Hierarchical Attention Networks based on Document-level Context for Emotion Cause Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-emnlp.51" target="_blank">https://aclanthology.org/2021.findings-emnlp.51</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Paraphrase generation is an important task in natural language processing. Previous works focus on sentence-level paraphrase generation while ignoring document-level paraphrase generation which is a more challenging and valuable task. In this paper we explore the task of document-level paraphrase generation for the first time and focus on the inter-sentence diversity by considering sentence rewriting and reordering. We propose CoRPG (Coherence Relationship guided Paraphrase Generation) which leverages graph GRU to encode the coherence relationship graph and get the coherence-aware representation for each sentence which can be used for re-arranging the multiple (possibly modified) input sentences. We create a pseudo document-level paraphrase dataset for training CoRPG. Automatic evaluation results show CoRPG outperforms several strong baseline models on the BERTScore and diversity scores. Human evaluation also shows our model can generate document paraphrase with more diversity and semantic preservation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards Document-Level Paraphrase Generation with Sentence Rewriting and Reordering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-emnlp.89" target="_blank">https://aclanthology.org/2021.findings-emnlp.89</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper we propose a novel abstractive MDS model in which we represent multiple documents as a heterogeneous graph taking semantic nodes of different granularities into account and then apply a graph-to-sequence framework to generate summaries. Moreover we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that ``summarizes&#39;&#39; texts into a more abstract format i.e. a topic distribution we adopt a multi-task learning strategy to jointly train the topic and summarization module allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge scores and human evaluation meanwhile learns high-quality topics.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Topic-Guided Abstractive Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-emnlp.126" target="_blank">https://aclanthology.org/2021.findings-emnlp.126</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Transformer-based pre-trained models such as BERT have achieved remarkable results on machine reading comprehension. However due to the constraint of encoding length (e.g. 512 WordPiece tokens) a long document is usually split into multiple chunks that are independently read. It results in the reading field being limited to individual chunks without information collaboration for long document machine reading comprehension. To address this problem we propose RoR a read-over-read method which expands the reading field from chunk to document. Specifically RoR includes a chunk reader and a document reader. The former first predicts a set of regional answers for each chunk which are then compacted into a highly-condensed version of the original document guaranteeing to be encoded once. The latter further predicts the global answers from this condensed document. Eventually a voting strategy is utilized to aggregate and rerank the regional and global answers for final prediction. Extensive experiments on two benchmarks QuAC and TriviaQA demonstrate the effectiveness of RoR for long document reading. Notably RoR ranks 1st place on the QuAC leaderboard (https://quac.ai/) at the time of submission (May 17th 2021).</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>RoR: Read-over-Read for Long Document Machine Reading Comprehension</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-emnlp.160" target="_blank">https://aclanthology.org/2021.findings-emnlp.160</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Reviewing contracts is a time-consuming procedure that incurs large expenses to companies and social inequality to those who cannot afford it. In this work we propose ``document-level natural language inference (NLI) for contracts&#39;&#39; a novel real-world application of NLI that addresses such problems. In this task a system is given a set of hypotheses (such as ``Some obligations of Agreement may survive termination.&#39;&#39;) and a contract and it is asked to classify whether each hypothesis is ``entailed by&#39;&#39; ``contradicting to&#39;&#39; or ``not mentioned by&#39;&#39; (neutral to) the contract as well as identifying ``evidence&#39;&#39; for the decision as spans in the contract. We annotated and release the largest corpus to date consisting of 607 annotated contracts. We then show that existing models fail badly on our task and introduce a strong baseline which (a) models evidence identification as multi-label classification over spans instead of trying to predict start and end tokens and (b) employs more sophisticated context segmentation for dealing with long documents. We also show that linguistic characteristics of contracts such as negations by exceptions are contributing to the difficulty of this task and that there is much room for improvement.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-emnlp.164" target="_blank">https://aclanthology.org/2021.findings-emnlp.164</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Existing unsupervised document hashing methods are mostly established on generative models. Due to the difficulties of capturing long dependency structures these methods rarely model the raw documents directly but instead to model the features extracted from them (textite.g. bag-of-words (BOG) TFIDF). In this paper we propose to learn hash codes from BERT embeddings after observing their tremendous successes on downstream tasks. As a first try we modify existing generative hashing models to accommodate the BERT embeddings. However little improvement is observed over the codes learned from the old BOG or TFIDF features. We attribute this to the reconstruction requirement in the generative hashing which will enforce irrelevant information that is abundant in the BERT embeddings also compressed into the codes. To remedy this issue a new unsupervised hashing paradigm is further proposed based on the mutual information (MI) maximization principle. Specifically the method first constructs appropriate global and local codes from the documents and then seeks to maximize their mutual information. Experimental results on three benchmark datasets demonstrate that the proposed method is able to generate hash codes that outperform existing ones learned from BOG features by a substantial margin.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Refining BERT Embeddings for Document Hashing via Mutual Information Maximization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-emnlp.203" target="_blank">https://aclanthology.org/2021.findings-emnlp.203</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Language use differs between domains and even within a domain language use changes over time. For pre-trained language models like BERT domain adaptation through continued pre-training has been shown to improve performance on in-domain downstream tasks. In this article we investigate whether temporal adaptation can bring additional benefits. For this purpose we introduce a corpus of social media comments sampled over three years. It contains unlabelled data for adaptation and evaluation on an upstream masked language modelling task as well as labelled data for fine-tuning and evaluation on a downstream document classification task. We find that temporality matters for both tasks: temporal adaptation improves upstream and temporal fine-tuning downstream task performance. Time-specific models generally perform better on past than on future test sets which matches evidence on the bursty usage of topical words. However adapting BERT to time and domain does not improve performance on the downstream task over only adapting to domain. Token-level analysis shows that temporal adaptation captures event-driven changes in language use in the downstream task but not those changes that are actually relevant to task performance. Based on our findings we discuss when temporal adaptation may be more effective.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Temporal Adaptation of BERT and Performance on Downstream Document Classification: Insights from Social Media</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-emnlp.206" target="_blank">https://aclanthology.org/2021.findings-emnlp.206</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Transformer-based pre-training techniques of text and layout have proven effective in a number of document understanding tasks. Despite this success multimodal pre-training models suffer from very high computational and memory costs. Motivated by human reading strategies this paper presents Skim-Attention a new attention mechanism that takes advantage of the structure of the document and its layout. Skim-Attention only attends to the 2-dimensional position of the words in a document. Our experiments show that Skim-Attention obtains a lower perplexity than prior works while being more computationally efficient. Skim-Attention can be further combined with long-range Transformers to efficiently process long documents. We also show how Skim-Attention can be used off-the-shelf as a mask for any Pre-trained Language Model allowing to improve their performance while restricting attention. Finally we show the emergence of a document structure representation in Skim-Attention.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Skim-Attention: Learning to Focus via Document Layout</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-emnlp.207" target="_blank">https://aclanthology.org/2021.findings-emnlp.207</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We introduce a new pretraining approach geared for multi-document language modeling incorporating two key ideas into the masked language modeling self-supervised objective. First instead of considering documents in isolation we pretrain over sets of multiple related documents encouraging the model to learn cross-document relationships. Second we improve over recent long-range transformers by introducing dynamic global attention that has access to the entire input to predict masked tokens. We release CDLM (Cross-Document Language Model) a new general language model for multi-document setting that can be easily applied to downstream tasks. Our extensive analysis shows that both ideas are essential for the success of CDLM and work in synergy to set new state-of-the-art results for several multi-text tasks.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CDLM: Cross-Document Language Modeling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-emnlp.225" target="_blank">https://aclanthology.org/2021.findings-emnlp.225</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Much of natural language processing is focused on leveraging large capacity language models typically trained over single messages with a task of predicting one or more tokens. However modeling human language at higher-levels of context (i.e. sequences of messages) is under-explored. In stance detection and other social media tasks where the goal is to predict an attribute of a message we have contextual data that is loosely semantically connected by authorship. Here we introduce Message-Level Transformer (MeLT) -- a hierarchical message-encoder pre-trained over Twitter and applied to the task of stance prediction. We focus on stance prediction as a task benefiting from knowing the context of the message (i.e. the sequence of previous messages). The model is trained using a variant of masked-language modeling; where instead of predicting tokens it seeks to generate an entire masked (aggregated) message vector via reconstruction loss. We find that applying this pre-trained masked message-level transformer to the downstream task of stance detection achieves F1 performance of 67%.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MeLT: Message-Level Transformer with Masked Document Representations as Pre-Training for Stance Detection</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-emnlp.253" target="_blank">https://aclanthology.org/2021.findings-emnlp.253</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recent progress in pretrained Transformer-based language models has shown great success in learning contextual representation of text. However due to the quadratic self-attention complexity most of the pretrained Transformers models can only handle relatively short text. It is still a challenge when it comes to modeling very long documents. In this work we propose to use a graph attention network on top of the available pretrained Transformers model to learn document embeddings. This graph attention network allows us to leverage the high-level semantic structure of the document. In addition based on our graph document model we design a simple contrastive learning strategy to pretrain our models on a large amount of unlabeled corpus. Empirically we demonstrate the effectiveness of our approaches in document classification and document retrieval tasks.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Contrastive Document Representation Learning with Graph Attention Networks</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-emnlp.327" target="_blank">https://aclanthology.org/2021.findings-emnlp.327</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents an unsupervised extractive approach to summarize scientific long documents based on the Information Bottleneck principle. Inspired by previous work which uses the Information Bottleneck principle for sentence compression we extend it to document level summarization with two separate steps. In the first step we use signal(s) as queries to retrieve the key content from the source document. Then a pre-trained language model conducts further sentence search and edit to return the final extracted summaries. Importantly our work can be flexibly extended to a multi-view framework by different signals. Automatic evaluation on three scientific document datasets verifies the effectiveness of the proposed framework. The further human evaluation suggests that the extracted summaries cover more content aspects than previous systems.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Leveraging Information Bottleneck for Scientific Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-emnlp.345" target="_blank">https://aclanthology.org/2021.findings-emnlp.345</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Jupyter notebook allows data scientists to write machine learning code together with its documentation in cells. In this paper we propose a new task of code documentation generation (CDG) for computational notebooks. In contrast to the previous CDG tasks which focus on generating documentation for single code snippets in a computational notebook one documentation in a markdown cell often corresponds to multiple code cells and these code cells have an inherent structure. We proposed a new model (HAConvGNN) that uses a hierarchical attention mechanism to consider the relevant code cells and the relevant code tokens information when generating the documentation. Tested on a new corpus constructed from well-documented Kaggle notebooks we show that our model outperforms other baseline models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>HAConvGNN: Hierarchical Attention Based Convolutional Graph Neural Network for Code Documentation Generation in Jupyter Notebooks</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-emnlp.381" target="_blank">https://aclanthology.org/2021.findings-emnlp.381</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Spatial Dependency Parsing for Semi-Structured Document Information Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-acl.28" target="_blank">https://aclanthology.org/2021.findings-acl.28</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Entity-Aware Abstractive Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-acl.30" target="_blank">https://aclanthology.org/2021.findings-acl.30</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SIRE: Separate Intra- and Inter-sentential Reasoning for Document-level Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-acl.47" target="_blank">https://aclanthology.org/2021.findings-acl.47</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MRN: A Locally and Globally Mention-Based Reasoning Network for Document-Level Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-acl.117" target="_blank">https://aclanthology.org/2021.findings-acl.117</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Discriminative Reasoning for Document-level Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-acl.144" target="_blank">https://aclanthology.org/2021.findings-acl.144</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Documents Representation via Generalized Coupled Tensor Chain with the Rotation Group constraint</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-acl.146" target="_blank">https://aclanthology.org/2021.findings-acl.146</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CiteWorth: Cite-Worthiness Detection for Improved Scientific Document Understanding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-acl.157" target="_blank">https://aclanthology.org/2021.findings-acl.157</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Document Sketching: Generating Drafts from Analogous Texts</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-acl.185" target="_blank">https://aclanthology.org/2021.findings-acl.185</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DocOIE: A Document-level Context-Aware Dataset for OpenIE</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-acl.210" target="_blank">https://aclanthology.org/2021.findings-acl.210</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Neural Edge-Editing Approach for Document-Level Relation Graph Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-acl.234" target="_blank">https://aclanthology.org/2021.findings-acl.234</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Self-Supervised Document Similarity Ranking via Contextualized Language Models and Hierarchical Inference</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-acl.272" target="_blank">https://aclanthology.org/2021.findings-acl.272</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>AgreeSum: Agreement-Oriented Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-acl.299" target="_blank">https://aclanthology.org/2021.findings-acl.299</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Scaling Within Document Coreference to Long Texts</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-acl.343" target="_blank">https://aclanthology.org/2021.findings-acl.343</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DocNLI: A Large-scale Dataset for Document-level Natural Language Inference</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-acl.435" target="_blank">https://aclanthology.org/2021.findings-acl.435</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Highlight-Transformer: Leveraging Key Phrase Aware Attention to Improve Abstractive Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-acl.445" target="_blank">https://aclanthology.org/2021.findings-acl.445</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-document Coreference Resolution over Predicted Mentions</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.findings-acl.453" target="_blank">https://aclanthology.org/2021.findings-acl.453</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Automatic fact-checking is crucial for recognizing misinformation spreading on the internet. Most existing fact-checkers break down the process into several subtasks one of which determines candidate evidence sentences that can potentially support or refute the claim to be verified; typically evidence sentences with gold-standard labels are needed for this. In a more realistic setting however such sentence-level annotations are not available. In this paper we tackle the natural language inference (NLI) subtask---given a document and a (sentence) claim determine whether the document supports or refutes the claim---only using document-level annotations. Using fine-tuned BERT and multiple instance learning we achieve 81.9% accuracy significantly outperforming the existing results on the WikiFactCheck-English dataset.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Fact-Checking with Document-level Annotations using BERT and Multiple Instance Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.fever-1.11" target="_blank">https://aclanthology.org/2021.fever-1.11</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level relation extraction aims to identify relations between entities in a whole document. Prior efforts to capture long-range dependencies have relied heavily on implicitly powerful representations learned through (graph) neural networks which makes the model less transparent. To tackle this challenge in this paper we propose LogiRE a novel probabilistic model for document-level relation extraction by learning logic rules. LogiRE treats logic rules as latent variables and consists of two modules: a rule generator and a relation extractor. The rule generator is to generate logic rules potentially contributing to final predictions and the relation extractor outputs final predictions based on the generated logic rules. Those two modules can be efficiently optimized with the expectation-maximization (EM) algorithm. By introducing logic rules into neural networks LogiRE can explicitly capture long-range dependencies as well as enjoy better interpretation. Empirical results show that significantly outperforms several strong baselines in terms of relation performance and logical consistency. Our code is available at https://github.com/rudongyu/LogiRE.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learning Logic Rules for Document-Level Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.95" target="_blank">https://aclanthology.org/2021.emnlp-main.95</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Large language models have led to remarkable progress on many NLP tasks and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al. 2020) a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself and find machine-generated text (e.g. from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset we evaluate the text that was removed and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.98" target="_blank">https://aclanthology.org/2021.emnlp-main.98</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Performing event and entity coreference resolution across documents vastly increases the number of candidate mentions making it intractable to do the full n^2 pairwise comparisons. Existing approaches simplify by considering coreference only within document clusters but this fails to handle inter-cluster coreference common in many applications. As a result cross-document coreference algorithms are rarely applied to downstream tasks. We draw on an insight from discourse coherence theory: potential coreferences are constrained by the reader&#39;s discourse focus. We model the entities/events in a reader&#39;s focus as a neighborhood within a learned latent embedding space which minimizes the distance between mentions and the centroids of their gold coreference clusters. We then use these neighborhoods to sample only hard negatives to train a fine-grained classifier on mention pairs and their local discourse features. Our approach achieves state-of-the-art results for both events and entities on the ECB+ Gun Violence Football Coreference and Cross-Domain Cross-Document Coreference corpora. Furthermore training on multiple corpora improves average performance across all datasets by 17.2 F1 points leading to a robust coreference resolution model that is now feasible to apply to downstream tasks.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Focus on what matters: Applying Discourse Coherence Theory to Cross Document Coreference</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.106" target="_blank">https://aclanthology.org/2021.emnlp-main.106</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Identifying relevant knowledge to be used in conversational systems that are grounded in long documents is critical to effective response generation. We introduce a knowledge identification model that leverages the document structure to provide dialogue-contextualized passage encodings and better locate knowledge relevant to the conversation. An auxiliary loss captures the history of dialogue-document connections. We demonstrate the effectiveness of our model on two document-grounded conversational datasets and provide analyses showing generalization to unseen documents and long dialogue contexts.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DIALKI: Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.140" target="_blank">https://aclanthology.org/2021.emnlp-main.140</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Natural language generation (NLG) tasks on pro-drop languages are known to suffer from zero pronoun (ZP) problems and the problems remain challenging due to the scarcity of ZP-annotated NLG corpora. In this case we propose a highly adaptive two-stage approach to couple context modeling with ZP recovering to mitigate the ZP problem in NLG tasks. Notably we frame the recovery process in a task-supervised fashion where the ZP representation recovering capability is learned during the NLG task learning process thus our method does not require NLG corpora annotated with ZPs. For system enhancement we learn an adversarial bot to adjust our model outputs to alleviate the error propagation caused by mis-recovered ZPs. Experiments on three document-level NLG tasks i.e. machine translation question answering and summarization show that our approach can improve the performance to a great extent and the improvement on pronoun translation is very impressive.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Coupling Context Modeling with Zero Pronoun Recovering for Document-Level Natural Language Generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.197" target="_blank">https://aclanthology.org/2021.emnlp-main.197</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Event factuality indicates the degree of certainty about whether an event occurs in the real world. Existing studies mainly focus on identifying event factuality at sentence level which easily leads to conflicts between different mentions of the same event. To this end we study the problem of document-level event factuality identification which determines the event factuality from the view of a document. For this task we need to consider two important characteristics: Local Uncertainty and Global Structure which can be utilized to improve performance. In this paper we propose an Uncertain Local-to-Global Network (ULGN) to make use of these two characteristics. Specifically we devise a Local Uncertainty Estimation module to model the uncertainty of local information. Moreover we propose an Uncertain Information Aggregation module to leverage the global structure for integrating the local information. Experimental results demonstrate the effectiveness of our proposed method outperforming the previous state-of-the-art model by 8.4% and 11.45% of F1 score on two widely used datasets.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Uncertain Local-to-Global Networks for Document-Level Event Factuality Identification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.207" target="_blank">https://aclanthology.org/2021.emnlp-main.207</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Previous works on key information extraction from visually rich documents (VRDs) mainly focus on labeling the text within each bounding box (i.e.semantic entity) while the relations in-between are largely unexplored. In this paper we adapt the popular dependency parsing model the biaffine parser to this entity relation extraction task. Being different from the original dependency parsing model which recognizes dependency relations between words we identify relations between groups of words with layout information instead. We have compared different representations of the semantic entity different VRD encoders and different relation decoders. For the model training we explore multi-task learning to combine entity labeling and relation extraction tasks; and for the evaluation we conduct experiments on different datasets with filtering and augmentation. The results demonstrate that our proposed model achieves 65.96% F1 score on the FUNSD dataset. As for the real-world application our model has been applied to the in-house customs data achieving reliable performance in the production setting.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Entity Relation Extraction as Dependency Parsing in Visually Rich Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.218" target="_blank">https://aclanthology.org/2021.emnlp-main.218</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Multi-label document classification associating one document instance with a set of relevant labels is attracting more and more research attention. Existing methods explore the incorporation of information beyond text such as document metadata or label structure. These approaches however either simply utilize the semantic information of metadata or employ the predefined parent-child label hierarchy ignoring the heterogeneous graphical structures of metadata and labels which we believe are crucial for accurate multi-label document classification. Therefore in this paper we propose a novel neural network based approach for multi-label document classification in which two heterogeneous graphs are constructed and learned using heterogeneous graph transformers. One is metadata heterogeneous graph which models various types of metadata and their topological relations. The other is label heterogeneous graph which is constructed based on both the labels&#39; hierarchy and their statistical dependencies. Experimental results on two benchmark datasets show the proposed approach outperforms several state-of-the-art baselines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Beyond Text: Incorporating Metadata and Label Structure for Multi-Label Document Classification using Heterogeneous Graphs</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.253" target="_blank">https://aclanthology.org/2021.emnlp-main.253</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recently a number of approaches have been proposed to improve translation performance for document-level neural machine translation (NMT). However few are focusing on the subject of lexical translation consistency. In this paper we apply ``one translation per discourse&#39;&#39; in NMT and aim to encourage lexical translation consistency for document-level NMT. This is done by first obtaining a word link for each source word in a document which tells the positions where the source word appears. Then we encourage the translation of those words within a link to be consistent in two ways. On the one hand when encoding sentences within a document we properly share context information of those words. On the other hand we propose an auxiliary loss function to better constrain that their translation should be consistent. Experimental results on Chinese↔English and English→French translation tasks show that our approach not only achieves state-of-the-art performance in BLEU scores but also greatly improves lexical consistency in translation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Encouraging Lexical Translation Consistency for Document-Level Neural Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.262" target="_blank">https://aclanthology.org/2021.emnlp-main.262</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>A real-world information extraction (IE) system for semi-structured document images often involves a long pipeline of multiple modules whose complexity dramatically increases its development and maintenance cost. One can instead consider an end-to-end model that directly maps the input to the target output and simplify the entire process. However such generation approach is known to lead to unstable performance if not designed carefully. Here we present our recent effort on transitioning from our existing pipeline-based IE system to an end-to-end system focusing on practical challenges that are associated with replacing and deploying the system in real large-scale production. By carefully formulating document IE as a sequence generation task we show that a single end-to-end IE system can be built and still achieve competent performance.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cost-effective End-to-end Information Extraction for Semi-structured Document Images</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.271" target="_blank">https://aclanthology.org/2021.emnlp-main.271</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we propose a new ranking model DR-BERT which improves the Document Retrieval (DR) task by a task-adaptive training process and a Segmented Token Recovery Mechanism (STRM). In the task-adaptive training we first pre-train DR-BERT to be domain-adaptive and then make the two-phase fine-tuning. In the first-phase fine-tuning the model learns query-document matching patterns regarding different query types in a pointwise way. Next in the second-phase fine-tuning the model learns document-level ranking features and ranks documents with regard to a given query in a listwise manner. Such pointwise plus listwise fine-tuning enables the model to minimize errors in the document ranking by incorporating ranking-specific supervisions. Meanwhile the model derived from pointwise fine-tuning is also used to reduce noise in the training data of the listwise fine-tuning. On the other hand we present STRM which can compute OOV word representation and contextualization more precisely in BERT-based models. As an effective strategy in DR-BERT STRM improves the matching perfromance of OOV words between a query and a document. Notably our DR-BERT model keeps in the top three on the MS MARCO leaderboard since May 20 2020.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Enhancing Document Ranking with Task-adaptive Training and Segmented Token Recovery Mechanism</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.289" target="_blank">https://aclanthology.org/2021.emnlp-main.289</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Most of existing extractive multi-document summarization (MDS) methods score each sentence individually and extract salient sentences one by one to compose a summary which have two main drawbacks: (1) neglecting both the intra and cross-document relations between sentences; (2) neglecting the coherence and conciseness of the whole summary. In this paper we propose a novel MDS framework (SgSum) to formulate the MDS task as a sub-graph selection problem in which source documents are regarded as a relation graph of sentences (e.g. similarity graph or discourse graph) and the candidate summaries are its sub-graphs. Instead of selecting salient sentences SgSum selects a salient sub-graph from the relation graph as the summary. Comparing with traditional methods our method has two main advantages: (1) the relations between sentences are captured by modeling both the graph structure of the whole document set and the candidate sub-graphs; (2) directly outputs an integrate summary in the form of sub-graph which is more informative and coherent. Extensive experiments on MultiNews and DUC datasets show that our proposed method brings substantial improvements over several strong baselines. Human evaluation results also demonstrate that our model can produce significantly more coherent and informative summaries compared with traditional MDS methods. Moreover the proposed architecture has strong transfer ability from single to multi-document input which can reduce the resource bottleneck in MDS tasks.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SgSum:Transforming Multi-document Summarization into Sub-graph Selection</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.333" target="_blank">https://aclanthology.org/2021.emnlp-main.333</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Existing relation extraction (RE) methods typically focus on extracting relational facts between entity pairs within single sentences or documents. However a large quantity of relational facts in knowledge bases can only be inferred across documents in practice. In this work we present the problem of cross-document RE making an initial step towards knowledge acquisition in the wild. To facilitate the research we construct the first human-annotated cross-document RE dataset CodRED. Compared to existing RE datasets CodRED presents two key challenges: Given two entities (1) it requires finding the relevant documents that can provide clues for identifying their relations; (2) it requires reasoning over multiple documents to extract the relational facts. We conduct comprehensive experiments to show that CodRED is challenging to existing RE methods including strong BERT-based models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CodRED: A Cross-Document Relation Extraction Dataset for Acquiring Knowledge in the Wild</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.366" target="_blank">https://aclanthology.org/2021.emnlp-main.366</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Relating entities and events in text is a key component of natural language understanding. Cross-document coreference resolution in particular is important for the growing interest in multi-document analysis tasks. In this work we propose a new model that extends the efficient sequential prediction paradigm for coreference resolution to cross-document settings and achieves competitive results for both entity and event coreference while providing strong evidence of the efficacy of both sequential models and higher-order inference in cross-document settings. Our model incrementally composes mentions into cluster representations and predicts links between a mention and the already constructed clusters approximating a higher-order model. In addition we conduct extensive ablation studies that provide new insights into the importance of various inputs and representation types in coreference.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Sequential Cross-Document Coreference Resolution</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.382" target="_blank">https://aclanthology.org/2021.emnlp-main.382</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level entity-based extraction (EE) aiming at extracting entity-centric information such as entity roles and entity relations is key to automatic knowledge acquisition from text corpora for various domains. Most document-level EE systems build extractive models which struggle to model long-term dependencies among entities at the document level. To address this issue we propose a generative framework for two document-level EE tasks: role-filler entity extraction (REE) and relation extraction (RE). We first formulate them as a template generation problem allowing models to efficiently capture cross-entity dependencies exploit label semantics and avoid the exponential computation complexity of identifying N-ary relations. A novel cross-attention guided copy mechanism TopK Copy is incorporated into a pre-trained sequence-to-sequence model to enhance the capabilities of identifying key information in the input document. Experiments done on the MUC-4 and SciREX dataset show new state-of-the-art results on REE (+3.26%) binary RE (+4.8%) and 4-ary RE (+2.7%) in F1 score.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-level Entity-based Extraction as Template Generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.426" target="_blank">https://aclanthology.org/2021.emnlp-main.426</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Extracting relations across large text spans has been relatively underexplored in NLP but it is particularly important for high-value domains such as biomedicine where obtaining high recall of the latest findings is crucial for practical applications. Compared to conventional information extraction confined to short text spans document-level relation extraction faces additional challenges in both inference and learning. Given longer text spans state-of-the-art neural architectures are less effective and task-specific self-supervision such as distant supervision becomes very noisy. In this paper we propose decomposing document-level relation extraction into relation detection and argument resolution taking inspiration from Davidsonian semantics. This enables us to incorporate explicit discourse modeling and leverage modular self-supervision for each sub-problem which is less noise-prone and can be further refined end-to-end via variational EM. We conduct a thorough evaluation in biomedical machine reading for precision oncology where cross-paragraph relation mentions are prevalent. Our method outperforms prior state of the art such as multi-scale learning and graph neural networks by over 20 absolute F1 points. The gain is particularly pronounced among the most challenging relation instances whose arguments never co-occur in a paragraph.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Modular Self-Supervision for Document-Level Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.429" target="_blank">https://aclanthology.org/2021.emnlp-main.429</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The task of Event Detection (ED) in Information Extraction aims to recognize and classify trigger words of events in text. The recent progress has featured advanced transformer-based language models (e.g. BERT) as a critical component in state-of-the-art models for ED. However the length limit for input texts is a barrier for such ED models as they cannot encode long-range document-level context that has been shown to be beneficial for ED. To address this issue we propose a novel method to model document-level context for ED that dynamically selects relevant sentences in the document for the event prediction of the target sentence. The target sentence will be then augmented with the selected sentences and consumed entirely by transformer-based language models for improved representation learning for ED. To this end the REINFORCE algorithm is employed to train the relevant sentence selection for ED. Several information types are then introduced to form the reward function for the training process including ED performance sentence similarity and discourse relations. Our extensive experiments on multiple benchmark datasets reveal the effectiveness of the proposed model leading to new state-of-the-art performance.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Modeling Document-Level Context for Event Detection via Important Context Selection</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.439" target="_blank">https://aclanthology.org/2021.emnlp-main.439</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Multi-label document classification (MLDC) problems can be challenging especially for long documents with a large label set and a long-tail distribution over labels. In this paper we present an effective convolutional attention network for the MLDC problem with a focus on medical code prediction from clinical documents. Our innovations are three-fold: (1) we utilize a deep convolution-based encoder with the squeeze-and-excitation networks and residual networks to aggregate the information across the document and learn meaningful document representations that cover different ranges of texts; (2) we explore multi-layer and sum-pooling attention to extract the most informative features from these multi-scale representations; (3) we combine binary cross entropy loss and focal loss to improve performance for rare labels. We focus our evaluation study on MIMIC-III a widely used dataset in the medical domain. Our models outperform prior work on medical coding and achieve new state-of-the-art results on multiple metrics. We also demonstrate the language independent nature of our approach by applying it to two non-English datasets. Our model outperforms prior best model and a multilingual Transformer model by a substantial margin.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Effective Convolutional Attention Network for Multi-label Clinical Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.481" target="_blank">https://aclanthology.org/2021.emnlp-main.481</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Question Answering (QA) tasks requiring information from multiple documents often rely on a retrieval model to identify relevant information for reasoning. The retrieval model is typically trained to maximize the likelihood of the labeled supporting evidence. However when retrieving from large text corpora such as Wikipedia the correct answer can often be obtained from multiple evidence candidates. Moreover not all such candidates are labeled as positive during annotation rendering the training signal weak and noisy. This problem is exacerbated when the questions are unanswerable or when the answers are Boolean since the model cannot rely on lexical overlap to make a connection between the answer and supporting evidence. We develop a new parameterization of set-valued retrieval that handles unanswerable queries and we show that marginalizing over this set during training allows a model to mitigate false negatives in supporting evidence annotations. We test our method on two multi-document QA datasets IIRC and HotpotQA. On IIRC we show that joint modeling with marginalization improves model performance by 5.5 F1 points and achieves a new state-of-the-art performance of 50.5 F1. We also show that retrieval marginalization results in 4.1 QA F1 improvement over a non-marginalized baseline on HotpotQA in the fullwiki setting.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Mitigating False-Negative Contexts in Multi-document Question Answering with Retrieval Marginalization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.497" target="_blank">https://aclanthology.org/2021.emnlp-main.497</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We propose MultiDoc2Dial a new task and dataset on modeling goal-oriented dialogues grounded in multiple documents. Most previous works treat document-grounded dialogue modeling as machine reading comprehension task based on a single given document or passage. In this work we aim to address more realistic scenarios where a goal-oriented information-seeking conversation involves multiple topics and hence is grounded on different documents. To facilitate such task we introduce a new dataset that contains dialogues grounded in multiple documents from four different domains. We also explore modeling the dialogue-based and document-based contexts in the dataset. We present strong baseline approaches and various experimental results aiming to support further research efforts on such a task.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.498" target="_blank">https://aclanthology.org/2021.emnlp-main.498</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We introduce MULTI-EURLEX a new multilingual dataset for topic classification of legal documents. The dataset comprises 65k European Union (EU) laws officially translated in 23 languages annotated with multiple labels from the EUROVOC taxonomy. We highlight the effect of temporal concept drift and the importance of chronological instead of random splits. We use the dataset as a testbed for zero-shot cross-lingual transfer where we exploit annotated training documents in one language (source) to classify documents in another language (target). We find that fine-tuning a multilingually pretrained model (XLM-ROBERTA MT5) in a single source language leads to catastrophic forgetting of multilingual knowledge and consequently poor zero-shot transfer to other languages. Adaptation strategies namely partial fine-tuning adapters BITFIT LNFIT originally proposed to accelerate fine-tuning for new end-tasks help retain multilingual knowledge from pretraining substantially improving zero-shot cross-lingual transfer but their impact also depends on the pretrained model used and the size of the label set.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MultiEURLEX - A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.559" target="_blank">https://aclanthology.org/2021.emnlp-main.559</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>To assess the effectiveness of any medical intervention researchers must conduct a time-intensive and manual literature review. NLP systems can help to automate or assist in parts of this expensive process. In support of this goal we release MS^2 (Multi-Document Summarization of Medical Studies) a dataset of over 470k documents and 20K summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies and is the first large-scale publicly available multi-document summarization dataset in the biomedical domain. We experiment with a summarization system based on BART with promising early results though significant work remains to achieve higher summarization quality. We formulate our summarization inputs and targets in both free text and structured forms and modify a recently proposed metric to assess the quality of our system&#39;s generated summaries. Data and models are available at https://github.com/allenai/ms2.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MS^2: Multi-Document Summarization of Medical Studies</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.594" target="_blank">https://aclanthology.org/2021.emnlp-main.594</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Text simplification is a valuable technique. However current research is limited to sentence simplification. In this paper we define and investigate a new task of document-level text simplification which aims to simplify a document consisting of multiple sentences. Based on Wikipedia dumps we first construct a large-scale dataset named D-Wikipedia and perform analysis and human evaluation on it to show that the dataset is reliable. Then we propose a new automatic evaluation metric called D-SARI that is more suitable for the document-level simplification task. Finally we select several representative models as baseline models for this task and perform automatic evaluation and human evaluation. We analyze the results and point out the shortcomings of the baseline models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Text Simplification: Dataset Criteria and Baseline</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.630" target="_blank">https://aclanthology.org/2021.emnlp-main.630</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Previous works have shown that contextual information can improve the performance of neural machine translation (NMT). However most existing document-level NMT methods failed to leverage contexts beyond a few set of previous sentences. How to make use of the whole document as global contexts is still a challenge. To address this issue we hypothesize that a document can be represented as a graph that connects relevant contexts regardless of their distances. We employ several types of relations including adjacency syntactic dependency lexical consistency and coreference to construct the document graph. Then we incorporate both source and target graphs into the conventional Transformer architecture with graph convolutional networks. Experiments on various NMT benchmarks including IWSLT English--French Chinese-English WMT English--German and Opensubtitle English--Russian demonstrate that using document graphs can significantly improve the translation quality. Extensive analysis verifies that the document graph is beneficial for capturing discourse phenomena.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Graph for Neural Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-main.663" target="_blank">https://aclanthology.org/2021.emnlp-main.663</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>For many use cases it is required that MT does not just translate raw text but complex formatted documents (e.g. websites slides spreadsheets) and the result of the translation should reflect the formatting. This is challenging as markup can be nested apply to spans contiguous in source but non-contiguous in target etc. Here we present TransIns a system for non-plain text document translation that builds on the Okapi framework and MT models trained with Marian NMT. We develop implement and evaluate different strategies for reinserting markup into translated sentences using token alignments between source and target sentences. We propose a simple and effective strategy that compiles down all markup to single source tokens and transfers them to aligned target tokens. A first evaluation shows that this strategy yields highly accurate markup in the translated documents that outperforms the markup quality found in documents translated with popular translation services. We release TransIns under the MIT License as open-source software on https://github.com/DFKI-MLT/TransIns. An online demonstrator is available at https://transins.dfki.de.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>TransIns: Document Translation with Markup Reinsertion</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-demo.4" target="_blank">https://aclanthology.org/2021.emnlp-demo.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We introduce iFᴀᴄᴇᴛSᴜᴍ a web application for exploring topical document collections. iFᴀᴄᴇᴛSᴜᴍ integrates interactive summarization together with faceted search by providing a novel faceted navigation scheme that yields abstractive summaries for the user&#39;s selections. This approach offers both a comprehensive overview as well as particular details regard-ing subtopics of choice. The facets are automatically produced based on cross-document coreference pipelines rendering generic concepts entities and statements surfacing in the source texts. We analyze the effectiveness of our application through small-scale user studies that suggest the usefulness of our tool.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>iFacetSum: Coreference-based Interactive Faceted Summarization for Multi-Document Exploration</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.emnlp-demo.33" target="_blank">https://aclanthology.org/2021.emnlp-demo.33</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>State-of-the-art (SOTA) neural machine translation (NMT) systems translate texts at sentence level ignoring context: intra-textual information like the previous sentence and extra-textual information like the gender of the speaker. As a result some sentences are translated incorrectly. Personalised NMT (PersNMT) and document-level NMT (DocNMT) incorporate this information into the translation process. Both fields are relatively new and previous work within them is limited. Moreover there are no readily available robust evaluation metrics for them which makes it difficult to develop better systems as well as track global progress and compare different methods. This thesis proposal focuses on PersNMT and DocNMT for the domain of dialogue extracted from TV subtitles in five languages: English Brazilian Portuguese German French and Polish. Three main challenges are addressed: (1) incorporating extra-textual information directly into NMT systems; (2) improving the machine translation of cohesion devices; (3) reliable evaluation for PersNMT and DocNMT.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards Personalised and Document-level Machine Translation of Dialogue</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.eacl-srw.19" target="_blank">https://aclanthology.org/2021.eacl-srw.19</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Multi-document question generation focuses on generating a question that covers the common aspect of multiple documents. Such a model is useful in generating clarifying options. However a naive model trained only using the targeted (`positive&#39;) document set may generate too generic questions that cover a larger scope than delineated by the document set. To address this challenge we introduce the contrastive learning strategy where given `positive&#39; and `negative&#39; sets of documents we generate a question that is closely related to the `positive&#39; set but is far away from the `negative&#39; set. This setting allows generated questions to be more specific and related to the target document set. To generate such specific questions we propose Multi-Source Coordinated Question Generator (MSCQG) a novel framework that includes a supervised learning (SL) stage and a reinforcement learning (RL) stage. In the SL stage a single-document question generator is trained. In the RL stage a coordinator model is trained to find optimal attention weights to align multiple single-document generators by optimizing a reward designed to promote specificity of generated questions. We also develop an effective auxiliary objective named Set-induced Contrastive Regularization (SCR) that improves the coordinator&#39;s contrastive learning during the RL stage. We show that our model significantly outperforms several strong baselines as measured by automatic metrics and human evaluation. The source repository is publicly available at `www.github.com/woonsangcho/contrast_qgen&#39;.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Contrastive Multi-document Question Generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.eacl-main.2" target="_blank">https://aclanthology.org/2021.eacl-main.2</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Cross-document co-reference resolution (CDCR) is the task of identifying and linking mentions to entities and concepts across many text documents. Current state-of-the-art models for this task assume that all documents are of the same type (e.g. news articles) or fall under the same theme. However it is also desirable to perform CDCR across different domains (type or theme). A particular use case we focus on in this paper is the resolution of entities mentioned across scientific work and newspaper articles that discuss them. Identifying the same entities and corresponding concepts in both scientific articles and news can help scientists understand how their work is represented in mainstream media. We propose a new task and English language dataset for cross-document cross-domain co-reference resolution (CD^2CR). The task aims to identify links between entities across heterogeneous document types. We show that in this cross-domain cross-document setting existing CDCR models do not perform well and we provide a baseline model that outperforms current state-of-the-art CDCR models on CD^2CR. Our data set annotation tool and guidelines as well as our model for cross-document cross-domain co-reference are all supplied as open access open source resources.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CD^2CR: Co-reference resolution across documents and domains</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.eacl-main.21" target="_blank">https://aclanthology.org/2021.eacl-main.21</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Redundancy-aware extractive summarization systems score the redundancy of the sentences to be included in a summary either jointly with their salience information or separately as an additional sentence scoring step. Previous work shows the efficacy of jointly scoring and selecting sentences with neural sequence generation models. It is however not well-understood if the gain is due to better encoding techniques or better redundancy reduction approaches. Similarly the contribution of salience versus diversity components on the created summary is not studied well. Building on the state-of-the-art encoding methods for summarization we present two adaptive learning models: AREDSUM-SEQ that jointly considers salience and novelty during sentence selection; and a two-step AREDSUM-CTX that scores salience first then learns to balance salience and redundancy enabling the measurement of the impact of each aspect. Empirical results on CNN/DailyMail and NYT50 datasets show that by modeling diversity explicitly in a separate step AREDSUM-CTX achieves significantly better performance than AREDSUM-SEQ as well as state-of-the-art extractive summarization baselines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>AREDSUM: Adaptive Redundancy-Aware Iterative Sentence Ranking for Extractive Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.eacl-main.22" target="_blank">https://aclanthology.org/2021.eacl-main.22</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We consider the situation in which a user has collected a small set of documents on a cohesive topic and they want to retrieve additional documents on this topic from a large collection. Information Retrieval (IR) solutions treat the document set as a query and look for similar documents in the collection. We propose to extend the IR approach by treating the problem as an instance of positive-unlabeled (PU) learning---i.e. learning binary classifiers from only positive (the query documents) and unlabeled (the results of the IR engine) data. Utilizing PU learning for text with big neural networks is a largely unexplored field. We discuss various challenges in applying PU learning to the setting showing that the standard implementations of state-of-the-art PU solutions fail. We propose solutions for each of the challenges and empirically validate them with ablation tests. We demonstrate the effectiveness of the new method using a series of experiments of retrieving PubMed abstracts adhering to fine-grained topics showing improvements over the common IR solution and other baselines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Scalable Evaluation and Improvement of Document Set Expansion via Neural Positive-Unlabeled Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.eacl-main.47" target="_blank">https://aclanthology.org/2021.eacl-main.47</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We revisit the classic problem of document-level role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance e.g. by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>GRIT: Generative Role-filler Transformers for Document-level Event Entity Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.eacl-main.52" target="_blank">https://aclanthology.org/2021.eacl-main.52</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper studies the problem of generatinglikely queries for multimodal documents withimages. Our application scenario is enablingefficient ``first-stage retrieval&#39;&#39; of relevant doc-uments by attaching generated queries to doc-uments before indexing. We can then indexthis expanded text to efficiently narrow downto candidate matches using inverted index sothat expensive reranking can follow. Our eval-uation results show that our proposed multi-modal representation meaningfully improvesrelevance ranking.More importantly ourframework can achieve the state of the art inthe first stage retrieval scenarios</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Query Generation for Multimodal Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.eacl-main.54" target="_blank">https://aclanthology.org/2021.eacl-main.54</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Semi-supervised learning through deep generative models and multi-lingual pretraining techniques have orchestrated tremendous success across different areas of NLP. Nonetheless their development has happened in isolation while the combination of both could potentially be effective for tackling task-specific labelled data shortage. To bridge this gap we combine semi-supervised deep generative models and multi-lingual pretraining to form a pipeline for document classification task. Compared to strong supervised learning baselines our semi-supervised classification framework is highly competitive and outperforms the state-of-the-art counterparts in low-resource settings across several languages.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Combining Deep Generative Models and Multi-lingual Pretraining for Semi-supervised Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.eacl-main.76" target="_blank">https://aclanthology.org/2021.eacl-main.76</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We propose an unsupervised graph-based ranking model for extractive summarization of long scientific documents. Our method assumes a two-level hierarchical graph representation of the source document and exploits asymmetrical positional cues to determine sentence importance. Results on the PubMed and arXiv datasets show that our approach outperforms strong unsupervised baselines by wide margins in automatic metrics and human evaluation. In addition it achieves performance comparable to many state-of-the-art supervised approaches which are trained on hundreds of thousands of examples. These results suggest that patterns in the discourse structure are a strong signal for determining importance in scientific articles.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Discourse-Aware Unsupervised Summarization for Long Scientific Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.eacl-main.93" target="_blank">https://aclanthology.org/2021.eacl-main.93</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>``Transcription bottlenecks&#39;&#39; created by a shortage of effective human transcribers (i.e. transcriber shortage) are one of the main challenges to endangered language (EL) documentation. Automatic speech recognition (ASR) has been suggested as a tool to overcome such bottlenecks. Following this suggestion we investigated the effectiveness for EL documentation of end-to-end ASR which unlike Hidden Markov Model ASR systems eschews linguistic resources but is instead more dependent on large-data settings. We open source a Yolox&#39;ochitl Mixtec EL corpus. First we review our method in building an end-to-end ASR system in a way that would be reproducible by the ASR community. We then propose a novice transcription correction task and demonstrate how ASR systems and novice transcribers can work together to improve EL documentation. We believe this combinatory methodology would mitigate the transcription bottleneck and transcriber shortage that hinders EL documentation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Leveraging End-to-End ASR for Endangered Language Documentation: An Empirical Study on Yol&#39;oxochitl Mixtec</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.eacl-main.96" target="_blank">https://aclanthology.org/2021.eacl-main.96</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We address the problem of unsupervised abstractive summarization of collections of user generated reviews through self-supervision and control. We propose a self-supervised setup that considers an individual document as a target summary for a set of similar documents. This setting makes training simpler than previous approaches by relying only on standard log-likelihood loss and mainstream models. We address the problem of hallucinations through the use of control codes to steer the generation towards more coherent and relevant summaries.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Self-Supervised and Controlled Multi-Document Opinion Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.eacl-main.141" target="_blank">https://aclanthology.org/2021.eacl-main.141</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Fine-tuning a large language model on downstream tasks has become a commonly adopted process in the Natural Language Processing (NLP) (CITATION). However such a process when associated with the current transformer-based (CITATION) architectures shows several limitations when the target task requires to reason with long documents. In this work we introduce a novel hierarchical propagation layer that spreads information between multiple transformer windows. We adopt a hierarchical approach where the input is divided in multiple blocks independently processed by the scaled dot-attentions and combined between the successive layers. We validate the effectiveness of our approach on three extractive summarization corpora of long scientific papers and news articles. We compare our approach to standard and pre-trained language-model-based summarizers and report state-of-the-art results for long document summarization and comparable results for smaller document summarization.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Globalizing BERT-based Transformer Architectures for Long Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.eacl-main.154" target="_blank">https://aclanthology.org/2021.eacl-main.154</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The great majority of languages in the world are considered under-resourced for successful application of deep learning methods. In this work we propose a meta-learning approach to document classification in low-resource languages and demonstrate its effectiveness in two different settings: few-shot cross-lingual adaptation to previously unseen languages; and multilingual joint-training when limited target-language data is available during trai-ing. We conduct a systematic comparison of several meta-learning methods investigate multiple settings in terms of data availability and show that meta-learning thrives in settings with a heterogeneous task distribution. We propose a simple yet effective adjustment to existing meta-learning methods which allows for better and more stable learning and set a new state-of-the-art on a number of languages while performing on-par on others using only a small amount of labeled data.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilingual and cross-lingual document classification: A meta-learning approach</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.eacl-main.168" target="_blank">https://aclanthology.org/2021.eacl-main.168</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper proposes a new task regarding event reason extraction from document-level texts. Unlike the previous causality detection task we do not assign target events in the text but only provide structural event descriptions and such settings accord more with practice scenarios. Moreover we annotate a large dataset FinReason for evaluation which provides Reasons annotation for Financial events in company announcements. This task is challenging because the cases of multiple-events multiple-reasons and implicit-reasons are included. In total FinReason contains 8794 documents 12861 financial events and 11006 reason spans. We also provide the performance of existing canonical methods in event extraction and machine reading comprehension on this task. The results show a 7 percentage point F1 score gap between the best model and human performance and existing methods are far from resolving this problem.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Probing into the Root: A Dataset for Reason Extraction of Structural Events from Financial Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.eacl-main.175" target="_blank">https://aclanthology.org/2021.eacl-main.175</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper explores learning rich self-supervised entity representations from large amounts of associated text. Once pre-trained these models become applicable to multiple entity-centric tasks such as ranked retrieval knowledge base completion question answering and more. Unlike other methods that harvest self-supervision signals based merely on a local context within a sentence we radically expand the notion of context to include any available text related to an entity. This enables a new class of powerful high-capacity representations that can ultimately distill much of the useful information about an entity from multiple text sources without any human supervision. We present several training strategies that unlike prior approaches learn to jointly predict words and entities -- strategies we compare experimentally on downstream tasks in the TV-Movies domain such as MovieLens tag prediction from user reviews and natural language movie search. As evidenced by results our models match or outperform competitive baselines sometimes with little or no fine-tuning and are also able to scale to very large corpora. Finally we make our datasets and pre-trained models publicly available. This includes Reviews2Movielens mapping the textasciitilde1B word corpus of Amazon movie reviews (He and McAuley 2016) to MovieLens tags (Harper and Konstan 2016) as well as Reddit Movie Suggestions with natural language queries and corresponding community recommendations.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DOCENT: Learning Self-Supervised Entity Representations from Large Document Collections</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.eacl-main.217" target="_blank">https://aclanthology.org/2021.eacl-main.217</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Abstractive summarization systems generally rely on large collections of document-summary pairs. However the performance of abstractive systems remains a challenge due to the unavailability of the parallel data for low-resource languages like Bengali. To overcome this problem we propose a graph-based unsupervised abstractive summarization system in the single-document setting for Bengali text documents which requires only a Part-Of-Speech (POS) tagger and a pre-trained language model trained on Bengali texts. We also provide a human-annotated dataset with document-summary pairs to evaluate our abstractive model and to support the comparison of future abstractive summarization systems of the Bengali Language. We conduct experiments on this dataset and compare our system with several well-established unsupervised extractive summarization systems. Our unsupervised abstractive summarization model outperforms the baselines without being exposed to any human-annotated reference summaries.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Abstractive Summarization of Bengali Text Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.eacl-main.224" target="_blank">https://aclanthology.org/2021.eacl-main.224</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Tables in web documents are pervasive and can be directly used to answer many of the queries searched on the web motivating their integration in question answering. Very often information presented in tables is succinct and hard to interpret with standard language representations. On the other hand tables often appear within textual context such as an article describing the table. Using the information from an article as additional context can potentially enrich table representations. In this work we aim to improve question answering from tables by refining table representations based on information from surrounding text. We also present an effective method to combine text and table-based predictions for question answering from full documents obtaining significant improvements on the Natural Questions dataset (Kwiatkowski et al. 2019).</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Representations for Question Answering from Documents with Tables and Text</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.eacl-main.253" target="_blank">https://aclanthology.org/2021.eacl-main.253</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We introduce a Content-based Document Alignment approach (CDA) an efficient method to align multilingual web documents based on content in creating parallel training data for machine translation (MT) systems operating at the industrial level. CDA works in two steps: (i) projecting documents of a web domain to a shared multilingual space; then (ii) aligning them based on the similarity of their representations in such space. We leverage lexical translation models to build vector representations using TFmboxtimesIDF. CDA achieves performance comparable with state-of-the-art systems in the WMT-16 Bilingual Document Alignment Shared Task benchmark while operating in multilingual space. Besides we created two web-scale datasets to examine the robustness of CDA in an industrial setting involving up to 28 languages and millions of documents. The experiments show that CDA is robust cost-effective and is significantly superior in (i) processing large and noisy web data and (ii) scaling to new and low-resourced languages.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CDA: a Cost Efficient Content-based Multilingual Web Document Aligner</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.eacl-main.266" target="_blank">https://aclanthology.org/2021.eacl-main.266</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>One strategy for facilitating reading comprehension is to present information in a question-and-answer format. We demo a system that integrates the tasks of question answering (QA) and question generation (QG) in order to produce Q&amp;A items that convey the content of multi-paragraph documents. We report some experiments for QA and QG that yield improvements on both tasks and assess how they interact to produce a list of Q&amp;A items for a text. The demo is accessible at qna.sdl.com.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>AnswerQuest: A System for Generating Question-Answer Items from Multi-Paragraph Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.eacl-demos.6" target="_blank">https://aclanthology.org/2021.eacl-demos.6</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.dialdoc-1.0" target="_blank">https://aclanthology.org/2021.dialdoc-1.0</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present the results of Shared Task at Workshop DialDoc 2021 that is focused on document-grounded dialogue and conversational question answering. The primary goal of this Shared Task is to build goal-oriented information-seeking conversation systems that can identify the most relevant knowledge in the associated document for generating agent responses in natural language. It includes two subtasks on predicting agent responses: the first subtask is to predict the grounding text span in the given document for next agent response; the second subtask is to generate agent response in natural language given the context. Many submissions outperform baseline significantly. For the first task the best-performing system achieved 67.1 Exact Match and 76.3 F1. For the second subtask the best system achieved 41.1 SacreBLEU and highest rank by human evaluation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DialDoc 2021 Shared Task: Goal-Oriented Document-grounded Dialogue Modeling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.dialdoc-1.1" target="_blank">https://aclanthology.org/2021.dialdoc-1.1</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper summarizes our entries to both subtasks of the first DialDoc shared task which focuses on the agent response prediction task in goal-oriented document-grounded dialogs. The task is split into two subtasks: predicting a span in a document that grounds an agent turn and generating an agent response based on a dialog and grounding document. In the first subtask we restrict the set of valid spans to the ones defined in the dataset use a biaffine classifier to model spans and finally use an ensemble of different models. For the second sub-task we use a cascaded model which grounds the response prediction on the predicted span instead of the full document. With these approaches we obtain significant improvements in both subtasks compared to the baseline.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cascaded Span Extraction and Response Generation for Document-Grounded Dialog</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.dialdoc-1.8" target="_blank">https://aclanthology.org/2021.dialdoc-1.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-grounded goal-oriented dialog system understands users&#39; utterances and generates proper responses by using information obtained from documents. The Dialdoc21 shared task consists of two subtasks; subtask1 finding text spans associated with users&#39; utterances from documents and subtask2 generating responses based on information obtained from subtask1. In this paper we propose two models (i.e. a knowledge span prediction model and a response generation model) for the subtask1 and the subtask2. In the subtask1 dialogue act losses are used with RoBERTa and title embeddings are added to input representation of RoBERTa. In the subtask2 various special tokens and embeddings are added to input representation of BART&#39;s encoder. Then we propose a method to assign different difficulty scores to leverage curriculum learning. In the subtask1 our span prediction model achieved F1-scores of 74.81 (ranked at top 7) and 73.41 (ranked at top 5) in test-dev phase and test phase respectively. In the subtask2 our response generation model achieved sacreBLEUs of 37.50 (ranked at top 3) and 41.06 (ranked at top 1) in in test-dev phase and test phase respectively.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Grounded Goal-Oriented Dialogue Systems on Pre-Trained Language Model with Diverse Input Representation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.dialdoc-1.12" target="_blank">https://aclanthology.org/2021.dialdoc-1.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we describe our systems for solving the two Doc2Dial shared task: knowledge identification and response generation. We proposed several pre-processing and post-processing methods and we experimented with data augmentation by pre-training the models on other relevant datasets. Our best model for knowledge identification outperformed the baseline by 10.5+ f1-score on the test-dev split and our best model for response generation outperformed the baseline by 11+ Sacrebleu score on the test-dev split.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Building Goal-oriented Document-grounded Dialogue Systems</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.dialdoc-1.14" target="_blank">https://aclanthology.org/2021.dialdoc-1.14</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we study the identity of textual events from different documents. While the complex nature of event identity is previously studied (Hovy et al. 2013) the case of events across documents is unclear. Prior work on cross-document event coreference has two main drawbacks. First they restrict the annotations to a limited set of event types. Second they insufficiently tackle the concept of event identity. Such annotation setup reduces the pool of event mentions and prevents one from considering the possibility of quasi-identity relations. We propose a dense annotation approach for cross-document event coreference comprising a rich source of event mentions and a dense annotation effort between related document pairs. To this end we design a new annotation workflow with careful quality control and an easy-to-use annotation interface. In addition to the links we further collect overlapping event contexts including time location and participants to shed some light on the relation between identity decisions and context. We present an open-access dataset for cross-document event coreference CDEC-WN collected from English Wikinews and open-source our annotation toolkit to encourage further research on cross-document tasks.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-document Event Identity via Dense Annotation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.conll-1.39" target="_blank">https://aclanthology.org/2021.conll-1.39</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The language documentation quartet</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.computel-1.2" target="_blank">https://aclanthology.org/2021.computel-1.2</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>LARA in the Service of Revivalistics and Documentary Linguistics: Community Engagement and Endangered Languages</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.computel-1.3" target="_blank">https://aclanthology.org/2021.computel-1.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Integrating Automated Segmentation and Glossing into Documentary and Descriptive Linguistics</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.computel-1.11" target="_blank">https://aclanthology.org/2021.computel-1.11</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Neural machine translation (NMT) has arguably achieved human level parity when trained and evaluated at the sentence-level. Document-level neural machine translation has received less attention and lags behind its sentence-level counterpart. The majority of the proposed document-level approaches investigate ways of conditioning the model on several source or target sentences to capture document context. These approaches require training a specialized NMT model from scratch on parallel document-level corpora. We propose an approach that doesn&#39;t require training a specialized model on parallel document-level corpora and is applied to a trained sentence-level NMT model at decoding time. We process the document from left to right multiple times and self-train the sentence-level model on pairs of source sentences and generated translations. Our approach reinforces the choices made by the model thus making it more likely that the same choices will be made in other sentences in the document. We evaluate our approach on three document-level datasets: NIST Chinese-English WMT19 Chinese-English and OpenSubtitles English-Russian. We demonstrate that our approach has higher BLEU score and higher human preference than the baseline. Qualitative analysis of our approach shows that choices made by model are consistent across the document.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Capturing document context inside sentence-level neural machine translation models with self-training</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.codi-main.14" target="_blank">https://aclanthology.org/2021.codi-main.14</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Text discourse parsing weighs importantly in understanding information flow and argumentative structure in natural language making it beneficial for downstream tasks. While previous work significantly improves the performance of RST discourse parsing they are not readily applicable to practical use cases: (1) EDU segmentation is not integrated into most existing tree parsing frameworks thus it is not straightforward to apply such models on newly-coming data. (2) Most parsers cannot be used in multilingual scenarios because they are developed only in English. (3) Parsers trained from single-domain treebanks do not generalize well on out-of-domain inputs. In this work we propose a document-level multilingual RST discourse parsing framework which conducts EDU segmentation and discourse tree parsing jointly. Moreover we propose a cross-translation augmentation strategy to enable the framework to support multilingual parsing and improve its domain generality. Experimental results show that our model achieves state-of-the-art performance on document-level multilingual RST parsing in all sub-tasks.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DMRST: A Joint Framework for Document-Level Multilingual RST Discourse Segmentation and Parsing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.codi-main.15" target="_blank">https://aclanthology.org/2021.codi-main.15</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Cross-document event coreference resolution (CDCR) is an NLP task in which mentions of events need to be identified and clustered throughout a collection of documents. CDCR aims to benefit downstream multidocument applications but despite recent progress on corpora and system development downstream improvements from applying CDCR have not been shown yet. We make the observation that every CDCR system to date was developed trained and tested only on a single respective corpus. This raises strong concerns on their generalizability---a must-have for downstream applications where the magnitude of domains or event mentions is likely to exceed those found in a curated corpus. To investigate this assumption we define a uniform evaluation setup involving three CDCR corpora: ECB+ the Gun Violence Corpus and the Football Coreference Corpus (which we reannotate on token level to make our analysis possible). We compare a corpus-independent feature-based system against a recent neural system developed for ECB+. Although being inferior in absolute numbers the feature-based system shows more consistent performance across all corpora whereas the neural system is hit-or-miss. Via model introspection we find that the importance of event actions event time and so forth for resolving coreference in practice varies greatly between the corpora. Additional analysis shows that several systems overfit on the structure of the ECB+ corpus. We conclude with recommendations on how to achieve generally applicable CDCR systems in the future---the most important being that evaluation on multiple CDCR corpora is strongly necessary. To facilitate future research we release our dataset annotation guidelines and system implementation to the public.1</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Generalizing Cross-Document Event Coreference Resolution Across Multiple Corpora</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.cl-3.18" target="_blank">https://aclanthology.org/2021.cl-3.18</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>命名实体识别是文学作品智能分析的基础性工作当前文学领域命名实体识别的研究还较薄弱一个主要的原因是缺乏标注语料。本文从金庸小说入手对两部小说180余万字进行了命名实体的标注共标注4类实体5万多个。针对小说文本的特点本文提出融入篇章信息的命名实体识别模型引入篇章字典保存汉字的历史状态利用可信度计算融合BiGRU-CRF与Transformer模型。实验结果表明利用篇章信息有效地提升了命名实体识别的效果。最后我们还探讨了命名实体识别在小说社会网络构建中的应用。</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>融入篇章信息的文学作品命名实体识别(Document-level Literary Named Entity Recognition)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.ccl-1.54" target="_blank">https://aclanthology.org/2021.ccl-1.54</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Manifold ranking has been successfully applied in query-oriented multi-document summariza-tion. It not only makes use of the relationships among the sentences but also the relationships between the given query and the sentences. However the information of original query is often insufficient. So we present a query expansion method which is combined in the manifold rank-ing to resolve this problem. Our method not only utilizes the information of the query term itselfand the knowledge base WordNet to expand it by synonyms but also uses the information of the document set itself to expand the query in various ways (mean expansion variance expansionand TextRank expansion). Compared with the previous query expansion methods our methodcombines multiple query expansion methods to better represent query information and at the same time it makes a useful attempt on manifold ranking. In addition we use the degree of wordoverlap and the proximity between words to calculate the similarity between sentences. We per-formed experiments on the datasets of DUC 2006 and DUC2007 and the evaluation results showthat the proposed query expansion method can significantly improve the system performance andmake our system comparable to the state-of-the-art systems.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using Query Expansion in Manifold Ranking for Query-Oriented Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.ccl-1.84" target="_blank">https://aclanthology.org/2021.ccl-1.84</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Most of the existing information extraction frameworks (Wadden et al. 2019; Veysehet al. 2020) focus on sentence-level tasks and are hardly able to capture the consolidated information from a given document. In our endeavour to generate precise document-level information frames from lengthy textual records we introduce the task of Information Aggregation or Argument Aggregation. More specifically our aim is to filter irrelevant and redundant argument mentions that were extracted at a sentence level and render a document level information frame. Majority of the existing works have been observed to resolve related tasks of document-level event argument extraction (Yang et al. 2018; Zheng et al. 2019) and salient entity identification (Jain et al. 2020) using supervised techniques. To remove dependency from large amounts of labelled data we explore the task of information aggregation using weakly supervised techniques. In particular we present an extractive algorithm with multiple sieves which adopts active learning strategies to work efficiently in low-resource settings. For this task we have annotated our own test dataset comprising of 131 document information frames and have released the code and dataset to further research prospects in this new domain. To the best of our knowledge we are the first to establish baseline results for this task in English. Our data and code are publicly available at https://github.com/DebanjanaKar/ArgFuse.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ArgFuse: A Weakly-Supervised Framework for Document-Level Event Argument Aggregation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.case-1.5" target="_blank">https://aclanthology.org/2021.case-1.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper accompanies our top-performing submission to the CASE 2021 shared task which is hosted at the workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text. Subtasks 1 and 2 of Task 1 concern the classification of newspaper articles and sentences into ``conflict&#39;&#39; versus ``not conflict&#39;&#39;-related in four different languages. Our model performs competitively in both subtasks (up to 0.8662 macro F1) obtaining the highest score of all contributions for subtask 1 on Hindi articles (0.7877 macro F1). We describe all experiments conducted with the XLM-RoBERTa (XLM-R) model and report results obtained in each binary classification task. We propose supplementing the original training data with additional data on political conflict events. In addition we provide an analysis of unigram probability estimates and geospatial references contained within the original training corpus.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Team ``DaDeFrNi&#39;&#39; at CASE 2021 Task 1: Document and Sentence Classification for Protest Event Detection</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.case-1.22" target="_blank">https://aclanthology.org/2021.case-1.22</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present our submission to Task 2 of the Socio-political and Crisis Events Detection Shared Task at the CASE @ ACL-IJCNLP 2021 workshop. The task at hand aims at the fine-grained classification of socio-political events. Our best model was a fine-tuned RoBERTa transformer model using document embeddings. The corpus consisted of a balanced selection of sub-events extracted from the ACLED event dataset. We achieved a macro F-score of 0.923 and a micro F-score of 0.932 during our preliminary experiments on a held-out test set. The same model also performed best on the shared task test data (weighted F-score = 0.83). To analyze the results we calculated the topic compactness of the commonly misclassified events and conducted an error analysis.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CASE 2021 Task 2 Socio-political Fine-grained Event Classification using Fine-tuned RoBERTa Document Embeddings</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.case-1.26" target="_blank">https://aclanthology.org/2021.case-1.26</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We describe a simple procedure for the automatic creation of word-level alignments between printed documents and their respective full-text versions. The procedure is unsupervised uses standard off-the-shelf components only and reaches an F-score of 85.01 in the basic setup and up to 86.63 when using pre- and post-processing. Potential areas of application are manual database curation (incl. document triage) and biomedical expression OCR.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Word-Level Alignment of Paper Documents with their Electronic Full-Text Counterparts</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.bionlp-1.19" target="_blank">https://aclanthology.org/2021.bionlp-1.19</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes a system developed to summarize multiple answers challenge in the MEDIQA 2021 shared task collocated with the BioNLP 2021 Workshop. We propose an extractive summarization architecture based on several scores and state-of-the-art techniques. We also present our novel prosper-thy-neighbour strategies to improve performance. Our model has been proven to be effective with the best ROUGE-1/ROUGE-L scores being the shared task runner up by ROUGE-2 F1 score (over 13 participated teams).</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>UETrice at MEDIQA 2021: A Prosper-thy-neighbour Extractive Multi-document Summarization Model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.bionlp-1.36" target="_blank">https://aclanthology.org/2021.bionlp-1.36</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level context can provide valuable information in grammatical error correction (GEC) which is crucial for correcting certain errors and resolving inconsistencies. In this paper we investigate context-aware approaches and propose document-level GEC systems. Additionally we employ a three-step training strategy to benefit from both sentence-level and document-level data. Our system outperforms previous document-level and all other NMT-based single-model systems achieving state of the art on a common test set.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-level grammatical error correction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.bea-1.8" target="_blank">https://aclanthology.org/2021.bea-1.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Documentation of endangered languages (ELs) has become increasingly urgent as thousands of languages are on the verge of disappearing by the end of the 21st century. One challenging aspect of documentation is to develop machine learning tools to automate the processing of EL audio via automatic speech recognition (ASR) machine translation (MT) or speech translation (ST). This paper presents an open-access speech translation corpus of Highland Puebla Nahuatl (glottocode high1278) an EL spoken in central Mexico. It then addresses machine learning contributions to endangered language documentation and argues for the importance of speech translation as a key element in the documentation process. In our experiments we observed that state-of-the-art end-to-end ST models could outperform a cascaded ST (ASR textgreater MT) pipeline when translating endangered language documentation materials.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Highland Puebla Nahuatl Speech Translation Corpus for Endangered Language Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.americasnlp-1.7" target="_blank">https://aclanthology.org/2021.americasnlp-1.7</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes three open access Yolox&#39;ochitl Mixtec corpora and presents the results and implications of end-to-end automatic speech recognition for endangered language documentation. Two issues are addressed. First the advantage for ASR accuracy of targeting informational (BPE) units in addition to or in substitution of linguistic units (word morpheme morae) and then using ROVER for system combination. BPE units consistently outperform linguistic units although the best results are obtained by system combination of different BPE targets. Second a case is made that for endangered language documentation ASR contributions should be evaluated according to extrinsic criteria (e.g. positive impact on downstream tasks) and not simply intrinsic metrics (e.g. CER and WER). The extrinsic metric chosen is the level of reduction in the human effort needed to produce high-quality transcriptions for permanent archiving.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>End-to-End Automatic Speech Recognition: Its Impact on the Workflowin Documenting Yolox&#39;ochitl Mixtec</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.americasnlp-1.8" target="_blank">https://aclanthology.org/2021.americasnlp-1.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Generating long and coherent text is an important and challenging task encompassing many application areas such as summarization document level machine translation and story generation. Despite the success in modeling intra-sentence coherence existing long text generation models (e.g. BART and GPT-3) still struggle to maintain a coherent event sequence throughout the generated text. We conjecture that this is because of the difficulty for the model to revise replace revoke or delete any part that has been generated by the model. In this paper we present a novel semi-autoregressive document generation model capable of revising and editing the generated text. Building on recent models by (Gu et al. 2019; Xu and Carpuat 2020) we propose document generation as a hierarchical Markov decision process with a two level hierarchy where the high and low level editing programs. We train our model using imitation learning (Hussein et al. 2017) and introduce roll-in policy such that each policy learns on the output of applying the previous action. Experiments applying the proposed approach sheds various insights on the problems of long text generation using our model. We suggest various remedies such as using distilled dataset designing better attention mechanisms and using autoregressive models as a low level program.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Level Hierarchical Transformer</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.alta-1.13" target="_blank">https://aclanthology.org/2021.alta-1.13</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Hierarchical document categorisation is a special case of multi-label document categorisation where there is a taxonomic hierarchy among the labels. While various approaches have been proposed for hierarchical document categorisation there is no standard benchmark dataset resulting in different methods being evaluated independently and there being no empirical consensus on what methods perform best. In this work we examine different combinations of neural text encoders and hierarchical methods in an end-to-end framework and evaluate over three datasets. We find that the performance of hierarchical document categorisation is determined not only by how the hierarchical information is modelled but also the structure of the label hierarchy and class distribution.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Evaluating Hierarchical Document Categorisation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.alta-1.20" target="_blank">https://aclanthology.org/2021.alta-1.20</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Achieving satisfying performance in machine translation on domains for which there is no training data is challenging. Traditional supervised domain adaptation is not suitable for addressing such zero-resource domains because it relies on in-domain parallel data. We show that when in-domain parallel data is not available access to document-level context enables better capturing of domain generalities compared to only having access to a single sentence. Having access to more information provides a more reliable domain estimation. We present two document-level Transformer models which are capable of using large context sizes and we compare these models against strong Transformer baselines. We obtain improvements for the two zero-resource domains we study. We additionally provide an analysis where we vary the amount of context and look at the case where in-domain data is available.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.adaptnlp-1.9" target="_blank">https://aclanthology.org/2021.adaptnlp-1.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Abstractive summarization is the task of compressing a long document into a coherent short document while retaining salient information. Modern abstractive summarization methods are based on deep neural networks which often require large training datasets. Since collecting summarization datasets is an expensive and time-consuming task practical industrial settings are usually low-resource. In this paper we study a challenging low-resource setting of summarizing long legal briefs with an average source document length of 4268 words and only 120 available (document summary) pairs. To account for data scarcity we used a modern pre-trained abstractive summarizer BART which only achieves 17.9 ROUGE-L as it struggles with long documents. We thus attempt to compress these long documents by identifying salient sentences in the source which best ground the summary using a novel algorithm based on GPT-2 language model perplexity scores that operates within the low resource regime. On feeding the compressed documents to BART we observe a 6.0 ROUGE-L improvement. Our method also beats several competitive salience detection baselines. Furthermore the identified salient sentences tend to agree with independent human labeling by domain experts.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Long Document Summarization in a Low Resource Setting using Pretrained Language Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-srw.7" target="_blank">https://aclanthology.org/2021.acl-srw.7</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Constructing knowledge graphs from unstructured text is an important task that is relevant to many domains. Most previous work focuses on extracting information from sentences or paragraphs due to the difficulty of analyzing longer contexts. In this paper we propose a new jointly trained model that can be used for various information extraction tasks at the document level. The tasks performed by this system are entity and event identification typing and coreference resolution. In order to improve entity and event typing we utilize context-aware representations aggregated from the detected mentions of the corresponding entities and events across the entire document. By extending our system to document-level we can improve our results by incorporating cross-sentence dependencies and additional contextual information that might not be available at the sentence level which allows for more globally optimized predictions. We evaluate our system on documents from the ACE05-E+ dataset and find significant improvement over the sentence-level SOTA on entity and event trigger identification and classification.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Joint Detection and Coreference Resolution of Entities and Events with Document-level Context Aggregation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-srw.18" target="_blank">https://aclanthology.org/2021.acl-srw.18</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Natural reading orders of words are crucial for information extraction from form-like documents. Despite recent advances in Graph Convolutional Networks (GCNs) on modeling spatial layout patterns of documents they have limited ability to capture reading orders of given word-level node representations in a graph. We propose Reading Order Equivariant Positional Encoding (ROPE) a new positional encoding technique designed to apprehend the sequential presentation of words in documents. ROPE generates unique reading order codes for neighboring words relative to the target word given a word-level graph connectivity. We study two fundamental document entity extraction tasks including word labeling and word grouping on the public FUNSD dataset and a large-scale payment dataset. We show that ROPE consistently improves existing GCNs with a margin up to 8.4% F1-score.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ROPE: Reading Order Equivariant Positional Encoding for Graph-based Document Information Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-short.41" target="_blank">https://aclanthology.org/2021.acl-short.41</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present TIMERS - a TIME Rhetorical and Syntactic-aware model for document-level temporal relation classification in the English language. Our proposed method leverages rhetorical discourse features and temporal arguments from semantic role labels in addition to traditional local syntactic features trained through a Gated Relational-GCN. Extensive experiments show that the proposed model outperforms previous methods by 5-18% on the TDDiscourse TimeBank-Dense and MATRES datasets due to our discourse-level modeling.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>TIMERS: Document-level Temporal Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-short.67" target="_blank">https://aclanthology.org/2021.acl-short.67</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Topic models extract groups of words from documents whose interpretation as a topic hopefully allows for a better understanding of the data. However the resulting word groups are often not coherent making them harder to interpret. Recently neural topic models have shown improvements in overall coherence. Concurrently contextual embeddings have advanced the state of the art of neural models in general. In this paper we combine contextualized representations with neural topic models. We find that our approach produces more meaningful and coherent topics than traditional bag-of-words topic models and recent neural models. Our results indicate that future improvements in language models will translate into better topic models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-short.96" target="_blank">https://aclanthology.org/2021.acl-short.96</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Transformer is important for text modeling. However it has difficulty in handling long documents due to the quadratic complexity with input text length. In order to handle this problem we propose a hierarchical interactive Transformer (Hi-Transformer) for efficient and effective long document modeling. Hi-Transformer models documents in a hierarchical way i.e. first learns sentence representations and then learns document representations. It can effectively reduce the complexity and meanwhile capture global document context in the modeling of each sentence. More specifically we first use a sentence Transformer to learn the representations of each sentence. Then we use a document Transformer to model the global document context from these sentence representations. Next we use another sentence Transformer to enhance sentence modeling using the global document context. Finally we use hierarchical pooling method to obtain document embedding. Extensive experiments on three benchmark datasets validate the efficiency and effectiveness of Hi-Transformer in long document modeling.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-short.107" target="_blank">https://aclanthology.org/2021.acl-short.107</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level Relation Extraction (RE) is a more challenging task than sentence RE as it often requires reasoning over multiple sentences. Yet human annotators usually use a small number of sentences to identify the relationship between a given entity pair. In this paper we present an embarrassingly simple but effective method to heuristically select evidence sentences for document-level RE which can be easily combined with BiLSTM to achieve good performance on benchmark datasets even better than fancy graph neural network based methods. We have released our code at https://github.com/AndrewZhe/Three-Sentences-Are-All-You-Need.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Three Sentences Are All You Need: Local Path Enhanced Document Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-short.126" target="_blank">https://aclanthology.org/2021.acl-short.126</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Faceted summarization provides briefings of a document from different perspectives. Readers can quickly comprehend the main points of a long document with the help of a structured outline. However little research has been conducted on this subject partially due to the lack of large-scale faceted summarization datasets. In this study we present FacetSum a faceted summarization benchmark built on Emerald journal articles covering a diverse range of domains. Different from traditional document-summary pairs FacetSum provides multiple summaries each targeted at specific sections of a long document including the purpose method findings and value. Analyses and empirical results on our dataset reveal the importance of bringing structure into summaries. We believe FacetSum will spur further advances in summarization research and foster the development of NLP systems that can leverage the structured information in both long texts and summaries.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-short.137" target="_blank">https://aclanthology.org/2021.acl-short.137</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We propose a new architecture for adapting a sentence-level sequence-to-sequence transformer by incorporating multiple pre-trained document context signals and assess the impact on translation performance of (1) different pretraining approaches for generating these signals (2) the quantity of parallel data for which document context is available and (3) conditioning on source target or source and target contexts. Experiments on the NIST Chinese-English and IWSLT and WMT English-German tasks support four general conclusions: that using pre-trained context representations markedly improves sample efficiency that adequate parallel data resources are crucial for learning to use document context that jointly conditioning on multiple context representations outperforms any single representation and that source context is more valuable for translation performance than target side context. Our best multi-context model consistently outperforms the best existing context-aware transformers.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Diverse Pretrained Context Encodings Improve Document Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-long.104" target="_blank">https://aclanthology.org/2021.acl-long.104</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We address the task of explaining relationships between two scientific documents using natural language text. This task requires modeling the complex content of long technical documents deducing a relationship between these documents and expressing the details of that relationship in text. In addition to the theoretical interest of this task successful solutions can help improve researcher efficiency in search and review. In this paper we establish a dataset of 622K examples from 154K documents. We pretrain a large language model to serve as the foundation for autoregressive approaches to the task. We explore the impact of taking different views on the two documents including the use of dense representations extracted with scientific IE systems. We provide extensive automatic and human evaluations which show the promise of such models but make clear challenges for future work.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Explaining Relationships Between Scientific Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-long.166" target="_blank">https://aclanthology.org/2021.acl-long.166</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>With the need of fast retrieval speed and small memory footprint document hashing has been playing a crucial role in large-scale information retrieval. To generate high-quality hashing code both semantics and neighborhood information are crucial. However most existing methods leverage only one of them or simply combine them via some intuitive criteria lacking a theoretical principle to guide the integration process. In this paper we encode the neighborhood information with a graph-induced Gaussian distribution and propose to integrate the two types of information with a graph-driven generative model. To deal with the complicated correlations among documents we further propose a tree-structured approximation method for learning. Under the approximation we prove that the training objective can be decomposed into terms involving only singleton or pairwise documents enabling the model to be trained as efficiently as uncorrelated ones. Extensive experimental results on three benchmark datasets show that our method achieves superior performance over state-of-the-art methods demonstrating the effectiveness of the proposed model for simultaneously preserving semantic and neighborhood information.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Integrating Semantics and Neighborhood Information with Graph-Driven Generative Models for Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-long.174" target="_blank">https://aclanthology.org/2021.acl-long.174</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text layout and image in a single multi-modal framework. Specifically with a two-stream multi-modal Transformer encoder LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks including FUNSD (0.7895 to 0.8420) CORD (0.9493 to 0.9601) SROIE (0.9524 to 0.9781) Kleister-NDA (0.8340 to 0.8520) RVL-CDIP (0.9443 to 0.9564) and DocVQA (0.7295 to 0.8672).</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-long.201" target="_blank">https://aclanthology.org/2021.acl-long.201</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Transformers are not suited for processing long documents due to their quadratically increasing memory and time consumption. Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. In this paper we propose ERNIE-Doc a document-level language pretraining model based on Recurrence Transformers. Two well-designed techniques namely the retrospective feed mechanism and the enhanced recurrence mechanism enable ERNIE-Doc which has a much longer effective context length to capture the contextual information of a complete document. We pretrain ERNIE-Doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective. Various experiments were conducted on both English and Chinese document-level tasks. ERNIE-Doc improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText-103. Moreover it outperformed competitive pretraining models by a large margin on most language understanding tasks such as text classification and question answering.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ERNIE-Doc: A Retrospective Long-Document Modeling Transformer</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-long.227" target="_blank">https://aclanthology.org/2021.acl-long.227</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level MT models are still far from satisfactory. Existing work extend translation unit from single sentence to multiple sentences. However study shows that when we further enlarge the translation unit to a whole document supervised training of Transformer can fail. In this paper we find such failure is not caused by overfitting but by sticking around local minima during training. Our analysis shows that the increased complexity of target-to-source attention is a reason for the failure. As a solution we propose G-Transformer introducing locality assumption as an inductive bias into Transformer reducing the hypothesis space of the attention from target to source. Experiments show that G-Transformer converges faster and more stably than Transformer achieving new state-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>G-Transformer for Document-Level Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-long.267" target="_blank">https://aclanthology.org/2021.acl-long.267</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level event extraction aims to recognize event information from a whole piece of article. Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. In this paper we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges. For the first challenge GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions. For the second GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events. Experiments on a large-scale dataset (Zheng et al 2019) show GIT outperforms the previous methods by 2.8 F1. Further analysis reveals is effective in extracting multiple correlated events and event arguments that scatter across the document.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-long.274" target="_blank">https://aclanthology.org/2021.acl-long.274</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Question answering (QA) systems for large document collections typically use pipelines that (i) retrieve possibly relevant documents (ii) re-rank them (iii) rank paragraphs or other snippets of the top-ranked documents and (iv) select spans of the top-ranked snippets as exact answers. Pipelines are conceptually simple but errors propagate from one component to the next without later components being able to revise earlier decisions. We present an architecture for joint document and snippet ranking the two middle stages which leverages the intuition that relevant documents have good snippets and good snippets come from relevant documents. The architecture is general and can be used with any neural text relevance ranker. We experiment with two main instantiations of the architecture based on POSIT-DRMM (PDRMM) and a BERT-based ranker. Experiments on biomedical data from BIOASQ show that our joint models vastly outperform the pipelines in snippet retrieval the main goal for QA with fewer trainable parameters also remaining competitive in document retrieval. Furthermore our joint PDRMM-based model is competitive with BERT-based models despite using orders of magnitude fewer parameters. These claims are also supported by human evaluation on two test batches of BIOASQ. To test our key findings on another dataset we modified the Natural Questions dataset so that it can also be used for document and snippet retrieval. Our joint PDRMM-based model again outperforms the corresponding pipeline in snippet retrieval on the modified Natural Questions dataset even though it performs worse than the pipeline in document retrieval. We make our code and the modified Natural Questions dataset publicly available.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Neural Model for Joint Document and Snippet Ranking in Question Answering for Large Document Collections</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-long.301" target="_blank">https://aclanthology.org/2021.acl-long.301</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>An automated system that could assist a judge in predicting the outcome of a case would help expedite the judicial process. For such a system to be practically useful predictions by the system should be explainable. To promote research in developing such a system we introduce ILDC (Indian Legal Documents Corpus). ILDC is a large corpus of 35k Indian Supreme Court cases annotated with original court decisions. A portion of the corpus (a separate test set) is annotated with gold standard explanations by legal experts. Based on ILDC we propose the task of Court Judgment Prediction and Explanation (CJPE). The task requires an automated system to predict an explainable outcome of a case. We experiment with a battery of baseline models for case predictions and propose a hierarchical occlusion based model for explainability. Our best prediction model has an accuracy of 78% versus 94% for human legal experts pointing towards the complexity of the prediction task. The analysis of explanations by the proposed algorithm reveals a significant difference in the point of view of the algorithm and legal experts for explaining the judgments pointing towards scope for future research.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ILDC for CJPE: Indian Legal Documents Corpus for Court Judgment Prediction and Explanation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-long.313" target="_blank">https://aclanthology.org/2021.acl-long.313</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Streaming cross document entity coreference (CDC) systems disambiguate mentions of named entities in a scalable manner via incremental clustering. Unlike other approaches for named entity disambiguation (e.g. entity linking) streaming CDC allows for the disambiguation of entities that are unknown at inference time. Thus it is well-suited for processing streams of data where new entities are frequently introduced. Despite these benefits this task is currently difficult to study as existing approaches are either evaluated on datasets that are no longer available or omit other crucial details needed to ensure fair comparison. In this work we address this issue by compiling a large benchmark adapted from existing free datasets and performing a comprehensive evaluation of a number of novel and existing baseline models. We investigate: how to best encode mentions which clustering algorithms are most effective for grouping mentions how models transfer to different domains and how bounding the number of mentions tracked during inference impacts performance. Our results show that the relative performance of neural and feature-based mention encoders varies across different domains and in most cases the best performance is achieved using a combination of both approaches. We also find that performance is minimally impacted by limiting the number of tracked mentions.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Benchmarking Scalable Methods for Streaming Cross Document Entity Coreference</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-long.364" target="_blank">https://aclanthology.org/2021.acl-long.364</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We study the problem of event coreference resolution (ECR) that seeks to group coreferent event mentions into the same clusters. Deep learning methods have recently been applied for this task to deliver state-of-the-art performance. However existing deep learning models for ECR are limited in that they cannot exploit important interactions between relevant objects for ECR e.g. context words and entity mentions to support the encoding of document-level context. In addition consistency constraints between golden and predicted clusters of event mentions have not been considered to improve representation learning in prior deep learning models for ECR. This work addresses such limitations by introducing a novel deep learning model for ECR. At the core of our model are document structures to explicitly capture relevant objects for ECR. Our document structures introduce diverse knowledge sources (discourse syntax semantics) to compute edges/interactions between structure nodes for document-level representation learning. We also present novel regularization techniques based on consistencies of golden and predicted clusters for event mentions in documents. Extensive experiments show that our model achieve state-of-the-art performance on two benchmark datasets.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploiting Document Structures and Cluster Consistencies for Event Coreference Resolution</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-long.374" target="_blank">https://aclanthology.org/2021.acl-long.374</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recently the retrieval models based on dense representations have been gradually applied in the first stage of the document retrieval tasks showing better performance than traditional sparse vector space models. To obtain high efficiency the basic structure of these models is Bi-encoder in most cases. However this simple structure may cause serious information loss during the encoding of documents since the queries are agnostic. To address this problem we design a method to mimic the queries to each of the documents by an iterative clustering process and represent the documents by multiple pseudo queries (i.e. the cluster centroids). To boost the retrieval process using approximate nearest neighbor search library we also optimize the matching function with a two-step score calculation procedure. Experimental results on several popular ranking and QA datasets show that our model can achieve state-of-the-art results while still remaining high efficiency.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-long.392" target="_blank">https://aclanthology.org/2021.acl-long.392</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2021</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level event extraction (DEE) is indispensable when events are described throughout a document. We argue that sentence-level extractors are ill-suited to the DEE task where event arguments always scatter across sentences and multiple events may co-exist in a document. It is a challenging task because it requires a holistic understanding of the document and an aggregated ability to assemble arguments across multiple sentences. In this paper we propose an end-to-end model which can extract structured events from a document in a parallel manner. Specifically we first introduce a document-level encoder to obtain the document-aware representations. Then a multi-granularity non-autoregressive decoder is used to generate events in parallel. Finally to train the entire model a matching loss function is proposed which can bootstrap a global optimization. The empirical results on the widely used DEE dataset show that our approach significantly outperforms current state-of-the-art methods in the challenging DEE task. Code will be available at https://github.com/HangYang-NLP/DE-PPN.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-level Event Extraction via Parallel Prediction Networks</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2021.acl-long.492" target="_blank">https://aclanthology.org/2021.acl-long.492</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The relatedness of research articles patents court rulings web pages and other document types is often calculated with citation or hyperlink-based approaches like co-citation (proximity) analysis. The main limitation of citation-based approaches is that they cannot be used for documents that receive little or no citations. We propose Virtual Citation Proximity (VCP) a Siamese Neural Network architecture which combines the advantages of co-citation proximity analysis (diverse notions of relatedness / high recommendation performance) with the advantage of content-based filtering (high coverage). VCP is trained on a corpus of documents with textual features and with real citation proximity as ground truth. VCP then predicts for any two documents based on their title and abstract in what proximity the two documents would be co-cited if they were indeed co-cited. The prediction can be used in the same way as real citation proximity to calculate document relatedness even for uncited documents. In our evaluation with 2 million co-citations from Wikipedia articles VCP achieves an MAE of 0.0055 i.e. an improvement of 20% over the baseline though the learning curve suggests that more work is needed.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Virtual Citation Proximity (VCP): Empowering Document Recommender Systems by Learning a Hypothetical In-Text Citation-Proximity Metric for Uncited Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.wosp-1.1" target="_blank">https://aclanthology.org/2020.wosp-1.1</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Given the global scale of COVID-19 and the flood of social media content related to it how can we find informative discussions? We present Gapformer which effectively classifies content as informative or not. It reformulates the problem as graph classification drawing on not only the tweet but connected webpages and entities. We leverage a pre-trained language model as well as the connections between nodes to learn a pooled representation for each document network. We show it outperforms several competitive baselines and present ablation studies supporting the benefit of the linked information. Code is available on Github.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ComplexDataLab at W-NUT 2020 Task 2: Detecting Informative COVID-19 Tweets by Attending over Linked Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.wnut-1.63" target="_blank">https://aclanthology.org/2020.wnut-1.63</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We describe our two NMT systems submitted to the WMT 2020 shared task in Englishtextless-textgreaterCzech and Englishtextless-textgreaterPolish news translation. One system is sentence level translating each sentence independently. The second system is document level translating multiple sentences trained on multi-sentence sequences up to 3000 characters long.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CUNI English-Czech and English-Polish Systems in WMT20: Robust Document-Level Training</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.wmt-1.28" target="_blank">https://aclanthology.org/2020.wmt-1.28</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes the DeepMind submission to the ChineserightarrowEnglish constrained data track of the WMT2020 Shared Task on News Translation. The submission employs a noisy channel factorization as the backbone of a document translation system. This approach allows the flexible combination of a number of independent component models which are further augmented with back-translation distillation fine-tuning with in-domain data Monte-Carlo Tree Search decoding and improved uncertainty estimation. In order to address persistent issues with the premature truncation of long sequences we included specialized length models and sentence segmentation techniques. Our final system provides a 9.9 BLEU points improvement over a baseline Transformer on our test set (newstest 2019).</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The DeepMind Chinese--English Document Translation System at WMT2020</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.wmt-1.36" target="_blank">https://aclanthology.org/2020.wmt-1.36</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Even though sentence-centric metrics are used widely in machine translation evaluation document-level performance is at least equally important for professional usage. In this paper we bring attention to detailed document-level evaluation focused on markables (expressions bearing most of the document meaning) and the negative impact of various markable error phenomena on the translation. For an annotation experiment of two phases we chose Czech and English documents translated by systems submitted to WMT20 News Translation Task. These documents are from the News Audit and Lease domains. We show that the quality and also the kind of errors varies significantly among the domains. This systematic variance is in contrast to the automatic evaluation results. We inspect which specific markables are problematic for MT systems and conclude with an analysis of the effect of markable error types on the MT performance measured by humans and automatic evaluation tools.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>WMT20 Document-Level Markable Error Exploration</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.wmt-1.41" target="_blank">https://aclanthology.org/2020.wmt-1.41</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes our system submission to WMT20 shared task on similar language translation. We examined the use of documentlevel neural machine translation (NMT) systems for low-resource similar language pair Marathi−Hindi. Our system is an extension of state-of-the-art Transformer architecture with hierarchical attention networks to incorporate contextual information. Since NMT requires large amount of parallel data which is not available for this task our approach is focused on utilizing monolingual data with back translation to train our models. Our experiments reveal that document-level NMT can be a reasonable alternative to sentence-level NMT for improving translation quality of low resourced languages even when used with synthetic data.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Level NMT of Low-Resource Languages with Backtranslation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.wmt-1.53" target="_blank">https://aclanthology.org/2020.wmt-1.53</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Sentence-level (SL) machine translation (MT) has reached acceptable quality for many high-resourced languages but not document-level (DL) MT which is difficult to 1) train with little amount of DL data; and 2) evaluate as the main methods and data sets focus on SL evaluation. To address the first issue we present a document-aligned Japanese-English conversation corpus including balanced high-quality business conversation data for tuning and testing. As for the second issue we manually identify the main areas where SL MT fails to produce adequate translations in lack of context. We then create an evaluation set where these phenomena are annotated to alleviate automatic evaluation of DL systems. We train MT models using our corpus to demonstrate how using context leads to improvements.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-aligned Japanese-English Conversation Parallel Corpus</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.wmt-1.74" target="_blank">https://aclanthology.org/2020.wmt-1.74</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The 2020 WMT Biomedical translation task evaluated Medline abstract translations. This is a small-domain translation task meaning limited relevant training data with very distinct style and vocabulary. Models trained on such data are susceptible to exposure bias effects particularly when training sentence pairs are imperfect translations of each other. This can result in poor behaviour during inference if the model learns to neglect the source sentence. The UNICAM entry addresses this problem during fine-tuning using a robust variant on Minimum Risk Training. We contrast this approach with data-filtering to remove `problem&#39; training examples. Under MRT fine-tuning we obtain good results for both directions of English-German and English-Spanish biomedical translation. In particular we achieve the best English-to-Spanish translation result and second-best Spanish-to-English result despite using only single models with no ensembling.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Addressing Exposure Bias With Document Minimum Risk Training: Cambridge at the WMT20 Biomedical Translation Task</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.wmt-1.94" target="_blank">https://aclanthology.org/2020.wmt-1.94</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>On the Same Page? Comparing Inter-Annotator Agreement in Sentence and Document Level Human Machine Translation Evaluation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.wmt-1.137" target="_blank">https://aclanthology.org/2020.wmt-1.137</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The Web has become a source of information where information is provided by humans for humans and its growth has increased necessity to get solutions that intelligently extract valuable knowledge from existing and newly added web documents with no (minimal) supervisions. However due to the unstructured nature of existing data on the Web effective extraction of this knowledge is limited for both human beings and software agents. Thus this research work designed generic and embedding oriented framework that automatically annotates semantically Amharic web documents using ontology. This framework significantly reduces manual annotation and learning cost used for semantic annotation of Amharic web documents with its nature of adaptability with minimal modification. The results have also implied that neural network techniques are promising for semantic annotation especially for less resourced languages like Amharic in comparison to language dependent techniques that have cost of speed and challenge of adaptation into new domains and languages. We experiment the feasibility of the proposed approach using Amharic news collected from WALTA news agency and Amharic Wikipedia. Our results show that the proposed solution exhibits 70.68% of precision 66.89% of recall and 68.53% of f-measure in semantic annotation for a morphologically complex Amharic language with limited size dataset.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Embedding Oriented Adaptable Semantic Annotation Framework for Amharic Web Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.winlp-1.3" target="_blank">https://aclanthology.org/2020.winlp-1.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we describe the ADAPT Centre&#39;s submissions to the WAT 2020 document-level Business Scene Dialogue (BSD) Translation task. We only consider translating from Japanese to English for this task and we use the MarianNMT toolkit to train Transformer models. In order to improve the translation quality we made use of both in-domain and out-of-domain data for training our Machine Translation (MT) systems as well as various data augmentation techniques for fine-tuning the model parameters. This paper outlines the experiments we ran to train our systems and report the accuracy achieved through these various experiments.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The ADAPT Centre&#39;s Neural MT Systems for the WAT 2020 Document-Level Translation Task</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.wat-1.17" target="_blank">https://aclanthology.org/2020.wat-1.17</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper accompanies the software documentation data set for machine translation a parallel evaluation data set of data originating from the SAP Help Portal that we released to the machine translation community for research purposes. It offers the possibility to tune and evaluate machine translation systems in the domain of corporate software documentation and contributes to the availability of a wider range of evaluation scenarios. The data set comprises of the language pairs English to Hindi Indonesian Malay and Thai and thus also increases the test coverage for the many low-resource language pairs. Unlike most evaluation data sets that consist of plain parallel text the segments in this data set come with additional metadata that describes structural information of the document context. We provide insights into the origin and creation the particularities and characteristics of the data set as well as machine translation results.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Parallel Evaluation Data Set of Software Documentation with Document Structure Annotation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.wat-1.20" target="_blank">https://aclanthology.org/2020.wat-1.20</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Named entity recognition (NER) from visual documents such as invoices receipts or business cards is a critical task for visual document understanding. Most classical approaches use a sequence-based model (typically BiLSTM-CRF framework) without considering document structure. Recent work on graph-based model using graph convolutional networks to encode visual and textual features have achieved promising performance on the task. However few attempts take geometry information of text segments (text in bounding box) in visual documents into account. Meanwhile existing methods do not consider that related text segments which need to be merged to form a complete entity in many real-world situations. In this paper we present GraphNEMR a graph-based model that uses graph convolutional networks to jointly merge text segments and recognize named entities. By incorporating geometry information from visual documents into our model richer 2D context information is generated to improve document representations. To merge text segments we introduce a novel mechanism that captures both geometry information as well as semantic information based on pre-trained language model. Experimental results show that the proposed GraphNEMR model outperforms both sequence-based and graph-based SOTA methods significantly.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Merge and Recognize: A Geometry and 2D Context Aware Graph Model for Named Entity Recognition from Visual Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.textgraphs-1.3" target="_blank">https://aclanthology.org/2020.textgraphs-1.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We show that Bayes&#39; rule provides an effective mechanism for creating document translation models that can be learned from only parallel sentences and monolingual documents a compelling benefit because parallel documents are not always available. In our formulation the posterior probability of a candidate translation is the product of the unconditional (prior) probability of the candidate output document and the ``reverse translation probability&#39;&#39; of translating the candidate output back into the source language. Our proposed model uses a powerful autoregressive language model as the prior on target language documents but it assumes that each sentence is translated independently from the target to the source language. Crucially at test time when a source document is observed the document language model prior induces dependencies between the translations of the source sentences in the posterior. The model&#39;s independence assumption not only enables efficient use of available data but it additionally admits a practical left-to-right beam-search algorithm for carrying out inference. Experiments show that our model benefits from using cross-sentence context in the language model and it outperforms existing document translation approaches.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Better Document-Level Machine Translation with Bayes&#39; Rule</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.tacl-1.23" target="_blank">https://aclanthology.org/2020.tacl-1.23</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We address the problem of unsupervised extractive document summarization especially for long documents. We model the unsupervised problem as a sparse auto-regression one and approximate the resulting combinatorial problem via a convex norm-constrained problem. We solve it using a dedicated Frank-Wolfe algorithm. To generate a summary with k sentences the algorithm only needs to execute approximately k iterations making it very efficient for a long document. We evaluate our approach against two other unsupervised methods using both lexical (standard) ROUGE scores as well as semantic (embedding-based) ones. Our method achieves better results with both datasets and works especially well when combined with embeddings for highly paraphrased summaries.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Sparse Optimization for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.sustainlp-1.8" target="_blank">https://aclanthology.org/2020.sustainlp-1.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Pre-trained language models such as BERT have shown their effectiveness in various tasks. Despite their power they are known to be computationally intensive which hinders real-world applications. In this paper we introduce early exiting BERT for document ranking. With a slight modification BERT becomes a model with multiple output paths and each inference sample can exit early from these paths. In this way computation can be effectively allocated among samples and overall system latency is significantly reduced while the original quality is maintained. Our experiments on two document ranking datasets demonstrate up to 2.5x inference speedup with minimal quality degradation. The source code of our implementation can be found at https://github.com/castorini/earlyexiting-monobert.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Early Exiting BERT for Efficient Document Ranking</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.sustainlp-1.11" target="_blank">https://aclanthology.org/2020.sustainlp-1.11</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The predominant approaches for extracting key information from documents resort to classifiers predicting the information type of each word. However the word level ground truth used for learning is expensive to obtain since it is not naturally produced by the extraction task. In this paper we discuss a new method for training extraction models directly from the textual value of information. The extracted information of a document is represented as a sequence of tokens in the XML language. We learn to output this representation with a pointer-generator network that alternately copies the document words carrying information and generates the XML tags delimiting the types of information. The ability of our end-to-end method to retrieve structured information is assessed on a large set of business documents. We show that it performs competitively with a standard word classifier without requiring costly word level supervision.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>End-to-End Extraction of Structured Information from Business Documents with Pointer-Generator Networks</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.spnlp-1.6" target="_blank">https://aclanthology.org/2020.spnlp-1.6</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Although BERT is widely used by the NLP community little is known about its inner workings. Several attempts have been made to shed light on certain aspects of BERT often with contradicting conclusions. A much raised concern focuses on BERT&#39;s over-parameterization and under-utilization issues. To this end we propose o novel approach to fine-tune BERT in a structured manner. Specifically we focus on Large Scale Multilabel Text Classification (LMTC) where documents are assigned with one or more labels from a large predefined set of hierarchically organized labels. Our approach guides specific BERT layers to predict labels from specific hierarchy levels. Experimenting with two LMTC datasets we show that this structured fine-tuning approach not only yields better classification results but also leads to better parameter utilization.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Layer-wise Guided Training for BERT: Learning Incrementally Refined Document Representations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.spnlp-1.7" target="_blank">https://aclanthology.org/2020.spnlp-1.7</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We seek to maximally use various data sources such as parallel and monolingual data to build an effective and efficient document-level translation system. In particular we start by considering a noisy channel approach (CITATION) that combines a target-to-source translation model and a language model. By applying Bayes&#39; rule strategically we reformulate this approach as a log-linear combination of translation sentence-level and document-level language model probabilities. In addition to using static coefficients for each term this formulation alternatively allows for the learning of dynamic per-token weights to more finely control the impact of the language models. Using both static or dynamic coefficients leads to improvements over a context-agnostic baseline and a context-aware concatenation model.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Log-Linear Reformulation of the Noisy Channel Model for Document-Level Neural Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.spnlp-1.11" target="_blank">https://aclanthology.org/2020.spnlp-1.11</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>For endangered languages data collection campaigns have to accommodate the challenge that many of them are from oral tradition and producing transcriptions is costly. Therefore it is fundamental to translate them into a widely spoken language to ensure interpretability of the recordings. In this paper we investigate how the choice of translation language affects the posterior documentation work and potential automatic approaches which will work on top of the produced bilingual corpus. For answering this question we use the MaSS multilingual speech corpus (Boito et al. 2020) for creating 56 bilingual pairs that we apply to the task of low-resource unsupervised word segmentation and alignment. Our results highlight that the choice of language for translation influences the word segmentation performance and that different lexicons are learned by using different aligned translations. Lastly this paper proposes a hybrid approach for bilingual word segmentation combining boundary clues extracted from a non-parametric Bayesian model (Goldwater et al. 2009a) with the attentional word segmentation neural model from Godard et al. (2018). Our results suggest that incorporating these clues into the neural models&#39; input representation increases their translation and alignment quality specially for challenging language pairs.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Investigating Language Impact in Bilingual Approaches for Computational Language Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.sltu-1.11" target="_blank">https://aclanthology.org/2020.sltu-1.11</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Despite recent advances in natural language processing and other language technology the application of such technology to language documentation and conservation has been limited. In August 2019 a workshop was held at Carnegie Mellon University in Pittsburgh PA USA to attempt to bring together language community members documentary linguists and technologists to discuss how to bridge this gap and create prototypes of novel and practical language revitalization technologies. The workshop focused on developing technologies to aid language documentation and revitalization in four areas: 1) spoken language (speech transcription phone to orthography decoding text-to-speech and text-speech forced alignment) 2) dictionary extraction and management 3) search tools for corpora and 4) social media (language learning bots and social media analysis). This paper reports the results of this workshop including issues discussed and various conceived and implemented technologies for nine languages: Arapaho Cayuga Inuktitut Irish Gaelic Kidaw&#39;ida Kwak&#39;wala Ojibwe San Juan Quiahije Chatino and Seneca.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Summary of the First Workshop on Language Technology for Language Documentation and Revitalization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.sltu-1.48" target="_blank">https://aclanthology.org/2020.sltu-1.48</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper outlines our approach to Tasks A &amp; B for the English Language track of SemEval-2020 Task 12: OffensEval 2: Multilingual Offensive Language Identification in Social Media. We use a Linear SVM with document vectors computed from pre-trained word embeddings and we explore the effectiveness of lexical part of speech dependency and named entity (NE) features. We manually annotate a subset of the training data which we use for error analysis and to tune a threshold for mapping training confidence values to labels. While document vectors are consistently the most informative features for both tasks testing on the development set suggests that dependency features are an effective addition for Task A and NE features for Task B.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>UNT Linguistics at SemEval-2020 Task 12: Linear SVC with Pre-trained Word Embeddings as Document Vectors and Targeted Linguistic Features</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.semeval-1.294" target="_blank">https://aclanthology.org/2020.semeval-1.294</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Proceedings of the First Workshop on Scholarly Document Processing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.sdp-1.0" target="_blank">https://aclanthology.org/2020.sdp-1.0</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Next to keeping up with the growing literature in their own and related fields scholars increasingly also need to rebut pseudo-science and disinformation. To address these challenges computational work on enhancing search summarization and analysis of scholarly documents has flourished. However the various strands of research on scholarly document processing remain fragmented. To reach to the broader NLP and AI/ML community pool distributed efforts and enable shared access to published research we held the 1st Workshop on Scholarly Document Processing at EMNLP 2020 as a virtual event. The SDP workshop consisted of a research track (including a poster session) two invited talks and three Shared Tasks (CL-SciSumm Lay-Summ and LongSumm) geared towards easier access to scientific methods and results. textbfWebsite: urlhttps://ornlcda.github.io/SDProc</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Overview of the First Workshop on Scholarly Document Processing (SDP)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.sdp-1.1" target="_blank">https://aclanthology.org/2020.sdp-1.1</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We introduce a novel scientific document processing task for making previously inaccessible information in printed paper documents available to automatic processing. We describe our data set of scanned documents and data records from the biological database SABIO-RK provide a definition of the task and report findings from preliminary experiments. Rigorous evaluation proved challenging due to lack of gold-standard data and a difficult notion of correctness. Qualitative inspection of results however showed the feasibility and usefulness of the task</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Reconstructing Manual Information Extraction with DB-to-Document Backprojection: Experiments in the Life Science Domain</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.sdp-1.9" target="_blank">https://aclanthology.org/2020.sdp-1.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We introduce SciWING an open-source soft-ware toolkit which provides access to state-of-the-art pre-trained models for scientific document processing (SDP) tasks such as citation string parsing logical structure recovery and citation intent classification. Compared to other toolkits SciWING follows a full neural pipeline and provides a Python inter-face for SDP. When needed SciWING provides fine-grained control for rapid experimentation with different models by swapping and stacking different modules. Transfer learning from general and scientific documents specific pre-trained transformers (i.e. BERT SciBERT etc.) can be performed. SciWING incorporates ready-to-use web and terminal-based applications and demonstrations to aid adoption and development. The toolkit is available from http://sciwing.io and the demos are available at http://rebrand.ly/sciwing-demo.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SciWING-- A Software Toolkit for Scientific Document Processing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.sdp-1.13" target="_blank">https://aclanthology.org/2020.sdp-1.13</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Predicting the number of citations of scholarly documents is an upcoming task in scholarly document processing. Besides the intrinsic merit of this information it also has a wider use as an imperfect proxy for quality which has the advantage of being cheaply available for large volumes of scholarly documents. Previous work has dealt with number of citations prediction with relatively small training data sets or larger datasets but with short incomplete input text. In this work we leverage the open access ACL Anthology collection in combination with the Semantic Scholar bibliometric database to create a large corpus of scholarly documents with associated citation information and we propose a new citation prediction model called SChuBERT. In our experiments we compare SChuBERT with several state-of-the-art citation prediction models and show that it outperforms previous methods by a large margin. We also show the merit of using more training data and longer input for number of citations prediction.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SChuBERT: Scholarly Document Chunks with BERT-encoding boost Citation Count Prediction.</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.sdp-1.17" target="_blank">https://aclanthology.org/2020.sdp-1.17</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Training recurrent neural networks on long texts in particular scholarly documents causes problems for learning. While hierarchical attention networks (HANs) are effective in solving these problems they still lose important information about the structure of the text. To tackle these problems we propose the use of HANs combined with structure-tags which mark the role of sentences in the document. Adding tags to sentences marking them as corresponding to title abstract or main body text yields improvements over the state-of-the-art for scholarly document quality prediction. The proposed system is applied to the task of accept/reject prediction on the PeerRead dataset and compared against a recent BiLSTM-based model and joint textual+visual model as well as against plain HANs. Compared to plain HANs accuracy increases on all three domains.On the computation and language domain our new model works best overall and increases accuracy 4.7% over the best literature result. We also obtain improvements when introducing the tags for prediction of the number of citations for 88k scientific publications that we compiled from the Allen AI S2ORC dataset. For our HAN-system with structure-tags we reach 28.5% explained variance an improvement of 1.8% over our reimplementation of the BiLSTM-based model as well as 1.0% improvement over plain HANs.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Structure-Tags Improve Text Classification for Scholarly Document Quality Prediction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.sdp-1.18" target="_blank">https://aclanthology.org/2020.sdp-1.18</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The task of definition detection is important for scholarly papers because papers often make use of technical terminology that may be unfamiliar to readers. Despite prior work on definition detection current approaches are far from being accurate enough to use in realworld applications. In this paper we first perform in-depth error analysis of the current best performing definition detection system and discover major causes of errors. Based on this analysis we develop a new definition detection system HEDDEx that utilizes syntactic features transformer encoders and heuristic filters and evaluate it on a standard sentence-level benchmark. Because current benchmarks evaluate randomly sampled sentences we propose an alternative evaluation that assesses every sentence within a document. This allows for evaluating recall in addition to precision. HEDDEx outperforms the leading system on both the sentence-level and the document-level tasks by 12.7 F1 points and 14.4 F1 points respectively. We note that performance on the high-recall document-level task is much lower than in the standard evaluation approach due to the necessity of incorporation of document structure as features. We discuss remaining challenges in document-level definition detection ideas for improvements and potential issues for the development of reading aid applications.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Definition Detection in Scholarly Documents: Existing Models Error Analyses and Future Directions</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.sdp-1.22" target="_blank">https://aclanthology.org/2020.sdp-1.22</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present the results of three Shared Tasks held at the Scholarly Document Processing Workshop at EMNLP2020: CL-SciSumm LaySumm and LongSumm. We report on each of the tasks which received 18 submissions in total with some submissions addressing two or three of the tasks. In summary the quality and quantity of the submissions show that there is ample interest in scholarly document summarization and the state of the art in this domain is at a midway point between being an impossible task and one that is fully resolved.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Overview and Insights from the Shared Tasks at Scholarly Document Processing 2020: CL-SciSumm LaySumm and LongSumm</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.sdp-1.24" target="_blank">https://aclanthology.org/2020.sdp-1.24</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Our system participates in two shared tasks CL-SciSumm 2020 and LongSumm 2020. In the CL-SciSumm shared task based on our previous work we apply more machine learning methods on position features and content features for facet classification in Task1B. And GCN is introduced in Task2 to perform extractive summarization. In the LongSumm shared task we integrate both the extractive and abstractive summarization ways. Three methods were tested which are T5 Fine-tuning DPPs Sampling and GRU-GCN/GAT.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CIST@CL-SciSumm 2020 LongSumm 2020: Automatic Scientific Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.sdp-1.25" target="_blank">https://aclanthology.org/2020.sdp-1.25</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents our methods for the LongSumm 2020: Shared Task on Generating Long Summaries for Scientific Documents where the task is to generatelong summaries given a set of scientific papers provided by the organizers. We explore 3 main approaches for this task: 1. An extractive approach using a BERT-based summarization model; 2. A two stage model that additionally includes an abstraction step using BART; and 3. A new multi-tasking approach on incorporating document structure into the summarizer. We found that our new multi-tasking approach outperforms the two other methods by large margins. Among 9 participants in the shared task our best model ranks top according to Rouge-1 score (53.11%) while staying competitive in terms of Rouge-2.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>GUIR @ LongSumm 2020: Learning to Generate Long Summaries from Scientific Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.sdp-1.41" target="_blank">https://aclanthology.org/2020.sdp-1.41</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Analysis of Multimodal Document Intent in Instagram Posts</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.rocling-1.20" target="_blank">https://aclanthology.org/2020.rocling-1.20</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Fine-tuned variants of BERT are able to achieve state-of-the-art accuracy on many natural language processing tasks although at significant computational costs. In this paper we verify BERT&#39;s effectiveness for document classification and investigate the extent to which BERT-level effectiveness can be obtained by different baselines combined with knowledge distillation---a popular model compression method. The results show that BERT-level effectiveness can be achieved by a single-layer LSTM with at least 40times fewer FLOPS and only sim3% parameters. More importantly this study analyzes the limits of knowledge distillation as we distill BERT&#39;s knowledge all the way down to linear models---a relevant baseline for the task. We report substantial improvement in effectiveness for even the simplest models as they capture the knowledge learnt by BERT.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploring the Limits of Simple Learners in Knowledge Distillation for Document Classification with DocBERT</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.repl4nlp-1.10" target="_blank">https://aclanthology.org/2020.repl4nlp-1.10</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Cross-Document Event Coreference (CDEC) is the task of finding coreference relationships between events in separate documents most commonly assessed using the Event Coreference Bank+ corpus (ECB+). At least two different approaches have been proposed for CDEC on ECB+ that use only event triggers and at least four have been proposed that use both triggers and entities. Comparing these approaches is complicated by variation in the systems&#39; use of gold vs. computed labels as well as variation in the document clustering pre-processing step. We present an approach that matches or slightly beats state-of-the-art performance on CDEC over ECB+ with only event trigger annotations but with a significantly simpler framework and much smaller feature set relative to prior work. This study allows us to directly compare with prior systems and draw conclusions about the effectiveness of various strategies. Additionally we provide the first cross-validated evaluation on the ECB+ dataset; the first explicit evaluation of the pairwise event coreference classification step; and the first quantification of the effect of document clustering on system performance. The last in particular reveals that while document clustering is a crucial pre-processing step improvements can at most provide for a 3 point improvement in CDEC performance though this might be attributable to ease of document clustering on ECB+.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>New Insights into Cross-Document Event Coreference: Systematic Comparison and a Simplified Approach</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.nuse-1.1" target="_blank">https://aclanthology.org/2020.nuse-1.1</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The global pandemic has made it more important than ever to quickly and accurately retrieve relevant scientific literature for effective consumption by researchers in a wide range of fields. We provide an analysis of several multi-label document classification models on the LitCovid dataset. We find that pre-trained language models outperform other models in both low and high data regimes achieving a maximum F1 score of around 86%. We note that even the highest performing models still struggle with label correlation distraction from introductory text and CORD-19 generalization. Both data and code are available on GitHub.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Classification for COVID-19 Literature</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.nlpcovid19-acl.3" target="_blank">https://aclanthology.org/2020.nlpcovid19-acl.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we aim to develop a self-supervised grounding of Covid-related medical text based on the actual spatial relationships between the referred anatomical concepts. More specifically we learn to project sentences into a physical space defined by a three-dimensional anatomical atlas allowing for a visual approach to navigating Covid-related literature. We design a straightforward and empirically effective training objective to reduce the curated data dependency issue. We use BERT as the main building block of our model and perform a quantitative analysis that demonstrates that the model learns a context-aware mapping. We illustrate two potential use-cases for our approach one in interactive 3D data exploration and the other in document retrieval. To accelerate research in this direction we make public all trained models codebase and the developed tools which can be accessed at https://github.com/gorjanradevski/macchina/.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Self-supervised context-aware COVID-19 document exploration through atlas grounding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.nlpcovid19-acl.5" target="_blank">https://aclanthology.org/2020.nlpcovid19-acl.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Efficient discovery and exploration of biomedical literature has grown in importance in the context of the COVID-19 pandemic and topic-based methods such as latent Dirichlet allocation (LDA) are a useful tool for this purpose. In this study we compare traditional topic models based on word tokens with topic models based on medical concepts and propose several ways to improve topic coherence and specificity.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improved Topic Representations of Medical Documents to Assist COVID-19 Literature Exploration</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.nlpcovid19-2.12" target="_blank">https://aclanthology.org/2020.nlpcovid19-2.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present CAiRE-COVID a real-time question answering (QA) and multi-document summarization system which won one of the 10 tasks in the Kaggle COVID-19 Open Research Dataset Challenge judged by medical experts. Our system aims to tackle the recent challenge of mining the numerous scientific articles being published on COVID-19 by answering high priority questions from the community and summarizing salient question-related information. It combines information extraction with state-of-the-art QA and query-focused multi-document summarization techniques selecting and highlighting evidence snippets from existing literature given a query. We also propose query-focused abstractive and extractive multi-document summarization methods to provide more relevant information related to the question. We further conduct quantitative experiments that show consistent improvements on various metrics for each module. We have launched our website CAiRE-COVID for broader use by the medical community and have open-sourced the code for our system to bootstrap further study by other researches.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CAiRE-COVID: A Question Answering and Query-focused Multi-Document Summarization System for COVID-19 Scholarly Information Management</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.nlpcovid19-2.14" target="_blank">https://aclanthology.org/2020.nlpcovid19-2.14</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recent studies have shown that translation quality of NMT systems can be improved by providing document-level contextual information. In general sentence-based NMT models are extended to capture contextual information from large-scale document-level corpora which are difficult to acquire. Domain adaptation on the other hand promises adapting components of already developed systems by exploiting limited in-domain data. This paper presents FJWU&#39;s system submission at WNGT we specifically participated in Document level MT task for German-English translation. Our system is based on context-aware Transformer model developed on top of original NMT architecture by integrating contextual information using attention networks. Our experimental results show providing previous sentences as context significantly improves the BLEU score as compared to a strong NMT baseline. We also studied the impact of domain adaptation on document level translationand were able to improve results by adaptingthe systems according to the testing domain.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Document-Level Neural Machine Translation with Domain Adaptation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.ngt-1.27" target="_blank">https://aclanthology.org/2020.ngt-1.27</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Automatic dating of ancient documents is a very important area of research for digital humanities applications. Many documents available via digital libraries do not have any dating or dating that is uncertain. Document dating is not only useful by itself but it also helps to choose the appropriate NLP tools (lemmatizer POS tagger ) for subsequent analysis. This paper provides a dataset with thousands of ancient documents in French and present methods and evaluation metrics for this task. We compare character-level methods with token-level methods on two different datasets of two different time periods and two different text genres. Our results show that character-level models are more robust to noise than classical token-level models. The experiments presented in this article focused on documents written in French but we believe that the ability of character-level models to handle noise properly would help to achieve comparable results on other languages and more ancient languages in particular.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Dating Ancient texts: an Approach for Noisy French Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lt4hala-1.3" target="_blank">https://aclanthology.org/2020.lt4hala-1.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Optical character recognition (OCR) for historical documents is a complex procedure subject to a unique set of material issues including inconsistencies in typefaces and low quality scanning. Consequently even the most sophisticated OCR engines produce errors. This paper reports on a tool built for postediting the output of Tesseract more specifically for correcting common errors in digitized historical documents. The proposed tool suggests alternatives for word forms not found in a specified vocabulary. The assumed error is replaced by a presumably correct alternative in the post-edition based on the scores of a Language Model (LM). The tool is tested on a chapter of the book An Essay Towards Regulating the Trade and Employing the Poor of this Kingdom (Cary 1719). As demonstrated below the tool is successful in correcting a number of common errors. If sometimes unreliable it is also transparent and subject to human intervention.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Tool for Facilitating OCR Postediting in Historical Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lt4hala-1.7" target="_blank">https://aclanthology.org/2020.lt4hala-1.7</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents the BDCam~oes Collection of Portuguese Literary Documents a new corpus of literary texts written in Portuguese that in its inaugural version includes close to 4 million words from over 200 complete documents from 83 authors in 14 genres covering a time span from the 16th to the 21st century and adhering to different orthographic conventions. Many of the texts in the corpus have also been automatically parsed with state-of-the-art language processing tools forming the BDCam~oes Treebank subcorpus. This set of characteristics makes of BDCam~oes an invaluable resource for research in language technology (e.g. authorship detection genre classification etc.) and in language science and digital humanities (e.g. comparative literature diachronic linguistics etc.).</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The BDCam~oes Collection of Portuguese Literary Documents: a Research Resource for Digital Humanities and Language Technology</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.106" target="_blank">https://aclanthology.org/2020.lrec-1.106</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Deduplication is the task of identifying near and exact duplicate data items in a collection. In this paper we present a novel method for deduplication of scholarly documents. We develop a hybrid model which uses structural similarity (locality sensitive hashing) and meaning representation (word embeddings) of document texts to determine (near) duplicates. Our collection constitutes a subset of multidisciplinary scholarly documents aggregated from research repositories. We identify several issues causing data inaccuracies in such collections and motivate the need for deduplication. In lack of existing dataset suitable for study of deduplication of scholarly documents we create a ground truth dataset of 100K scholarly documents and conduct a series of experiments to empirically establish optimal values for the parameters of our deduplication method. Experimental evaluation shows that our method achieves a macro F1-score of 0.90. We productionise our method as a publicly accessible web API service serving deduplication of scholarly documents in real time.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Deduplication of Scholarly Documents using Locality Sensitive Hashing and Word Embeddings</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.113" target="_blank">https://aclanthology.org/2020.lrec-1.113</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this work we present a corpus for the evaluation of sensitive information detection approaches that addresses the need for real world sensitive information for empirical studies. Our sentence corpus contains different notions of complex sensitive information that correspond to different aspects of concern in a current trial of the Monsanto company. This paper describes the annotations process where we both employ human annotators and furthermore create automatically inferred labels regarding technical legal and informal communication within and with employees of Monsanto drawing on a classification of documents by lawyers involved in the Monsanto court case. We release corpus of high quality sentences and parse trees with these two types of labels on sentence level. We characterize the sensitive information via several representative sensitive information detection models in particular both keyword-based (n-gram) approaches and recent deep learning models namely recurrent neural networks (LSTM) and recursive neural networks (RecNN). Data and code are made publicly available.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Real-World Data Resource of Complex Sensitive Sentences Based on Documents from the Monsanto Trial</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.158" target="_blank">https://aclanthology.org/2020.lrec-1.158</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes VICTOR a novel dataset built from Brazil&#39;s Supreme Court digitalized legal documents composed of more than 45 thousand appeals which includes roughly 692 thousand documents---about 4.6 million pages. The dataset contains labeled text data and supports two types of tasks: document type classification; and theme assignment a multilabel problem. We present baseline results using bag-of-words models convolutional neural networks recurrent neural networks and boosting algorithms. We also experiment using linear-chain Conditional Random Fields to leverage the sequential nature of the lawsuits which we find to lead to improvements on document type classification. Finally we compare a theme classification approach where we use domain knowledge to filter out the less informative document pages to the default one where we use all pages. Contrary to the Court experts&#39; expectations we find that using all available data is the better method. We make the dataset available in three versions of different sizes and contents to encourage explorations of better models and techniques.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>VICTOR: a Dataset for Brazilian Legal Documents Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.181" target="_blank">https://aclanthology.org/2020.lrec-1.181</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Despite the excellent performance of black box approaches to modeling sentiment and emotion lexica (sets of informative words and associated weights) that characterize different emotions are indispensable to the NLP community because they allow for interpretable and robust predictions. Emotion analysis of text is increasing in popularity in NLP; however manually creating lexica for psychological constructs such as empathy has proven difficult. This paper automatically creates empathy word ratings from document-level ratings. The underlying problem of learning word ratings from higher-level supervision has to date only been addressed in an ad hoc fashion and has not used deep learning methods. We systematically compare a number of approaches to learning word ratings from higher-level supervision against a Mixed-Level Feed Forward Network (MLFFN) which we find performs best and use the MLFFN to create the first-ever empathy lexicon. We then use Signed Spectral Clustering to gain insights into the resulting words. The empathy and distress lexica are publicly available at: http://www.wwbp.org/lexica.html.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learning Word Ratings for Empathy and Distress from Document-Level User Responses</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.206" target="_blank">https://aclanthology.org/2020.lrec-1.206</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Natural speech data on many languages have been collected by language documentation projects aiming to preserve lingustic and cultural traditions in audivisual records. These data hold great potential for large-scale cross-linguistic research into phonetics and language processing. Major obstacles to utilizing such data for typological studies include the non-homogenous nature of file formats and annotation conventions found both across and within archived collections. Moreover time-aligned audio transcriptions are typically only available at the level of broad (multi-word) phrases but not at the word and segment levels. We report on solutions developed for these issues within the DoReCo (DOcumentation REference COrpus) project. DoReCo aims at providing time-aligned transcriptions for at least 50 collections of under-resourced languages. This paper gives a preliminary overview of the current state of the project and details our workflow in particular standardization of formats and conventions the addition of segmental alignments with WebMAUS and DoReCo&#39;s applicability for subsequent research programs. By making the data accessible to the scientific community DoReCo is designed to bridge the gap between language documentation and linguistic inquiry.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Building a Time-Aligned Cross-Linguistic Reference Corpus from Language Documentation Data (DoReCo)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.324" target="_blank">https://aclanthology.org/2020.lrec-1.324</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>For most of the world&#39;s languages no primary data are available even as many languages are disappearing. Throughout the last two decades however language documentation projects have produced substantial amounts of primary data from a wide variety of endangered languages. These resources are still in the early days of their exploration. One of the factors that makes them hard to use is a relative lack of standardized annotation conventions. In this paper we will describe common practices in existing corpora in order to facilitate their future processing. After a brief introduction of the main formats used for annotation files we will focus on commonly used tiers in the widespread ELAN and Toolbox formats. Minimally corpora from language documentation contain a transcription tier and an aligned translation tier which means they constitute parallel corpora. Additional common annotations include named references morpheme separation morpheme-by-morpheme glosses part-of-speech tags and notes.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Empirical Evaluation of Annotation Practices in Corpora from Language Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.338" target="_blank">https://aclanthology.org/2020.lrec-1.338</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Since at least half of the world&#39;s 6000 plus languages will vanish during the 21st century language documentation has become a rapidly growing field in linguistics. A fundamental challenge for language documentation is the &#39;&#39;transcription bottleneck&#39;&#39;. Speech technology may deliver the decisive breakthrough for overcoming the transcription bottleneck. This paper presents first experiments from the development of ASR4LD a new automatic speech recognition (ASR) based tool for language documentation (LD). The experiments are based on recordings from an ongoing documentation project for the endangered Muyu language in New Guinea. We compare phoneme recognition experiments with American English Austrian German and Slovenian as source language and Muyu as target language. The Slovenian acoustic models achieve the by far best performance (43.71% PER) in comparison to 57.14% PER with American English and 89.49% PER with Austrian German. Whereas part of the errors can be explained by phonetic variation the recording mismatch poses a major problem. On the long term ASR4LD will not only be an integral part of the ongoing documentation project of Muyu but will be further developed in order to facilitate also the language documentation process of other language groups.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards Building an Automatic Transcription System for Language Documentation: Experiences from Muyu</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.353" target="_blank">https://aclanthology.org/2020.lrec-1.353</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper reports on challenges and solution approaches in the development of methods for language resource overarching data analysis in the field of language documentation. It is based on the successful outcomes of the initial phase of an 18 year long-term project on lesser resourced and mostly endangered indigenous languages of the Northern Eurasian area which included the finalization and publication of multiple language corpora and additional language resources. While aiming at comprehensive cross-resource data analysis the project at the same time is confronted with a dynamic and complex resource landscape especially resulting from a vast amount of multi-layered information stored in the form of analogue primary data in different widespread archives on the territory of the Russian Federation. The methods described aim at solving the tension between unification of data sets and vocabularies on the one hand and maximum openness for the integration of future resources and adaption of external information on the other hand.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards Flexible Cross-Resource Exploitation of Heterogeneous Language Documentation Data</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.354" target="_blank">https://aclanthology.org/2020.lrec-1.354</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The major European language infrastructure initiatives like CLARIN (Hinrichs and Krauwer 2014) DARIAH (Edmond et al. 2017) or Europeana (Europeana Foundation 2015) have been built by focusing in the first place on institutions of larger scale like specialized research departments and larger official units like national libraries etc. However besides these principal players also a large number of smaller language actors could contribute to and benefit from language infrastructures. Especially since these smaller institutions like local libraries archives and publishers often collect manage and host language resources of particular value for their geographical and cultural region it seems highly relevant to find ways of engaging and connecting them to existing European infrastructure initiatives. In this article we first highlight the need for reaching out to smaller local language actors and discuss challenges related to this ambition. Then we present the first step in how this objective was approached within a local language infrastructure project namely by means of a structured documentation of the local language actors landscape in South Tyrol. We describe how the documentation efforts were structured and organized and what tool we have set up to distribute the collected data online by adapting existing CLARIN solutions.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Digital Language Infrastructures -- Documenting Language Actors</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.424" target="_blank">https://aclanthology.org/2020.lrec-1.424</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Current approaches to machine translation (MT) either translate sentences in isolation disregarding the context they appear in or model context at the level of the full document without a notion of any internal structure the document may have. In this work we consider the fact that documents are rarely homogeneous blocks of text but rather consist of parts covering different topics. Some documents such as biographies and encyclopedia entries have highly predictable regular structures in which sections are characterised by different topics. We draw inspiration from Louis and Webber (2014) who use this information to improve statistical MT and transfer their proposal into the framework of neural MT. We compare two different methods of including information about the topic of the section within which each sentence is found: one using side constraints and the other using a cache-based model. We create and release the data on which we run our experiments - parallel corpora for three language pairs (Chinese-English French-English Bulgarian-English) from Wikipedia biographies which we extract automatically preserving the boundaries of sections within the articles.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Sub-structure in Neural Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.451" target="_blank">https://aclanthology.org/2020.lrec-1.451</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we describe the details of the Timely Disclosure Documents Corpus (TDDC). TDDC was prepared by manually aligning the sentences from past Japanese and English timely disclosure documents in PDF format published by companies listed on the Tokyo Stock Exchange. TDDC consists of approximately 1.4 million parallel sentences in Japanese and English. TDDC was used as the official dataset for the 6th Workshop on Asian Translation to encourage the development of machine translation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>TDDC: Timely Disclosure Documents Corpus</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.459" target="_blank">https://aclanthology.org/2020.lrec-1.459</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Instead of translating sentences in isolation document-level machine translation aims to capture discourse dependencies across sentences by considering a document as a whole. In recent years there have been more interests in modelling larger context for the state-of-the-art neural machine translation (NMT). Although various document-level NMT models have shown significant improvements there nonetheless exist three main problems: 1) compared with sentence-level translation tasks the data for training robust document-level models are relatively low-resourced; 2) experiments in previous work are conducted on their own datasets which vary in size domain and language; 3) proposed approaches are implemented on distinct NMT architectures such as recurrent neural networks (RNNs) and self-attention networks (SANs). In this paper we aims to alleviate the low-resource and under-universality problems for document-level NMT. First we collect a large number of existing document-level corpora which covers 7 language pairs and 6 domains. In order to address resource sparsity we construct a novel document parallel corpus in Chinese-Portuguese which is a non-English-centred and low-resourced language pair. Besides we implement and evaluate the commonly-cited document-level method on top of the advanced Transformer model with universal settings. Finally we not only demonstrate the effectiveness and universality of document-level NMT but also release the preprocessed data source code and trained models for comparison and reproducibility.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Corpora for Document-Level Neural Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.466" target="_blank">https://aclanthology.org/2020.lrec-1.466</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Several studies (covering many language pairs and translation tasks) have demonstrated that translation quality has improved enormously since the emergence of neural machine translation systems. This raises the question whether such systems are able to produce high-quality translations for more creative text types such as literature and whether they are able to generate coherent translations on document level. Our study aimed to investigate these two questions by carrying out a document-level evaluation of the raw NMT output of an entire novel. We translated Agatha Christie&#39;s novel The Mysterious Affair at Styles with Google&#39;s NMT system from English into Dutch and annotated it in two steps: first all fluency errors then all accuracy errors. We report on the overall quality determine the remaining issues compare the most frequent error types to those in general-domain MT and investigate whether any accuracy and fluency errors co-occur regularly. Additionally we assess the inter-annotator agreement on the first chapter of the novel.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Literary Machine Translation under the Magnifying Glass: Assessing the Quality of an NMT-Translated Detective Novel on Document Level</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.468" target="_blank">https://aclanthology.org/2020.lrec-1.468</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Conventional multimodal tasks such as caption generation and visual question answering have allowed machines to understand an image by describing or being asked about it in natural language often via a sentence. Datasets for these tasks contain a large number of pairs of an image and the corresponding sentence as an instance. However a real multimodal document such as a news article or Wikipedia page consists of multiple sentences with multiple images. Such documents require an advanced skill of jointly considering the multiple texts and multiple images beyond a single sentence and image for the interpretation. Therefore aiming at building a system that can understand multimodal documents we propose a task called image position prediction (IPP). In this task a system learns plausible positions of images in a given document. To study this task we automatically constructed a dataset of 66K multimodal documents with 320K images from Wikipedia articles. We conducted a preliminary experiment to evaluate the performance of a current multimodal system on our task. The experimental results show that the system outperformed simple baselines while the performance is still far from human performance which thus poses new challenges in multimodal research.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Image Position Prediction in Multimodal Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.526" target="_blank">https://aclanthology.org/2020.lrec-1.526</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We describe a dataset developed for Named Entity Recognition in German federal court decisions. It consists of approx. 67000 sentences with over 2 million tokens. The resource contains 54000 manually annotated entities mapped to 19 fine-grained semantic classes: person judge lawyer country city street landscape organization company institution court brand law ordinance European legal norm regulation contract court decision and legal literature. The legal documents were furthermore automatically annotated with more than 35000 TimeML-based time expressions. The dataset which is available under a CC-BY 4.0 license in the CoNNL-2002 format was developed for training an NER service for German legal documents in the EU project Lynx.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Dataset of German Legal Documents for Named Entity Recognition</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.551" target="_blank">https://aclanthology.org/2020.lrec-1.551</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Current sentence boundary detectors split documents into sequentially ordered sentences by detecting their beginnings and ends. Sentences however are more deeply structured even on this side of constituent and dependency structure: they can consist of a main sentence and several subordinate clauses as well as further segments (e.g. inserts in parentheses); they can even recursively embed whole sentences and then contain multiple sentence beginnings and ends. In this paper we introduce a tool that segments sentences into tree structures to detect this type of recursive structure. To this end we retrain different constituency parsers with the help of modified training data to transform them into sentence segmenters. With these segmenters documents are mapped to sequences of sentence-related ``logical document structures&#39;&#39;. The resulting segmenters aim to improve downstream tasks by providing additional structural information. In this context we experiment with German dependency parsing. We show that for certain sentence categories which can be determined automatically improvements in German dependency parsing can be achieved using our segmenter for preprocessing. The assumption suggests that improvements in other languages and tasks can be achieved.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Recognizing Sentence-level Logical Document Structures with the Help of Context-free Grammars</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.650" target="_blank">https://aclanthology.org/2020.lrec-1.650</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Abstractive summarization typically relies on large collections of paired articles and summaries. However in many cases parallel data is scarce and costly to obtain. We develop an abstractive summarization system that relies only on large collections of example summaries and non-matching articles. Our approach consists of an unsupervised sentence extractor that selects salient sentences to include in the final summary as well as a sentence abstractor that is trained on pseudo-parallel and synthetic data that paraphrases each of the extracted sentences. We perform an extensive evaluation of our method: on the CNN/DailyMail benchmark on which we compare our approach to fully supervised baselines as well as on the novel task of automatically generating a press release from a scientific journal article which is well suited for our system. We show promising performance on both tasks without relying on any article-summary pairs.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Abstractive Document Summarization without Parallel Data</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.819" target="_blank">https://aclanthology.org/2020.lrec-1.819</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Today&#39;s research progress in the field of multi-document summarization is obstructed by the small number of available datasets. Since the acquisition of reference summaries is costly existing datasets contain only hundreds of samples at most resulting in heavy reliance on hand-crafted features or necessitating additional manually annotated data. The lack of large corpora therefore hinders the development of sophisticated models. Additionally most publicly available multi-document summarization corpora are in the news domain and no analogous dataset exists in the video game domain. In this paper we propose GameWikiSum a new domain-specific dataset for multi-document summarization which is one hundred times larger than commonly used datasets and in another domain than news. Input documents consist of long professional video game reviews as well as references of their gameplay sections in Wikipedia pages. We analyze the proposed dataset and show that both abstractive and extractive models can be trained on it. We release GameWikiSum for further research: https://github.com/Diego999/GameWikiSum.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>GameWikiSum: a Novel Large Multi-Document Summarization Dataset</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.820" target="_blank">https://aclanthology.org/2020.lrec-1.820</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Related work sections or literature reviews are an essential part of every scientific article being crucial for paper reviewing and assessment. The automatic generation of related work sections can be considered an instance of the multi-document summarization problem. In order to allow the study of this specific problem we have developed a manually annotated machine readable data-set of related work sections cited papers (e.g. references) and sentences together with an additional layer of papers citing the references. We additionally present experiments on the identification of cited sentences using as input citation contexts. The corpus alongside the gold standard are made available for use by the scientific community.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Multi-level Annotated Corpus of Scientific Papers for Scientific Document Summarization and Cross-document Relation Discovery</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.824" target="_blank">https://aclanthology.org/2020.lrec-1.824</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The paper presents the Bulgarian MARCELL corpus part of a recently developed multilingual corpus representing the national legislation in seven European countries and the NLP pipeline that turns the web crawled data into structured linguistically annotated dataset. The Bulgarian data is web crawled extracted from the original HTML format filtered by document type tokenised sentence split tagged and lemmatised with a fine-grained version of the Bulgarian Language Processing Chain dependency parsed with NLP- Cube annotated with named entities (persons locations organisations and others) noun phrases IATE terms and EuroVoc descriptors. An orchestrator process has been developed to control the NLP pipeline performing an end-to-end data processing and annotation starting from the documents identification and ending in the generation of statistical reports. The Bulgarian MARCELL corpus consists of 25283 documents (at the beginning of November 2019) which are classified into eleven types.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Natural Language Processing Pipeline to Annotate Bulgarian Legislative Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.lrec-1.863" target="_blank">https://aclanthology.org/2020.lrec-1.863</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Nous pr&#39;esentons une exp&#39;erience visant `a mesurer en quoi la structure logique d&#39;un document impacte les repr&#39;esentations lexicales dans les mod`eles de s&#39;emantique distributionnelle. En nous basant sur des documents structur&#39;es (articles de recherche en TAL) nous comparons des mod`eles construits sur des corpus obtenus par suppression de certaines parties des textes du corpus : titres de section r&#39;esum&#39;es introductions et conclusions. Nous montrons que malgr&#39;e des diff&#39;erences selon les parties et le lexique pris en compte ces zones r&#39;eput&#39;ees particuli`erement informatives du contenu d&#39;un article ont un impact globalement moins significatif que le reste du texte sur la construction du mod`ele.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Impact de la structure logique des documents sur les mod`eles distributionnels : exp&#39;erimentations sur le corpus TALN (Impact of document structure on distributional semantics models: a case study on NLP research articles )</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.jeptalnrecital-taln.10" target="_blank">https://aclanthology.org/2020.jeptalnrecital-taln.10</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Pour comparer deux sorties de logiciels d&#39;OCR le Character Error Rate (ou CER) est fr&#39;equemment utilis&#39;e. Moyennant l&#39;existence d&#39;une transcription de r&#39;ef&#39;erence de qualit&#39;e pour certains documents du corpus le CER calcule le taux d&#39;erreurs de ces pi`eces et permet ensuite de s&#39;electionner le logiciel d&#39;OCR le plus adapt&#39;e. Toutefois ces transcriptions sont tr`es co^uteuses `a produire et peuvent freiner certaines &#39;etudes m^eme prospectives. Nous explorons l&#39;exploitation des mod`eles de langue en agr&#39;egeant selon diff&#39;erentes m&#39;ethodes les probabilit&#39;es offertes par ceux-ci pour estimer la qualit&#39;e d&#39;une sortie d&#39;OCR. L&#39;indice de corr&#39;elation Pearson est ici utilis&#39;e pour comprendre dans quelle mesure ces estimations issues de mod`eles de langue co-varient avec le CER mesure de r&#39;ef&#39;erence.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploiter des mod`eles de langue pour &#39;evaluer des sorties de logiciels d&#39;OCR pour des documents franccais du XVIIe si`ecle ()</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.jeptalnrecital-recital.16" target="_blank">https://aclanthology.org/2020.jeptalnrecital-recital.16</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>La compr&#39;ehensibilit&#39;e de documents audiovisuels peut d&#39;ependre de facteurs propres `a l&#39;auditeur/spectateur (ex. langue maternelle performances cognitives) et de facteurs propres aux contenus des documents (ex. complexit&#39;e linguistique intelligibilit&#39;e de la parole). Dans ces travaux nous &#39;etudions les effets de facteurs propres aux contenus sur la compr&#39;ehensibilit&#39;e de 55 dialogues extraits de films pr&#39;esent&#39;es `a 15 experts (enseignants de franccais langue &#39;etrang`ere) selon cinq modalit&#39;es diff&#39;erentes (transcription transcription + audio audio audio + vid&#39;eo transcription + audio + vid&#39;eo). Les experts ont &#39;evalu&#39;e les dialogues en termes de compr&#39;ehensibilit&#39;e g&#39;en&#39;erale de complexit&#39;e du vocabulaire de complexit&#39;e grammaticale et d&#39;intelligibilit&#39;e de la parole. L&#39;analyse de leurs &#39;evaluations montre que (1) la complexit&#39;e du vocabulaire la complexit&#39;e grammaticale et l&#39;intelligibilit&#39;e de la parole sont significativement corr&#39;el&#39;ees `a la compr&#39;ehensibilit&#39;e g&#39;en&#39;erale et (2) que les &#39;evaluations de compr&#39;ehensibilit&#39;e g&#39;en&#39;erale ont tendance `a ^etre plus &#39;elev&#39;ees lors de pr&#39;esentations multimodales.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>&#39;Etude des facteurs affectant la compr&#39;ehensibilit&#39;e de documents multimodaux : une &#39;etude exp&#39;erimentale (Factors affecting the comprehensibility of multimodal documents : an experimental study )</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.jeptalnrecital-jep.60" target="_blank">https://aclanthology.org/2020.jeptalnrecital-jep.60</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The need to evaluate the ability of context-aware neural machine translation (NMT) models in dealing with specific discourse phenomena arises in document-level NMT. However test sets that satisfy this need are rare. In this paper we propose a test suite to evaluate three common discourse phenomena in English-Chinese translation: pronoun discourse connective and ellipsis where discourse divergences lie across the two languages. The test suite contains 1200 instances 400 for each type of discourse phenomena. We perform both automatic and human evaluation with three state-of-the-art context-aware NMT models on the proposed test suite. Results suggest that our test suite can be used as a challenging benchmark test bed for evaluating document-level NMT. The test suite will be publicly available soon.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Test Suite for Evaluating Discourse Phenomena in Document-level Neural Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.iwdp-1.3" target="_blank">https://aclanthology.org/2020.iwdp-1.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>With regards to WikiSum (CITATION) that empowers applicative explorations of Neural Multi-Document Summarization (MDS) to learn from large scale dataset this study develops two hierarchical Transformers (HT) that describe both the cross-token and cross-document dependencies at the same time allow extended length of input documents. By incorporating word- and paragraph-level multi-head attentions in the decoder based on the parallel and vertical architectures the proposed parallel and vertical hierarchical Transformers (PHT &amp;VHT) generate summaries utilizing context-aware word embeddings together with static and dynamics paragraph embeddings respectively. A comprehensive evaluation is conducted on WikiSum to compare PHT &amp;VHT with established models and to answer the question whether hierarchical structures offer more promising performances than flat structures in the MDS task. The results suggest that our hierarchical models generate summaries of higher quality by better capturing cross-document relationships and save more memory spaces in comparison to flat-structure models. Moreover we recommend PHT given its practical value of higher inference speed and greater memory-saving capacity.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Neural Abstractive Multi-Document Summarization: Hierarchical or Flat Structure?</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.iwdp-1.6" target="_blank">https://aclanthology.org/2020.iwdp-1.6</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A pseudonymisation method for language documentation corpora: An experiment with spoken Komi</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.iwclul-1.1" target="_blank">https://aclanthology.org/2020.iwclul-1.1</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Topic models have been widely used to discover hidden topics in a collection of documents. In this paper we propose to investigate the role of two different types of relational information i.e. document relationships and concept relationships. While exploiting the document network significantly improves topic coherence the introduction of concepts and their relationships does not influence the results both quantitatively and qualitatively.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Which Matters Most? Comparing the Impact of Concept and Document Relationships in Topic Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.insights-1.5" target="_blank">https://aclanthology.org/2020.insights-1.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Clustering documents by type---grouping invoices with invoices and articles with articles---is a desirable first step for organizing large collections of document scans. Humans approaching this task use both the semantics of the text and the document layout to assist in grouping like documents. LayoutLM (Xu et al. 2019) a layout-aware transformer built on top of BERT with state-of-the-art performance on document-type classification could reasonably be expected to outperform regular BERT (Devlin et al. 2018) for document-type clustering. However we find experimentally that BERT significantly outperforms LayoutLM on this task (p textless0.001). We analyze clusters to show where layout awareness is an asset and where it is a liability.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Layout-Aware Text Representations Harm Clustering Documents by Type</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.insights-1.9" target="_blank">https://aclanthology.org/2020.insights-1.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>基於端對端模型化技術之語音文件摘要 (Spoken Document Summarization Using End-to-End Modeling Techniques)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.ijclclp-1.2" target="_blank">https://aclanthology.org/2020.ijclclp-1.2</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents the FinTOC-2020 Shared Task on structure extraction from financial documents its participants results and their findings. This shared task was organized as part of The 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation (FNP-FNS 2020) held at The 28th International Conference on Computational Linguistics (COLING&#39;2020). This shared task aimed to stimulate research in systems for extracting table-of-contents (TOC) from investment documents (such as financial prospectuses) by detecting the document titles and organizing them hierarchically into a TOC. For the second edition of this shared task two subtasks were presented to the participants: one with English documents and the other one with French documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The Financial Document Structure Extraction Shared task (FinToc 2020)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.fnp-1.2" target="_blank">https://aclanthology.org/2020.fnp-1.2</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present the FinCausal 2020 Shared Task on Causality Detection in Financial Documents and the associated FinCausal dataset and discuss the participating systems and results. Two sub-tasks are proposed: a binary classification task (Task 1) and a relation extraction task (Task 2). A total of 16 teams submitted runs across the two Tasks and 13 of them contributed with a system description paper. This workshop is associated to the Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation (FNP-FNS 2020) held at The 28th International Conference on Computational Linguistics (COLING&#39;2020) Barcelona Spain on September 12 2020.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The Financial Document Causality Detection Shared Task (FinCausal 2020)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.fnp-1.3" target="_blank">https://aclanthology.org/2020.fnp-1.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This document describes a system for causality extraction from financial documents submitted as part of the FinCausal 2020 Workshop. The main contribution of this paper is a description of the robust post-processing used to detect the number of cause and effect clauses in a document and extract them. The proposed system achieved a weighted-average F1 score of more than 95% for the official blind test set during the post-evaluation phase and exact clauses match for 83% of the documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>GBe at FinCausal 2020 Task 2: Span-based Causality Extraction for Financial Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.fnp-1.5" target="_blank">https://aclanthology.org/2020.fnp-1.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes the approach we built for the Financial Document Causality Detection Shared Task (FinCausal-2020) Task 2: Cause and Effect Detection. Our approach is based on a multi-class classifier using BiLSTM with Graph Convolutional Neural Network (GCN) trained by minimizing the binary cross entropy loss. In our approach we have not used any extra data source apart from combining the trial and practice dataset. We achieve weighted F1 score to 75.61 percent and are ranked at 7-th place.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>JDD @ FinCausal 2020 Task 2: Financial Document Causality Detection</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.fnp-1.7" target="_blank">https://aclanthology.org/2020.fnp-1.7</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Financial causality detection is centered on identifying connections between different assets from financial news in order to improve trading strategies. FinCausal 2020 - Causality Identification in Financial Documents -- is a competition targeting to boost results in financial causality by obtaining an explanation of how different individual events or chain of events interact and generate subsequent events in a financial environment. The competition is divided into two tasks: (a) a binary classification task for determining whether sentences are causal or not and (b) a sequence labeling task aimed at identifying elements related to cause and effect. Various Transformer-based language models were fine-tuned for the first task and we obtained the second place in the competition with an F1-score of 97.55% using an ensemble of five such language models. Subsequently a BERT model was fine-tuned for the second task and a Conditional Random Field model was used on top of the generated language features; the system managed to identify the cause and effect relationships with an F1-score of 73.10%. We open-sourced the code and made it available at: https://github.com/avramandrei/FinCausal2020.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>UPB at FinCausal-2020 Tasks 1 &amp; 2: Causality Analysis in Financial Documents using Pretrained Language Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.fnp-1.8" target="_blank">https://aclanthology.org/2020.fnp-1.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The FinCausal 2020 shared task aims to detect causality on financial news and identify those parts of the causal sentences related to the underlying cause and effect. We apply ensemble-based and sequence tagging methods for identifying causality and extracting causal subsequences. Our models yield promising results on both sub-tasks with the prospect of further improvement given more time and computing resources. With respect to task 1 we achieved an F1 score of 0.9429 on the evaluation data and a corresponding ranking of 12/14. For task 2 we were ranked 6/10 with an F1 score of 0.76 and an ExactMatch score of 0.1912.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Fraunhofer IAIS at FinCausal 2020 Tasks 1 &amp; 2: Using Ensemble Methods and Sequence Tagging to Detect Causality in Financial Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.fnp-1.10" target="_blank">https://aclanthology.org/2020.fnp-1.10</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents our participation to the FinCausal-2020 Shared Task whose ultimate aim is to extract cause-effect relations from a given financial text. Our participation includes two systems for the two sub-tasks of the FinCausal-2020 Shared Task. The first sub-task (Task-1) consists of the binary classification of the given sentences as causal meaningful (1) or causal meaningless (0). Our approach for the Task-1 includes applying linear support vector machines after transforming the input sentences into vector representations using term frequency-inverse document frequency scheme with 3-grams. The second sub-task (Task-2) consists of the identification of the cause-effect relations in the sentences which are detected as causal meaningful. Our approach for the Task-2 is a CRF-based model which uses linguistically informed features. For the Task-1 the obtained results show that there is a small difference between the proposed approach based on linear support vector machines (F-score 94%) which requires less time compared to the BERT-based baseline (F-score 95%). For the Task-2 although a minor modifications such as the learning algorithm type and the feature representations are made in the conditional random fields based baseline (F-score 52%) we have obtained better results (F-score 60%). The source codes for the both tasks are available online (https://github.com/ozenirgokberk/FinCausal2020.git/).</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ISIKUN at the FinCausal 2020: Linguistically informed Machine-learning Approach for Causality Identification in Financial Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.fnp-1.14" target="_blank">https://aclanthology.org/2020.fnp-1.14</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We describe the work carried out by AMEX AI-LABS on an extractive summarization benchmark task focused on Financial Narratives Summarization (FNS). This task focuses on summarizing annual financial reports which poses two main challenges as compared to typical news document summarization tasks : i) annual reports are more lengthier (average length about 80 pages) as compared to typical news documents and ii) annual reports are more loosely structured e.g. comprising of tables charts textual data and images which makes it challenging to effectively summarize. To address this summarization task we investigate a range of unsupervised supervised and ensemble based techniques. We find that ensemble based techniques perform relatively better as compared to using only the unsupervised and supervised based techniques. Our ensemble based model achieved the highest rank of 9 out of 31 systems submitted for the benchmark task based on Rouge-L evaluation metric.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>AMEX AI-Labs: An Investigative Study on Extractive Summarization of Financial Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.fnp-1.23" target="_blank">https://aclanthology.org/2020.fnp-1.23</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes our system created for the Financial Document Structure Extraction Shared Task (FinTOC-2020): Title Detection. We rely on the Apache PDFBox library to extract text and all additional information e.g. font type and font size from the financial prospectuses. Our constrained system uses only the provided training data without any additional external resources. Our system is based on the Maximum Entropy classifier and various features including font type and font size. Our system achieves F1 score 81% and #1 place in the French track and F1 score 77% and #2 place among 5 participating teams in the English track.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>UWB@FinTOC-2020 Shared Task: Financial Document Title Detection</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.fnp-1.27" target="_blank">https://aclanthology.org/2020.fnp-1.27</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we describe our system submitted to the FinTOC-2020 shared task on financial doc- ument structure extraction. We propose a two-step approach to identify titles in financial docu- ments and to extract their table of contents (TOC). First we identify text blocks as candidates for titles using unsupervised learning based on character-level information of each document. Then we apply supervised learning on a self-constructed regression task to predict the depth of each text block in the document structure hierarchy using transfer learning combined with document features and layout features. It is noteworthy that our single multilingual model performs well on both tasks and on different languages which indicates the usefulness of transfer learning for title detection and TOC generation. Moreover our approach is independent of the presence of actual TOC pages in the documents. It is also one of the few submissions to the FinTOC-2020 shared task addressing both subtasks in both languages English and French with one single model.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Taxy.io@FinTOC-2020: Multilingual Document Structure Extraction using Transfer Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.fnp-1.28" target="_blank">https://aclanthology.org/2020.fnp-1.28</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Title Detection and Table of Contents Generation are important components in detecting document structure. In particular these two elements serve to provide the skeleton of the document providing users with an understanding of organization as well as the relevance of information and where to find information within the document. Here we show that using tesseract with Levenstein distance a feature set inspired by Alk et al. we were able to correctly classify the title to an F1 measure 0.73 and 0.87 and the table-of-contents to a harmonic mean of 0.36 and 0.39 in English and French respectively. Our methodology works with both PDF and scanned documents giving it a wide range of applicability within the document engineering and storage domains.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DNLP@FinTOC&#39;20: Table of Contents Detection in Financial Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.fnp-1.29" target="_blank">https://aclanthology.org/2020.fnp-1.29</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Daniel at the FinSBD-2 Task: Extracting List and Sentence Boundaries from PDF Documents a model-driven approach to PDF document analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.finnlp-1.11" target="_blank">https://aclanthology.org/2020.finnlp-1.11</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Subtl.ai at the FinSBD-2 task: Document Structure Identification by Paying Attention</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.finnlp-1.12" target="_blank">https://aclanthology.org/2020.finnlp-1.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Transformers have shown great success in learning representations for language modelling. However an open challenge still remains on how to systematically aggregate semantic information (word embedding) with positional (or temporal) information (word orders). In this work we propose a new architecture to aggregate the two sources of information using cascaded semantic and positional self-attention network (CSPAN) in the context of document classification. The CSPAN uses a semantic self-attention layer cascaded with Bi-LSTM to process the semantic and positional information in a sequential manner and then adaptively combine them together through a residue connection. Compared with commonly used positional encoding schemes CSPAN can exploit the interaction between semantics and word positions in a more interpretable and adaptive manner and the classification performance can be notably improved while simultaneously preserving a compact model size and high convergence rate. We evaluate the CSPAN model on several benchmark data sets for document classification with careful ablation studies and demonstrate the encouraging results compared with state of the art.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cascaded Semantic and Positional Self-Attention Network for Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.findings-emnlp.59" target="_blank">https://aclanthology.org/2020.findings-emnlp.59</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This work proposes the use of a pretrained sequence-to-sequence model for document ranking. Our approach is fundamentally different from a commonly adopted classification-based formulation based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence model can be trained to generate relevance labels as ``target tokens&#39;&#39; and how the underlying logits of these target tokens can be interpreted as relevance probabilities for ranking. Experimental results on the MS MARCO passage ranking task show that our ranking approach is superior to strong encoder-only models. On three other document retrieval test collections we demonstrate a zero-shot transfer-based approach that outperforms previous state-of-the-art models requiring in-domain cross-validation. Furthermore we find that our approach significantly outperforms an encoder-only architecture in a data-poor setting. We investigate this observation in more detail by varying target tokens to probe the model&#39;s use of latent knowledge. Surprisingly we find that the choice of target tokens impacts effectiveness even for words that are closely related semantically. This finding sheds some light on why our sequence-to-sequence formulation for document ranking is effective. Code and models are available at pygaggle.ai.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Ranking with a Pretrained Sequence-to-Sequence Model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.findings-emnlp.63" target="_blank">https://aclanthology.org/2020.findings-emnlp.63</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Form understanding depends on both textual contents and organizational structure. Although modern OCR performs well it is still challenging to realize general form understanding because forms are commonly used and of various formats. The table detection and handcrafted features in previous works cannot apply to all forms because of their requirements on formats. Therefore we concentrate on the most elementary components the key-value pairs and adopt multimodal methods to extract features. We consider the form structure as a tree-like or graph-like hierarchy of text fragments. The parent-child relation corresponds to the key-value pairs in forms. We utilize the state-of-the-art models and design targeted extraction modules to extract multimodal features from semantic contents layout information and visual images. A hybrid fusion method of concatenation and feature shifting is designed to fuse the heterogeneous features and provide an informative joint representation. We adopt an asymmetric algorithm and negative sampling in our model as well. We validate our method on two benchmarks MedForm and FUNSD and extensive experiments demonstrate the effectiveness of our method.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DocStruct: A Multimodal Method to Extract Hierarchy Structure in Document for General Form Understanding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.findings-emnlp.80" target="_blank">https://aclanthology.org/2020.findings-emnlp.80</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Unstructured documents serving as external knowledge of the dialogues help to generate more informative responses. Previous research focused on knowledge selection (KS) in the document with dialogue. However dialogue history that is not related to the current dialogue may introduce noise in the KS processing. In this paper we propose a Compare Aggregate Transformer (CAT) to jointly denoise the dialogue context and aggregate the document information for response generation. We designed two different comparison mechanisms to reduce noise (before and during decoding). In addition we propose two metrics for evaluating document utilization efficiency based on word overlap. Experimental results on the CMU_DoG dataset show that the proposed CAT model outperforms the state-of-the-art approach and strong baselines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Compare Aggregate Transformer for Understanding Document-grounded Dialogue</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.findings-emnlp.122" target="_blank">https://aclanthology.org/2020.findings-emnlp.122</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Prior research notes that BERT&#39;s computational cost grows quadratically with sequence length thus leading to longer training times higher GPU memory constraints and carbon emissions. While recent work seeks to address these scalability issues at pre-training these issues are also prominent in fine-tuning especially for long sequence tasks like document classification. Our work thus focuses on optimizing the computational cost of fine-tuning for document classification. We achieve this by complementary learning of both topic and language models in a unified framework named TopicBERT. This significantly reduces the number of self-attention operations -- a main performance bottleneck. Consequently our model achieves a 1.4x ( 40%) speedup with 40% reduction in CO2 emission while retaining 99.9% performance over 5 datasets.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>TopicBERT for Energy Efficient Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.findings-emnlp.152" target="_blank">https://aclanthology.org/2020.findings-emnlp.152</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Data programming aims to reduce the cost of curating training data by encoding domain knowledge as labeling functions over source data. As such it not only requires domain expertise but also programming experience a skill that many subject matter experts lack. Additionally generating functions by enumerating rules is not only time consuming but also inherently difficult even for people with programming experience. In this paper we introduce Ruler an interactive system that synthesizes labeling rules using span-level interactive demonstrations over document examples. Ruler is a first-of-a-kind implementation of data programming by demonstration (DPBD). This new framework aims to relieve users from the burden of writing labeling functions enabling them to focus on higher-level semantic analysis such as identifying relevant signals for the labeling task. We compare Ruler with conventional data programming through a user study conducted with 10 data scientists who were asked to create labeling functions for sentiment and spam classification tasks. Results show Ruler is easier to learn and to use and that it offers higher overall user-satisfaction while providing model performances comparable to those achieved by conventional data programming.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Ruler: Data Programming by Demonstration for Document Labeling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.findings-emnlp.181" target="_blank">https://aclanthology.org/2020.findings-emnlp.181</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper problematizes the reliance on documents as the basic notion for defining term interactions in standard topic models. As an alternative to this practice we reformulate topic distributions as latent factors in term similarity space. We exemplify the idea using a number of standard word embeddings built with very wide context windows. The embedding spaces are transformed to sparse similarity spaces and topics are extracted in standard fashion by factorizing to a lower-dimensional space. We use a number of different factorization techniques and evaluate the various models using a large set of evaluation metrics including previously published coherence measures as well as a number of novel measures that we suggest better correspond to real-world applications of topic models. Our results clearly demonstrate that term-based models outperform standard document-based models by a large margin.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Rethinking Topic Modelling: From Document-Space to Term-Space</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.findings-emnlp.204" target="_blank">https://aclanthology.org/2020.findings-emnlp.204</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Single-document and multi-document summarizations are very closely related in both task definition and solution method. In this work we propose to improve neural abstractive multi-document summarization by jointly learning an abstractive single-document summarizer. We build a unified model for single-document and multi-document summarizations by fully sharing the encoder and decoder and utilizing a decoding controller to aggregate the decoder&#39;s outputs for multiple input documents. We evaluate our model on two multi-document summarization datasets: Multi-News and DUC-04. Experimental results show the efficacy of our approach and it can substantially outperform several strong baselines. We also verify the helpfulness of single-document summarization to abstractive multi-document summarization task.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.findings-emnlp.231" target="_blank">https://aclanthology.org/2020.findings-emnlp.231</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present BlockBERT a lightweight and efficient BERT model for better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training/inference time which also enables attention heads to capture either short- or long-range contextual information. We conduct experiments on language model pre-training and several benchmark question answering datasets with various paragraph lengths. BlockBERT uses 18.7-36.1% less memory and 12.0-25.1% less time to learn the model. During testing BlockBERT saves 27.8% inference time while having comparable and sometimes better prediction accuracy compared to an advanced BERT-based model RoBERTa.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Blockwise Self-Attention for Long Document Understanding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.findings-emnlp.232" target="_blank">https://aclanthology.org/2020.findings-emnlp.232</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We tackle the challenge of cross-lingual training of neural document ranking models for mono-lingual retrieval specifically leveraging relevance judgments in English to improve search in non-English languages. Our work successfully applies multi-lingual BERT (mBERT) to document ranking and additionally compares against a number of alternatives: translating the training data translating documents multi-stage hybrids and ensembles. Experiments on test collections in six different languages from diverse language families reveal many interesting findings: model-based relevance transfer using mBERT can significantly improve search quality in (non-English) mono-lingual retrieval but other ``low resource&#39;&#39; approaches are competitive as well.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-Lingual Training of Neural Models for Document Ranking</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.findings-emnlp.249" target="_blank">https://aclanthology.org/2020.findings-emnlp.249</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Multi-document summarization (MDS) is the task of reflecting key points from any set of documents into a concise text paragraph. In the past it has been used to aggregate news tweets product reviews etc. from various sources. Owing to no standard definition of the task we encounter a plethora of datasets with varying levels of overlap and conflict between participating documents. There is also no standard regarding what constitutes summary information in MDS. Adding to the challenge is the fact that new systems report results on a set of chosen datasets which might not correlate with their performance on the other datasets. In this paper we study this heterogeneous task with the help of a few widely used MDS corpora and a suite of state-of-theart models. We make an attempt to quantify the quality of summarization corpus and prescribe a list of points to consider while proposing a new MDS corpus. Next we analyze the reason behind the absence of an MDS system which achieves superior performance across all corpora. We then observe the extent to which system metrics are influenced and bias is propagated due to corpus properties. The scripts to reproduce the experiments in this work are available at https://github.com/LCS2-IIITD/summarization_bias.git</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Corpora Evaluation and System Bias Detection in Multi-document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.findings-emnlp.254" target="_blank">https://aclanthology.org/2020.findings-emnlp.254</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present in this work a method for incorporating global context in long documents when making local decisions in sequence labeling problems like NER. Inspired by work in featurized log-linear models (Chieu and Ng 2002; Sutton and McCallum 2004) our model learns to attend to multiple mentions of the same word type in generating a representation for each token in context extending that work to learning representations that can be incorporated into modern neural models. Attending to broader context at test time provides complementary information to pretraining (Gururangan et al. 2020) yields strong gains over equivalently parameterized models lacking such context and performs best at recognizing entities with high TF-IDF scores (i.e. those that are important within a document).</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Attending to Long-Distance Document Context for Sequence Labeling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.findings-emnlp.330" target="_blank">https://aclanthology.org/2020.findings-emnlp.330</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The global pandemic has made it more important than ever to quickly and accurately retrieve relevant scientific literature for effective consumption by researchers in a wide range of fields. We provide an analysis of several multi-label document classification models on the LitCovid dataset a growing collection of 23000 research papers regarding the novel 2019 coronavirus. We find that pre-trained language models fine-tuned on this dataset outperform all other baselines and that BioBERT surpasses the others by a small margin with micro-F1 and accuracy scores of around 86% and 75% respectively on the test set. We evaluate the data efficiency and generalizability of these models as essential features of any system prepared to deal with an urgent situation like the current health crisis. We perform a data ablation study to determine how important article titles are for achieving reasonable performance on this dataset. Finally we explore 50 errors made by the best performing models on LitCovid documents and find that they often (1) correlate certain labels too closely together and (2) fail to focus on discriminative sections of the articles; both of which are important issues to address in future work. Both data and code are available on GitHub.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Classification for COVID-19 Literature</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.findings-emnlp.332" target="_blank">https://aclanthology.org/2020.findings-emnlp.332</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The Natural Questions (NQ) benchmark set brings new challenges to Machine Reading Comprehension: the answers are not only at different levels of granularity (long and short) but also of richer types (including no-answer yes/no single-span and multi-span). In this paper we target at this challenge and handle all answer types systematically. In particular we propose a novel approach called Reflection Net which leverages a two-step training procedure to identify the no-answer and wrong-answer cases. Extensive experiments are conducted to verify the effectiveness of our approach. At the time of paper writing (May. 20 2020) our approach achieved the top 1 on both long and short answer leaderboard with F1 scores of 77.2 and 64.1 respectively.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>No Answer is Better Than Wrong Answer: A Reflection Model for Document Level Machine Reading Comprehension</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.findings-emnlp.370" target="_blank">https://aclanthology.org/2020.findings-emnlp.370</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The goal of Document-level Relation Extraction (DRE) is to recognize the relations between entity mentions that can span beyond sentence boundary. The current state-of-the-art method for this problem has involved the graph-based edge-oriented model where the entity mentions entities and sentences in the documents are used as the nodes of the document graphs for representation learning. However this model does not capture the representations for the nodes in the graphs thus preventing it from effectively encoding the specific and relevant information of the nodes for DRE. To address this issue we propose to explicitly compute the representations for the nodes in the graph-based edge-oriented model for DRE. These node representations allow us to introduce two novel representation regularization mechanisms to improve the representation vectors for DRE. The experiments show that our model achieves state-of-the-art performance on two benchmark datasets.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The Dots Have Their Values: Exploiting the Node-Edge Connections in Graph-based Neural Models for Document-level Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.findings-emnlp.409" target="_blank">https://aclanthology.org/2020.findings-emnlp.409</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The computing cost of transformer self-attention often necessitates breaking long documents to fit in pretrained models in document ranking tasks. In this paper we design Query-Directed Sparse attention that induces IR-axiomatic structures in transformer self-attention. Our model QDS-Transformer enforces the principle properties desired in ranking: local contextualization hierarchical representation and query-oriented proximity matching while it also enjoys efficiency from sparsity. Experiments on four fully supervised and few-shot TREC document ranking benchmarks demonstrate the consistent and robust advantage of QDS-Transformer over previous approaches as they either retrofit long documents into BERT or use sparse attention without emphasizing IR principles. We further quantify the computing complexity and demonstrates that our sparse attention with TVM implementation is twice more efficient that the fully-connected self-attention. All source codes trained model and predictions of this work are available at https://github.com/hallogameboy/QDS-Transformer.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Long Document Ranking with Query-Directed Sparse Transformer</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.findings-emnlp.412" target="_blank">https://aclanthology.org/2020.findings-emnlp.412</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Query expansion aims to mitigate the mismatch between the language used in a query and in a document. However query expansion methods can suffer from introducing non-relevant information when expanding the query. To bridge this gap inspired by recent advances in applying contextualized models like BERT to the document retrieval task this paper proposes a novel query expansion model that leverages the strength of the BERT model to select relevant document chunks for expansion. In evaluation on the standard TREC Robust04 and GOV2 test collections the proposed BERT-QE model significantly outperforms BERT-Large models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>BERT-QE: Contextualized Query Expansion for Document Re-ranking</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.findings-emnlp.424" target="_blank">https://aclanthology.org/2020.findings-emnlp.424</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We introduce TLDR generation a new form of extreme summarization for scientific papers. TLDR generation involves high source compression and requires expert background knowledge and understanding of complex domain-specific language. To facilitate study on this task we introduce SCITLDR a new multi-target dataset of 5.4K TLDRs over 3.2K papers. SCITLDR contains both author-written and expert-derived TLDRs where the latter are collected using a novel annotation protocol that produces high-quality summaries while minimizing annotation burden. We propose CATTS a simple yet effective learning strategy for generating TLDRs that exploits titles as an auxiliary training signal. CATTS improves upon strong baselines under both automated metrics and human evaluations. Data and code are publicly available at https://github.com/allenai/scitldr.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>TLDR: Extreme Summarization of Scientific Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.findings-emnlp.428" target="_blank">https://aclanthology.org/2020.findings-emnlp.428</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present BLANC a new approach to the automatic estimation of document summary quality. Our goal is to measure the functional performance of a summary with an objective reproducible and fully automated method. Our approach achieves this by measuring the performance boost gained by a pre-trained language model with access to a document summary while carrying out its language understanding task on the document&#39;s text. We present evidence that BLANC scores have as good correlation with human evaluations as do the ROUGE family of summary quality measurements. And unlike ROUGE the BLANC method does not require human-written reference summaries allowing for fully human-free summary quality estimation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Fill in the BLANC: Human-free quality estimation of document summaries</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.eval4nlp-1.2" target="_blank">https://aclanthology.org/2020.eval4nlp-1.2</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We describe Artemis (Annotation methodology for Rich Tractable Extractive Multi-domain Indicative Summarization) a novel hierarchical annotation process that produces indicative summaries for documents from multiple domains. Current summarization evaluation datasets are single-domain and focused on a few domains for which naturally occurring summaries can be easily found such as news and scientific articles. These are not sufficient for training and evaluation of summarization models for use in document management and information retrieval systems which need to deal with documents from multiple domains. Compared to other annotation methods such as Relative Utility and Pyramid Artemis is more tractable because judges don&#39;t need to look at all the sentences in a document when making an importance judgment for one of the sentences while providing similarly rich sentence importance annotations. We describe the annotation process in detail and compare it with other similar evaluation systems. We also present analysis and experimental results over a sample set of 532 annotated documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Artemis: A Novel Annotation Methodology for Indicative Single Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.eval4nlp-1.8" target="_blank">https://aclanthology.org/2020.eval4nlp-1.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>A frequent pattern in customer care conversations is the agents responding with appropriate webpage URLs that address users&#39; needs. We study the task of predicting the documents that customer care agents can use to facilitate users&#39; needs. We also introduce a new public dataset which supports the aforementioned problem. Using this dataset and two others we investigate state-of-the art deep learning (DL) and information retrieval (IR) models for the task. Additionally we analyze the practicality of such systems in terms of inference time complexity. Our show that an hybrid IR+DL approach provides the best of both worlds.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Conversational Document Prediction to Assist Customer Care Agents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.25" target="_blank">https://aclanthology.org/2020.emnlp-main.25</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Multi-document summarization (MDS) aims at producing a good-quality summary for several related documents. In this paper we propose a spectral-based hypothesis which states that the goodness of summary candidate is closely linked to its so-called spectral impact. Here spectral impact considers the perturbation to the dominant eigenvalue of affinity matrix when dropping the summary candidate from the document cluster. The hypothesis is validated by three theoretical perspectives: semantic scaling propagation dynamics and matrix perturbation. According to the hypothesis we formulate the MDS task as the combinatorial optimization of spectral impact and propose an accelerated greedy solution based on a surrogate of spectral impact. The evaluation results on various datasets demonstrate: (1) The performance of the summary candidate is positively correlated with its spectral impact which accords with our hypothesis; (2) Our spectral-based method has a competitive result as compared to state-of-the-art MDS systems.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Spectral Method for Unsupervised Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.32" target="_blank">https://aclanthology.org/2020.emnlp-main.32</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Many document-level neural machine translation (NMT) systems have explored the utility of context-aware architecture usually requiring an increasing number of parameters and computational complexity. However few attention is paid to the baseline model. In this paper we research extensively the pros and cons of the standard transformer in document-level translation and find that the auto-regressive property can simultaneously bring both the advantage of the consistency and the disadvantage of error accumulation. Therefore we propose a surprisingly simple long-short term masking self-attention on top of the standard transformer to both effectively capture the long-range dependence and reduce the propagation of errors. We examine our approach on the two publicly available document-level datasets. We can achieve a strong result in BLEU and capture discourse phenomena.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Long-Short Term Masking Transformer: A Simple but Effective Baseline for Document-level Neural Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.81" target="_blank">https://aclanthology.org/2020.emnlp-main.81</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction it requires reasoning over multiple sentences across paragraphs. In this paper we propose Graph Aggregation-and-Inference Network (GAIN) a method to recognize such relations for long paragraphs. GAIN constructs two graphs a heterogeneous mention-level graph (MG) and an entity-level graph (EG). The former captures complex interaction among different mentions and the latter aggregates mentions underlying for the same entities. Based on the graphs we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset DocRED show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at https://github.com/PKUnlp-icler/GAIN.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Double Graph Based Reasoning for Document-level Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.127" target="_blank">https://aclanthology.org/2020.emnlp-main.127</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>While neural sequence learning methods have made significant progress in single-document summarization (SDS) they produce unsatisfactory results on multi-document summarization (MDS). We observe two major challenges when adapting SDS advances to MDS: (1) MDS involves larger search space and yet more limited training data setting obstacles for neural methods to learn adequate representations; (2) MDS needs to resolve higher information redundancy among the source documents which SDS methods are less effective to handle. To close the gap we present RL-MMR Maximal Margin Relevance-guided Reinforcement Learning for MDS which unifies advanced neural SDS methods and statistical measures used in classical MDS. RL-MMR casts MMR guidance on fewer promising candidates which restrains the search space and thus leads to better representation learning. Additionally the explicit redundancy measure in MMR helps the neural representation of the summary to better capture redundancy. Extensive experiments demonstrate that RL-MMR achieves state-of-the-art performance on benchmark MDS datasets. In particular we show the benefits of incorporating MMR into end-to-end learning when adapting SDS to MDS in terms of both learning effectiveness and efficiency.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-document Summarization with Maximal Marginal Relevance-guided Reinforcement Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.136" target="_blank">https://aclanthology.org/2020.emnlp-main.136</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Images can give us insights into the contextual meanings of words but current image-text grounding approaches require detailed annotations. Such granular annotation is rare expensive and unavailable in most domain-specific contexts. In contrast unlabeled multi-image multi-sentence documents are abundant. Can lexical grounding be learned from such documents even though they have significant lexical and visual overlap? Working with a case study dataset of real estate listings we demonstrate the challenge of distinguishing highly correlated grounded terms such as ``kitchen&#39;&#39; and ``bedroom&#39;&#39; and introduce metrics to assess this document similarity. We present a simple unsupervised clustering-based method that increases precision and recall beyond object detection and image tagging baselines when evaluated on labeled subsets of the dataset. The proposed method is particularly effective for local contextual meanings of a word for example associating ``granite&#39;&#39; with countertops in the real estate dataset and with rocky landscapes in a Wikipedia dataset.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Domain-Specific Lexical Grounding in Noisy Visual-Textual Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.160" target="_blank">https://aclanthology.org/2020.emnlp-main.160</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level neural machine translation has yielded attractive improvements. However majority of existing methods roughly use all context sentences in a fixed scope. They neglect the fact that different source sentences need different sizes of context. To address this problem we propose an effective approach to select dynamic context so that the document-level translation model can utilize the more useful selected context sentences to produce better translations. Specifically we introduce a selection module that is independent of the translation module to score each candidate context sentence. Then we propose two strategies to explicitly select a variable number of context sentences and feed them into the translation module. We train the two modules end-to-end via reinforcement learning. A novel reward is proposed to encourage the selection and utilization of dynamic context sentences. Experiments demonstrate that our approach can select adaptive context sentences for different source sentences and significantly improves the performance of document-level translation methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.175" target="_blank">https://aclanthology.org/2020.emnlp-main.175</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We consider the problem of better modeling query-cluster interactions to facilitate query focused multi-document summarization. Due to the lack of training data existing work relies heavily on retrieval-style methods for assembling query relevant summaries. We propose a coarse-to-fine modeling framework which employs progressively more accurate modules for estimating whether text segments are relevant likely to contain an answer and central. The modules can be independently developed and leverage training data if available. We present an instantiation of this framework with a trained evidence estimator which relies on distant supervision from question answering (where various resources exist) to identify segments which are likely to answer the query and should be included in the summary. Our framework is robust across domains and query types (i.e. long vs short) and outperforms strong comparison systems on benchmark datasets.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Coarse-to-Fine Query Focused Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.296" target="_blank">https://aclanthology.org/2020.emnlp-main.296</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Abstractive document summarization is usually modeled as a sequence-to-sequence (SEQ2SEQ) learning problem. Unfortunately training large SEQ2SEQ based summarization models on limited supervised summarization data is challenging. This paper presents three sequence-to-sequence pre-training (in shorthand STEP) objectives which allow us to pre-train a SEQ2SEQ based abstractive summarization model on unlabeled text. The main idea is that given an input text artificially constructed from a document a model is pre-trained to reinstate the original document. These objectives include sentence reordering next sentence generation and masked document generation which have close relations with the abstractive document summarization task. Experiments on two benchmark summarization datasets (i.e. CNN/DailyMail and New York Times) show that all three objectives can improve performance upon baselines. Compared to models pre-trained on large-scale data (larger than 160GB) our method with only 19GB text for pre-training achieves comparable results which demonstrates its effectiveness.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Pre-training for Abstractive Document Summarization by Reinstating Source Text</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.297" target="_blank">https://aclanthology.org/2020.emnlp-main.297</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Distant supervision (DS) has been widely adopted to generate auto-labeled data for sentence-level relation extraction (RE) and achieved great results. However the existing success of DS cannot be directly transferred to more challenging document-level relation extraction (DocRE) as the inevitable noise caused by DS may be even multiplied in documents and significantly harm the performance of RE. To alleviate this issue we propose a novel pre-trained model for DocRE which de-emphasize noisy DS data via multiple pre-training tasks. The experimental results on the large-scale DocRE benchmark show that our model can capture useful information from noisy data and achieve promising results.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Denoising Relation Extraction from Document-level Distant Supervision</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.300" target="_blank">https://aclanthology.org/2020.emnlp-main.300</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Relation extraction (RE) aims to identify the semantic relations between named entities in text. Recent years have witnessed it raised to the document level which requires complex reasoning with entities and mentions throughout an entire document. In this paper we propose a novel model to document-level RE by encoding the document information in terms of entity global and local representations as well as context relation representations. Entity global representations model the semantic information of all entities in the document entity local representations aggregate the contextual information of multiple mentions of specific entities and context relation representations encode the topic information of other relations. Experimental results demonstrate that our model achieves superior performance on two public datasets for document-level RE. It is particularly effective in extracting relations between entities of long distance and having multiple mentions.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Global-to-Local Neural Networks for Document-Level Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.303" target="_blank">https://aclanthology.org/2020.emnlp-main.303</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Graph Neural Networks (GNNs) that capture the relationships between graph nodes via message passing have been a hot research direction in the natural language processing community. In this paper we propose Graph Topic Model (GTM) a GNN based neural topic model that represents a corpus as a document relationship graph. Documents and words in the corpus become nodes in the graph and are connected based on document-word co-occurrences. By introducing the graph structure the relationships between documents are established through their shared words and thus the topical representation of a document is enriched by aggregating information from its neighboring nodes using graph convolution. Extensive experiments on three datasets were conducted and the results demonstrate the effectiveness of the proposed approach.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Neural Topic Modeling by Incorporating Document Relationship Graph</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.310" target="_blank">https://aclanthology.org/2020.emnlp-main.310</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In politics neologisms are frequently invented for partisan objectives. For example ``undocumented workers&#39;&#39; and ``illegal aliens&#39;&#39; refer to the same group of people (i.e. they have the same denotation) but they carry clearly different connotations. Examples like these have traditionally posed a challenge to reference-based semantic theories and led to increasing acceptance of alternative theories (e.g. Two-Factor Semantics) among philosophers and cognitive scientists. In NLP however popular pretrained models encode both denotation and connotation as one entangled representation. In this study we propose an adversarial nerual netowrk that decomposes a pretrained representation as independent denotation and connotation representations. For intrinsic interpretability we show that words with the same denotation but different connotations (e.g. ``immigrants&#39;&#39; vs. ``aliens&#39;&#39; ``estate tax&#39;&#39; vs. ``death tax&#39;&#39;) move closer to each other in denotation space while moving further apart in connotation space. For extrinsic application we train an information retrieval system with our disentangled representations and show that the denotation vectors improve the viewpoint diversity of document rankings.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Are ``Undocumented Workers&#39;&#39; the Same as ``Illegal Aliens&#39;&#39;? Disentangling Denotation and Connotation in Vector Spaces</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.335" target="_blank">https://aclanthology.org/2020.emnlp-main.335</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We describe a weakly-supervised method for training deep learning models for the task of ad-hoc document retrieval. Our method is based on generative and discriminative models that are trained using weak-supervision just from the documents in the corpus. We present an end-to-end retrieval system that starts with traditional information retrieval methods followed by two deep learning re-rankers. We evaluate our method on three different datasets: a COVID-19 related scientific literature dataset and two news datasets. We show that our method outperforms state-of-the-art methods; this without the need for the expensive process of manually labeling data.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Ad-hoc Document Retrieval using Weak-Supervision with BERT and GPT2</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.343" target="_blank">https://aclanthology.org/2020.emnlp-main.343</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Text alignment finds application in tasks such as citation recommendation and plagiarism detection. Existing alignment methods operate at a single predefined level and cannot learn to align texts at for example sentence textitand document levels. We propose a new learning approach that equips previously established hierarchical attention encoders for representing documents with a cross-document attention component enabling structural comparisons across different levels (document-to-document and sentence-to-document). Our component is weakly supervised from document pairs and can align at multiple levels. Our evaluation on predicting document-to-document relationships and sentence-to-document relationships on the tasks of citation recommendation and plagiarism detection shows that our approach outperforms previously established hierarchical attention encoders based on recurrent and transformer contextualization that are unaware of structural correspondence between documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilevel Text Alignment with Cross-Document Attention</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.407" target="_blank">https://aclanthology.org/2020.emnlp-main.407</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Cross-lingual document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. In this paper we exploit the signals embedded in URLs to label web documents at scale with an average precision of 94.5% across different language pairs. We mine sixty-eight snapshots of the Common Crawl corpus and identify web document pairs that are translations of each other. We release a new web dataset consisting of over 392 million URL pairs from Common Crawl covering documents in 8144 language pairs of which 137 pairs include English. In addition to curating this massive dataset we introduce baseline methods that leverage cross-lingual representations to identify aligned documents based on their textual content. Finally we demonstrate the value of this parallel documents dataset through a downstream task of mining parallel sentences and measuring the quality of machine translations from models trained on this mined data. Our objective in releasing this dataset is to foster new research in cross-lingual NLP across a variety of low medium and high-resource languages.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CCAligned: A Massive Collection of Cross-Lingual Web-Document Pairs</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.480" target="_blank">https://aclanthology.org/2020.emnlp-main.480</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a simple document alignment method that incorporates sentence order information in both candidate generation and candidate re-scoring. Our method results in 61% relative reduction in error compared to the best previously published result on the WMT16 document alignment shared task. Our method improves downstream MT performance on web-scraped Sinhala--English documents from ParaCrawl outperforming the document alignment method used in the most recent ParaCrawl release. It also outperforms a comparable corpora method which uses the same multilingual embeddings demonstrating that exploiting sentence order is beneficial even if the end goal is sentence-level bitext.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploiting Sentence Order in Document Alignment</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.483" target="_blank">https://aclanthology.org/2020.emnlp-main.483</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Existing language models excel at writing from scratch but many real-world scenarios require rewriting an existing document to fit a set of constraints. Although sentence-level rewriting has been fairly well-studied little work has addressed the challenge of rewriting an entire document coherently. In this work we introduce the task of document-level targeted content transfer and address it in the recipe domain with a recipe as the document and a dietary restriction (such as vegan or dairy-free) as the targeted constraint. We propose a novel model for this task based on the generative pre-trained language model (GPT-2) and train on a large number of roughly-aligned recipe pairs. Both automatic and human evaluations show that our model out-performs existing methods by generating coherent and diverse rewrites that obey the constraint while remaining close to the original document. Finally we analyze our model&#39;s rewrites to assess progress toward the goal of making language generation more attuned to constraints that are substantive rather than stylistic.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Substance over Style: Document-Level Targeted Content Transfer</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.526" target="_blank">https://aclanthology.org/2020.emnlp-main.526</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Neural Document-level Multi-aspect Sentiment Classification (DMSC) usually requires a lot of manual aspect-level sentiment annotations which is time-consuming and laborious. As document-level sentiment labeled data are widely available from online service it is valuable to perform DMSC with such free document-level annotations. To this end we propose a novel Diversified Multiple Instance Learning Network (D-MILN) which is able to achieve aspect-level sentiment classification with only document-level weak supervision. Specifically we connect aspect-level and document-level sentiment by formulating this problem as multiple instance learning providing a way to learn aspect-level classifier from the back propagation of document-level supervision. Two diversified regularizations are further introduced in order to avoid the overfitting on document-level signals during training. Diversified textual regularization encourages the classifier to select aspect-relevant snippets and diversified sentimental regularization prevents the aspect-level sentiments from being overly consistent with document-level sentiment. Experimental results on TripAdvisor and BeerAdvocate datasets show that D-MILN remarkably outperforms recent weakly-supervised baselines and is also comparable to the supervised method.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Diversified Multiple Instance Learning for Document-Level Multi-Aspect Sentiment Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.570" target="_blank">https://aclanthology.org/2020.emnlp-main.570</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Multi-document summarization is a challenging task for which there exists little large-scale datasets. We propose Multi-XScience a large-scale multi-document summarization dataset created from scientific articles. Multi-XScience introduces a challenging multi-document summarization task: writing the related-work section of a paper based on its abstract and the articles it references. Our work is inspired by extreme summarization a dataset construction protocol that favours abstractive modeling approaches. Descriptive statistics and empirical results---using several state-of-the-art models trained on the Multi-XScience dataset---reveal that Multi-XScience is well suited for abstractive models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.648" target="_blank">https://aclanthology.org/2020.emnlp-main.648</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We introduce doc2dial a new dataset of goal-oriented dialogues that are grounded in the associated documents. Inspired by how the authors compose documents for guiding end users we first construct dialogue flows based on the content elements that corresponds to higher-level relations across text sections as well as lower-level relations between discourse units within a section. Then we present these dialogue flows to crowd contributors to create conversational utterances. The dataset includes over 4500 annotated conversations with an average of 14 turns that are grounded in over 450 documents from four domains. Compared to the prior document-grounded dialogue datasets this dataset covers a variety of dialogue scenes in information-seeking conversations. For evaluating the versatility of the dataset we introduce multiple dialogue modeling tasks and present baseline approaches.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.652" target="_blank">https://aclanthology.org/2020.emnlp-main.652</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Text classification is a fundamental problem in natural language processing. Recent studies applied graph neural network (GNN) techniques to capture global word co-occurrence in a corpus. However previous works are not scalable to large-sized corpus and ignore the heterogeneity of the text graph. To address these problems we introduce a novel Transformer based heterogeneous graph neural network namely Text Graph Transformer (TG-Transformer). Our model learns effective node representations by capturing structure and heterogeneity from the text graph. We propose a mini-batch text graph sampling method that significantly reduces computing and memory costs to handle large-sized corpus. Extensive experiments have been conducted on several benchmark datasets and the results demonstrate that TG-Transformer outperforms state-of-the-art approaches on text classification task.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Text Graph Transformer for Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.668" target="_blank">https://aclanthology.org/2020.emnlp-main.668</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory which can be impractical for long documents. We argue that keeping all entities in memory is unnecessary and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time thus guaranteeing a linear runtime in length of document. We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.685" target="_blank">https://aclanthology.org/2020.emnlp-main.685</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores. We provide extensive comparisons with strong baseline methods prior state of the art work as well as multiple variants of our approach including those using only transformers only extractive techniques and combinations of the two. We examine these models using four different summarization tasks and datasets: arXiv papers PubMed papers the Newsroom and BigPatent datasets. We find that transformer based methods produce summaries with fewer n-gram copies leading to n-gram copying statistics that are more similar to human generated abstracts. We include a human evaluation finding that transformers are ranked highly for coherence and fluency but purely extractive methods score higher for informativeness and relevance. We hope that these architectures and experiments may serve as strong points of comparison for future work. Note: The abstract above was collaboratively written by the authors and one of the models presented in this paper based on an earlier draft of this paper.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>On Extractive and Abstractive Neural Document Summarization with Transformer Language Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.emnlp-main.748" target="_blank">https://aclanthology.org/2020.emnlp-main.748</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we provide a systematic comparison of existing and new document-level neural machine translation solutions. As part of this comparison we introduce and evaluate a document-level variant of the recently proposed Star Transformer architecture. In addition to using the traditional metric BLEU we report the accuracy of the models in handling anaphoric pronoun translation as well as coherence and cohesion using contrastive test sets. Finally we report the results of human evaluation in terms of Multidimensional Quality Metrics (MQM) and analyse the correlation of the results obtained by the automatic metrics with human judgments.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-level Neural MT: A Systematic Comparison</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.eamt-1.24" target="_blank">https://aclanthology.org/2020.eamt-1.24</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We report on the features and current challenges of our on-going implementation of a Persistent MT workflow for Citrix Product Documentation to increase localization coverage to 100% content in docs.citrix.com into German French Spanish Japanese and Simplified Chinese. By the end of 2019 we had processed seven million words of English documentation with this model across 24 doc sets and raised localization coverage from 40% to 100% of the content of our documentation repositories. This has boosted our global reach across the entire Citrix portfolio (Digital Workspace Networking and Analytics). The current implementation requires a process of Light Post-editing (LPE) for all languages in order to fix over-translations out-of-domain words inline tags and markdown errors in the raw output.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Persistent MT on software technical documentation - a case study</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.eamt-1.37" target="_blank">https://aclanthology.org/2020.eamt-1.37</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level (doc-level) human eval-uation of machine translation (MT) has raised interest in the community after a fewattempts have disproved claims of ``human parity&#39;&#39; (Toral et al. 2018; Laubli et al.2018). However little is known about bestpractices regarding doc-level human evalu-ation. The goal of this project is to identifywhich methodologies better cope with i)the current state-of-the-art (SOTA) humanmetrics ii) a possible complexity when as-signing a single score to a text consisted of`good&#39; and `bad&#39; sentences iii) a possibletiredness bias in doc-level set-ups and iv)the difference in inter-annotator agreement(IAA) between sentence and doc-level set-ups.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Machine Translation Evaluation Project: Methodology Effort and Inter-Annotator Agreement</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.eamt-1.49" target="_blank">https://aclanthology.org/2020.eamt-1.49</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper tackles the task of named entity recognition (NER) applied to digitized historical texts obtained from processing digital images of newspapers using optical character recognition (OCR) techniques. We argue that the main challenge for this task is that the OCR process leads to misspellings and linguistic errors in the output text. Moreover historical variations can be present in aged documents which can impact the performance of the NER process. We conduct a comparative evaluation on two historical datasets in German and French against previous state-of-the-art models and we propose a model based on a hierarchical stack of Transformers to approach the NER task for historical data. Our findings show that the proposed model clearly improves the results on both historical datasets and does not degrade the results for modern datasets.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Alleviating Digitization Errors in Named Entity Recognition for Historical Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.conll-1.35" target="_blank">https://aclanthology.org/2020.conll-1.35</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Keyphrase extraction is the task of extracting a small set of phrases that best describe a document. Most existing benchmark datasets for the task typically have limited numbers of annotated documents making it challenging to train increasingly complex neural networks. In contrast digital libraries store millions of scientific articles online covering a wide range of topics. While a significant portion of these articles contain keyphrases provided by their authors most other articles lack such kind of annotations. Therefore to effectively utilize these large amounts of unlabeled articles we propose a simple and efficient joint learning approach based on the idea of self-distillation. Experimental results show that our approach consistently improves the performance of baseline models for keyphrase extraction. Furthermore our best models outperform previous methods for the task achieving new state-of-the-art results on two public benchmarks: Inspec and SemEval-2017.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Joint Learning Approach based on Self-Distillation for Keyphrase Extraction from Scientific Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.56" target="_blank">https://aclanthology.org/2020.coling-main.56</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document layout analysis usually relies on computer vision models to understand documents while ignoring textual information that is vital to capture. Meanwhile high quality labeled datasets with both visual and textual information are still insufficient. In this paper we present DocBank a benchmark dataset that contains 500K document pages with fine-grained token-level annotations for document layout analysis. DocBank is constructed using a simple yet effective way with weak supervision from the LaTeX documents available on the arXiv.com. With DocBank models from different modalities can be compared fairly and multi-modal approaches will be further investigated and boost the performance of document layout analysis. We build several strong baselines and manually split train/dev/test sets for evaluation. Experiment results show that models trained on DocBank accurately recognize the layout information for a variety of documents. The DocBank dataset is publicly available at https://github.com/doc-analysis/DocBank.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DocBank: A Benchmark Dataset for Document Layout Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.82" target="_blank">https://aclanthology.org/2020.coling-main.82</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Most of the research on text simplification is limited to sentence level nowadays. In this paper we are the first to investigate the helpfulness of document context on sentence simplification and apply it to the sequence-to-sequence model. We firstly construct a sentence simplification dataset in which the contexts for the original sentence are provided by Wikipedia corpus. The new dataset contains approximately 116K sentence pairs with context. We then propose a new model that makes full use of the context information. Our model uses neural networks to learn the different effects of the preceding sentences and the following sentences on the current sentence and applies them to the improved transformer model. Evaluated on the newly constructed dataset our model achieves 36.52 on SARI value which outperforms the best performing model in the baselines by 2.46 (7.22%) indicating that context indeed helps improve sentence simplification. In the ablation experiment we show that using either the preceding sentences or the following sentences as context can significantly improve simplification.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>On the Helpfulness of Document Context to Sentence Simplification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.121" target="_blank">https://aclanthology.org/2020.coling-main.121</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level relation extraction requires inter-sentence reasoning capabilities to capture local and global contextual information for multiple relational facts. To improve inter-sentence reasoning we propose to characterize the complex interaction between sentences and potential relation instances via a Graph Enhanced Dual Attention network (GEDA). In GEDA sentence representation generated by the sentence-to-relation (S2R) attention is refined and synthesized by a Heterogeneous Graph Convolutional Network before being fed into the relation-to-sentence (R2S) attention . We further design a simple yet effective regularizer based on the natural duality of the S2R and R2S attention whose weights are also supervised by the supporting evidence of relation instances during training. An extensive set of experiments on an existing large-scale dataset show that our model achieve competitive performance especially for the inter-sentence relation extraction while the neural predictions can also be interpretable and easily observed.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Graph Enhanced Dual Attention Network for Document-Level Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.136" target="_blank">https://aclanthology.org/2020.coling-main.136</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In recent years the plentiful information contained in Chinese legal documents has attracted a great deal of attention because of the large-scale release of the judgment documents on China Judgments Online. It is in great need of enabling machines to understand the semantic information stored in the documents which are transcribed in the form of natural language. The technique of information extraction provides a way of mining the valuable information implied in the unstructured judgment documents. We propose a Legal Triplet Extraction System for drug-related criminal judgment documents. The system extracts the entities and the semantic relations jointly and benefits from the proposed legal lexicon feature and multi-task learning framework. Furthermore we manually annotate a dataset for Named Entity Recognition and Relation Extraction in Chinese legal domain which contributes to training supervised triplet extraction models and evaluating the model performance. Our experimental results show that the legal feature introduction and multi-task learning framework are feasible and effective for the Legal Triplet Extraction System. The F1 score of triplet extraction finally reaches 0.836 on the legal dataset.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Joint Entity and Relation Extraction for Legal Documents with Legal Feature Enhancement</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.137" target="_blank">https://aclanthology.org/2020.coling-main.137</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level relation extraction (RE) poses new challenges over its sentence-level counterpart since it requires an adequate comprehension of the whole document and the multi-hop reasoning ability across multiple sentences to reach the final result. In this paper we propose a novel graph-based model with Dual-tier Heterogeneous Graph (DHG) for document-level RE. In particular DHG is composed of a structure modeling layer followed by a relation reasoning layer. The major advantage is that it is capable of not only capturing both the sequential and structural information of documents but also mixing them together to benefit for multi-hop reasoning and final decision-making. Furthermore we employ Graph Neural Networks (GNNs) based message propagation strategy to accumulate information on DHG. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on two widely used datasets and further analyses suggest that all the modules in our model are indispensable for document-level RE.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-level Relation Extraction with Dual-tier Heterogeneous Graph</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.143" target="_blank">https://aclanthology.org/2020.coling-main.143</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Cross-lingual document search is an information retrieval task in which the queries&#39; language and the documents&#39; language are different. In this paper we study the instability of neural document search models and propose a novel end-to-end robust framework that achieves improved performance in cross-lingual search with different documents&#39; languages. This framework includes a novel measure of the relevance smooth cosine similarity between queries and documents and a novel loss function Smooth Ordinal Search Loss as the objective function. We further provide theoretical guarantee on the generalization error bound for the proposed framework. We conduct experiments to compare our approach with other document search models and observe significant gains under commonly used ranking metrics on the cross-lingual document retrieval task in a variety of languages.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-Lingual Document Retrieval with Smooth Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.323" target="_blank">https://aclanthology.org/2020.coling-main.323</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Research on document-level Neural Machine Translation (NMT) models has attracted increasing attention in recent years. Although the proposed works have proved that the inter-sentence information is helpful for improving the performance of the NMT models what information should be regarded as context remains ambiguous. To solve this problem we proposed a novel cache-based document-level NMT model which conducts dynamic caching guided by theme-rheme information. The experiments on NIST evaluation sets demonstrate that our proposed model achieves substantial improvements over the state-of-the-art baseline NMT models. As far as we know we are the first to introduce theme-rheme theory into the field of machine translation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Document-Level Neural Machine Translation Model with Dynamic Caching Guided by Theme-Rheme Information</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.388" target="_blank">https://aclanthology.org/2020.coling-main.388</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level machine translation focuses on the translation of entire documents from a source to a target language. It is widely regarded as a challenging task since the translation of the individual sentences in the document needs to retain aspects of the discourse at document level. However document-level translation models are usually not trained to explicitly ensure discourse quality. Therefore in this paper we propose a training approach that explicitly optimizes two established discourse metrics lexical cohesion and coherence by using a reinforcement learning objective. Experiments over four different language pairs and three translation domains have shown that our training approach has been able to achieve more cohesive and coherent document translations than other competitive approaches yet without compromising the faithfulness to the reference translation. In the case of the Zh-En language pair our method has achieved an improvement of 2.46 percentage points (pp) in LC and 1.17 pp in COH over the runner-up while at the same time improving 0.63 pp in BLEU score and 0.47 pp in F-BERT.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Leveraging Discourse Rewards for Document-Level Neural Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.395" target="_blank">https://aclanthology.org/2020.coling-main.395</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>An essential task of most Question Answering (QA) systems is to re-rank the set of answer candidates i.e. Answer Sentence Selection (AS2). These candidates are typically sentences either extracted from one or more documents preserving their natural order or retrieved by a search engine. Most state-of-the-art approaches to the task use huge neural models such as BERT or complex attentive architectures. In this paper we argue that by exploiting the intrinsic structure of the original rank together with an effective word-relatedness encoder we achieve the highest accuracy among the cost-efficient models with two orders of magnitude fewer parameters than the current state of the art. Our model takes 9.5 seconds to train on the WikiQA dataset i.e. very fast in comparison with the 18 minutes required by a standard BERT-base fine-tuning.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Study on Efficiency Accuracy and Document Structure for Answer Sentence Selection</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.457" target="_blank">https://aclanthology.org/2020.coling-main.457</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level Relation Extraction (RE) is particularly challenging due to complex semantic interactions among multiple entities in a document. Among exiting approaches Graph Convolutional Networks (GCN) is one of the most effective approaches for document-level RE. However traditional GCN simply takes word nodes and adjacency matrix to represent graphs which is difficult to establish direct connections between distant entity pairs. In this paper we propose Global Context-enhanced Graph Convolutional Networks (GCGCN) a novel model which is composed of entities as nodes and context of entity pairs as edges between nodes to capture rich global context information of entities in a document. Two hierarchical blocks Context-aware Attention Guided Graph Convolution (CAGGC) for partially connected graphs and Multi-head Attention Guided Graph Convolution (MAGGC) for fully connected graphs could take progressively more global context into account. Meantime we leverage a large-scale distantly supervised dataset to pre-train a GCGCN model with curriculum learning which is then fine-tuned on the human-annotated dataset for further improving document-level RE performance. The experimental results on DocRED show that our model could effectively capture rich global context information in the document leading to a state-of-the-art result. Our code is available at https://github.com/Huiweizhou/GCGCN.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Global Context-enhanced Graph Convolutional Networks for Document-level Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.461" target="_blank">https://aclanthology.org/2020.coling-main.461</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Text summarization refers to the process that generates a shorter form of text from the source document preserving salient information. Many existing works for text summarization are generally evaluated by using recall-oriented understudy for gisting evaluation (ROUGE) scores. However as ROUGE scores are computed based on n-gram overlap they do not reflect semantic meaning correspondences between generated and reference summaries. Because Korean is an agglutinative language that combines various morphemes into a word that express several meanings ROUGE is not suitable for Korean summarization. In this paper we propose evaluation metrics that reflect semantic meanings of a reference summary and the original document Reference and Document Aware Semantic Score (RDASS). We then propose a method for improving the correlation of the metrics with human judgment. Evaluation results show that the correlation with human judgment is significantly higher for our evaluation metrics than for ROUGE scores.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Reference and Document Aware Semantic Evaluation Methods for Korean Language Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.491" target="_blank">https://aclanthology.org/2020.coling-main.491</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Extractive methods have been proven effective in automatic document summarization. Previous works perform this task by identifying informative contents at sentence level. However it is unclear whether performing extraction at sentence level is the best solution. In this work we show that unnecessity and redundancy issues exist when extracting full sentences and extracting sub-sentential units is a promising alternative. Specifically we propose extracting sub-sentential units based on the constituency parsing tree. A neural extractive model which leverages the sub-sentential information and extracts them is presented. Extensive experiments and analyses show that extracting sub-sentential units performs competitively comparing to full sentence extraction under the evaluation of both automatic and human evaluations. Hopefully our work could provide some inspiration of the basic extraction units in extractive summarization for future research.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>At Which Level Should We Extract? An Empirical Analysis on Extractive Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.492" target="_blank">https://aclanthology.org/2020.coling-main.492</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recent work has shown that pre-trained Transformers obtain remarkable performance on many natural language processing tasks including automatic summarization. However most work has focused on (relatively) data-rich single-document summarization settings. In this paper we explore highly-abstractive multi-document summarization where the summary is explicitly conditioned on a user-given topic statement or question. We compare the summarization quality produced by three state-of-the-art transformer-based models: BART T5 and PEGASUS. We report the performance on four challenging summarization datasets: three from the general domain and one from consumer health in both zero-shot and few-shot learning settings. While prior work has shown significant differences in performance for these models on standard summarization tasks our results indicate that with as few as 10 labeled examples there is no statistically significant difference in summary quality suggesting the need for more abstractive benchmark collections when determining state-of-the-art.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Flight of the PEGASUS? Comparing Transformers on Few-shot and Zero-shot Multi-document Abstractive Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.494" target="_blank">https://aclanthology.org/2020.coling-main.494</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In the Query Focused Multi-Document Summarization (QF-MDS) task a set of documents and a query are given where the goal is to generate a summary from these documents based on the given query. However one major challenge for this task is the lack of availability of labeled training datasets. To overcome this issue in this paper we propose a novel weakly supervised learning approach via utilizing distant supervision. In particular we use datasets similar to the target dataset as the training data where we leverage pre-trained sentence similarity models to generate the weak reference summary of each individual document in a document set from the multi-document gold reference summaries. Then we iteratively train our summarization model on each single-document to alleviate the computational complexity issue that occurs while training neural summarization models in multiple documents (i.e. long sequences) at once. Experimental results on the Document Understanding Conferences (DUC) datasets show that our proposed approach sets a new state-of-the-art result in terms of various evaluation metrics.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>WSL-DS: Weakly Supervised Learning with Distant Supervision for Query Focused Multi-Document Abstractive Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.495" target="_blank">https://aclanthology.org/2020.coling-main.495</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The widespread adoption of reference-based automatic evaluation metrics such as ROUGE has promoted the development of document summarization. In this paper we consider a new protocol for designing reference-based metrics that require the endorsement of source document(s). Following protocol we propose an anchored ROUGE metric fixing each summary particle on source document which bases the computation on more solid ground. Empirical results on benchmark datasets validate that source document helps to induce a higher correlation with human judgments for ROUGE metric. Being self-explanatory and easy-to-implement the protocol can naturally foster various effective designs of reference-based metrics besides the anchored ROUGE introduced here.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Anchor-Based Automatic Evaluation Metric for Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.500" target="_blank">https://aclanthology.org/2020.coling-main.500</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Podcasts are a large and growing repository of spoken audio. As an audio format podcasts are more varied in style and production type than broadcast news contain more genres than typically studied in video data and are more varied in style and format than previous corpora of conversations. When transcribed with automatic speech recognition they represent a noisy but fascinating collection of documents which can be studied through the lens of natural language processing information retrieval and linguistics. Paired with the audio files they are also a resource for speech processing and the study of paralinguistic sociolinguistic and acoustic aspects of the domain. We introduce the Spotify Podcast Dataset a new corpus of 100000 podcasts. We demonstrate the complexity of the domain with a case study of two tasks: (1) passage search and (2) summarization. This is orders of magnitude larger than previous speech corpora used for search and summarization. Our results show that the size and variability of this corpus opens up new avenues for research.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>100000 Podcasts: A Spoken English Document Corpus</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.519" target="_blank">https://aclanthology.org/2020.coling-main.519</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Traditional document similarity measures provide a coarse-grained distinction between similar and dissimilar documents. Typically they do not consider in what aspects two documents are similar. This limits the granularity of applications like recommender systems that rely on document similarity. In this paper we extend similarity with aspect information by performing a pairwise document classification task. We evaluate our aspect-based document similarity approach for research papers. Paper citations indicate the aspect-based similarity i.e. the title of a section in which a citation occurs acts as a label for the pair of citing and cited paper. We apply a series of Transformer models such as RoBERTa ELECTRA XLNet and BERT variations and compare them to an LSTM baseline. We perform our experiments on two newly constructed datasets of 172073 research paper pairs from the ACL Anthology and CORD-19 corpus. According to our results SciBERT is the best performing system with F1-scores of up to 0.83. A qualitative analysis validates our quantitative results and indicates that aspect-based document similarity indeed leads to more fine-grained recommendations.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Aspect-based Document Similarity for Research Papers</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.545" target="_blank">https://aclanthology.org/2020.coling-main.545</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Sentiment analysis is an area of substantial relevance both in industry and in academia including for instance in social studies. Although supervised learning algorithms have advanced considerably in recent years in many settings it remains more practical to apply an unsupervised technique. The latter are oftentimes based on sentiment lexicons. However existing sentiment lexicons reflect an abstract notion of polarity and do not do justice to the substantial differences of word polarities between different domains. In this work we draw on a collection of domain-specific data to induce a set of 24 domain-specific sentiment lexicons. We rely on initial linear models to induce initial word intensity scores and then train new deep models based on word vector representations to overcome the scarcity of the original seed data. Our analysis shows substantial differences between domains which make domain-specific sentiment lexicons a promising form of lexical resource in downstream tasks and the predicted lexicons indeed perform effectively on tasks such as review classification and cross-lingual word sentiment prediction.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Domain-Specific Sentiment Lexicons Induced from Labeled Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.578" target="_blank">https://aclanthology.org/2020.coling-main.578</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Past work that improves document-level sentiment analysis by encoding user and product in- formation has been limited to considering only the text of the current review. We investigate incorporating additional review text available at the time of sentiment prediction that may prove meaningful for guiding prediction. Firstly we incorporate all available historical review text belonging to the author of the review in question. Secondly we investigate the inclusion of his- torical reviews associated with the current product (written by other users). We achieve this by explicitly storing representations of reviews written by the same user and about the same product and force the model to memorize all reviews for one particular user and product. Additionally we drop the hierarchical architecture used in previous work to enable words in the text to directly attend to each other. Experiment results on IMDB Yelp 2013 and Yelp 2014 datasets show improvement to state-of-the-art of more than 2 percentage points in the best case.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Document-Level Sentiment Analysis with User and Product Context</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-main.590" target="_blank">https://aclanthology.org/2020.coling-main.590</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>A prominent problem faced by conversational agents working with large documents (Eg: email-based assistants) is the frequent presence of information in the document that is irrelevant to the assistant. This in turn makes it harder for the agent to accurately detect intents extract entities relevant to those intents and perform the desired action. To address this issue we present a neural model for scoping relevant information for the agent from a large document. We show that when used as the first step in a popularly used email-based assistant for helping users schedule meetings our proposed model helps improve the performance of the intent detection and entity extraction tasks required by the agent for correctly scheduling meetings: across a suite of 6 downstream tasks by using our proposed method we observe an average gain of 35% in precision without any drop in recall. Additionally we demonstrate that the same approach can be used for component level analysis in large documents such as signature block identification.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ScopeIt: Scoping Task Relevant Sentences in Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.coling-industry.20" target="_blank">https://aclanthology.org/2020.coling-industry.20</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We address the problem of linking related documents across languages in a multilingual collection. We evaluate three diverse unsupervised methods to represent and compare documents: (1) multilingual topic model; (2) cross-lingual document embeddings; and (3) Wasserstein distance.We test the performance of these methods in retrieving news articles in Swedish that are known to be related to a given Finnish article.The results show that ensembles of the methods outperform the stand-alone methods suggesting that they capture complementary characteristics of the documents</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Comparison of Unsupervised Methods for Ad hoc Cross-Lingual Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.clssts-1.6" target="_blank">https://aclanthology.org/2020.clssts-1.6</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We describe the human triage scenario envisioned in the Cross-Lingual Information Retrieval (CLIR) problem of the [REDUCT] Program. The overall goal is to maximize the quality of the set of documents that is given to a bilingual analyst as measured by the AQWV score. The initial set of source documents that are retrieved by the CLIR system is summarized in English and presented to human judges who attempt to remove the irrelevant documents (false alarms); the resulting documents are then presented to the analyst. First we describe the AQWV performance measure and show that in our experience if the acceptance threshold of the CLIR component has been optimized to maximize AQWV the loss in AQWV due to false alarms is relatively constant across many conditions which also limits the possible gain that can be achieved by any post filter (such as human judgments) that removes false alarms. Second we analyze the likely benefits for the triage operation as a function of the initial CLIR AQWV score and the ability of the human judges to remove false alarms without removing relevant documents. Third we demonstrate that we can increase the benefit for human judgments by combining the human judgment scores with the original document scores returned by the automatic CLIR system.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>What Set of Documents to Present to an Analyst?</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.clssts-1.9" target="_blank">https://aclanthology.org/2020.clssts-1.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The paper presents the categorisation of Bulgarian MARCELL corpus in toplevel EuroVoc domains. The Bulgarian MARCELL corpus is part of a recently developed multilingual corpus representing the national legislation in seven European countries. We performed several experiments with JEX Indexer with neural networks and with a basic method measuring the domain-specific terms in documents annotated in advance with IATE terms and EuroVoc descriptors (combined with grouping of a primary document and its satellites term extraction and parsing of the titles of the documents). The evaluation shows slight overweight of the basic method which makes it appropriate as the categorisation should be a module of a NLP Pipeline for Bulgarian that is continuously feeding and annotating the Bulgarian MARCELL corpus with newly issued legislative documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Categorisation of Bulgarian Legislative Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.clib-1.6" target="_blank">https://aclanthology.org/2020.clib-1.6</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we learn how to manage a dialogue relying on discourse of its utterances. We define extended discourse trees introduce means to manipulate with them and outline scenarios of multi-document navigation to extend the abilities of the interactive information retrieval-based chat bot. We also provide evaluation results of the comparison between conventional search and chat bot enriched with the multi-document navigation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Controlling Chat Bot Multi-Document Navigation with the Extended Discourse Trees</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.clib-1.7" target="_blank">https://aclanthology.org/2020.clib-1.7</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>篇章级事件抽取研究从整篇文档中检测事件识别出事件包含的元素并赋予每个元素特定的角色。本文针对限定领域的中文文档提出了基于BERT的端到端模型在模型的元素和角色识别中依次引入前序层输出的事件类型以及实体嵌入表示增强文本的事件、元素和角色关联表示提高篇章中各事件所属元素的识别精度。在此基础上利用标题信息和事件五元组的嵌入式表示实现主从事件的划分及元素融合。实验证明本文的方法与现有工作相比具有明显的提升。</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>基于BERT的端到端中文篇章事件抽取(A BERT-based End-to-End Model for Chinese Document-level Event Extraction)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.ccl-1.36" target="_blank">https://aclanthology.org/2020.ccl-1.36</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>如何有效利用篇章上下文信息一直是篇章级神经机器翻译研究领域的一大挑战。本文提出利用来源于整个篇章的层次化全局上下文提高篇章级神经机器翻译性能。为了实现该目标本文模型分别获取当前句内单词与篇章内所有句子及单词之间的依赖关系结合不同层次的依赖关系以获取含有层次化篇章信息的全局上下文。最终源语言当前句子中的每个单词都能获取其独有的综合词和句级别依赖关系的上下文。为了充分利用平行句对语料在训练中的优势本文使用两步训练法在句子级语料训练模型的基础上使用含有篇章信息的语料进行二次训练以获得捕获全局上下文的能力。在若干基准语料数据集上的实验表明本文提出的模型与若干强基准模型相比取得了有意义的翻译质量提升。实验进一步表明结合层次化篇章信息的上下文比仅使用词级别上下文更具优势。除此之外本文尝试通过不同方式将全局上下文与翻译模型结合并观察其对模型性能的影响并初步探究篇章翻译中全局上下文在篇章中的分布情况。</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>层次化结构全局上下文增强的篇章级神经机器翻译(Hierarchical Global Context Augmented Document-level Neural Machine Translation)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.ccl-1.40" target="_blank">https://aclanthology.org/2020.ccl-1.40</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Information extraction from documents such as receipts or invoices is a fundamental and crucial step for office automation. Many approaches focus on extracting entities and relationships from plain texts however when it comes to document images such demand becomes quite challenging since visual and layout information are also of great significance to help tackle this problem. In this work we propose the attention-based graph neural network to combine textual and visual information from document images.Moreover the global node is introduced in our graph construction algorithm which is used as a virtual hub to collect the information from all the nodes and edges to help improve the performance. Extensive experiments on real-world datasets show that our method outperforms baseline methods by significant margins.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Attention-Based Graph Neural Network with Global Context Awareness for Document Understanding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.ccl-1.79" target="_blank">https://aclanthology.org/2020.ccl-1.79</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents a reinforcement learning approach to extract noise in long clinical documents for the task of readmission prediction after kidney transplant. We face the challenges of developing robust models on a small dataset where each document may consist of over 10K tokens with full of noise including tabular text and task-irrelevant sentences. We first experiment four types of encoders to empirically decide the best document representation and then apply reinforcement learning to remove noisy text from the long documents which models the noise extraction process as a sequential decision problem. Our results show that the old bag-of-words encoder outperforms deep learning-based encoders on this task and reinforcement learning is able to improve upon baseline while pruning out 25% text segments. Our analysis depicts that reinforcement learning is able to identify both typical noisy tokens and task-specific noisy text.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Noise Pollution in Hospital Readmission Prediction: Long Document Classification with Reinforcement Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.bionlp-1.10" target="_blank">https://aclanthology.org/2020.bionlp-1.10</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recently document-level neural machine translation (NMT) has become a hot topic in the community of machine translation. Despite its success most of existing studies ignored the discourse structure information of the input document to be translated which has shown effective in other tasks. In this paper we propose to improve document-level NMT with the aid of discourse structure information. Our encoder is based on a hierarchical attention network (HAN) (Miculicich et al. 2018). Specifically we first parse the input document to obtain its discourse structure. Then we introduce a Transformer-based path encoder to embed the discourse structure information of each word. Finally we combine the discourse structure information with the word embedding before it is fed into the encoder. Experimental results on the English-to-German dataset show that our model can significantly outperform both Transformer and Transformer+HAN.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Modeling Discourse Structure for Document-level Neural Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.autosimtrans-1.5" target="_blank">https://aclanthology.org/2020.autosimtrans-1.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>`Common Law&#39; judicial systems follow the doctrine of precedent which means the legal principles articulated in court judgements are binding in subsequent cases in lower courts. For this reason lawyers must search prior judgements for the legal principles that are relevant to their case. The difficulty for those within the legal profession is that the information that they are looking for may be contained within a few paragraphs or sentences but those few paragraphs may be buried within a hundred-page document. In this study we create a schema based on the relevant information that legal professionals seek within judgements and perform text classification based on it with the aim of not only assisting lawyers in researching cases but eventually enabling large-scale analysis of legal judgements to find trends in court outcomes over time.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Information Extraction from Legal Documents: A Study in the Context of Common Law Court Judgements</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.alta-1.12" target="_blank">https://aclanthology.org/2020.alta-1.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Previous efforts to automate the detection of social and political events in text have primarily focused on identifying events described within single sentences or documents. Within a corpus of documents these automated systems are unable to link event references---recognize singular events across multiple sentences or documents. A separate literature in computational linguistics on event coreference resolution attempts to link known events to one another within (and across) documents. I provide a data set for evaluating methods to identify certain political events in text and to link related texts to one another based on shared events. The data set Headlines of War is built on the Militarized Interstate Disputes data set and offers headlines classified by dispute status and headline pairs labeled with coreference indicators. Additionally I introduce a model capable of accomplishing both tasks. The multi-task convolutional neural network is shown to be capable of recognizing events and event coreferences given the headlines&#39; texts and publication dates.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Seeing the Forest and the Trees: Detection and Cross-Document Coreference Resolution of Militarized Interstate Disputes</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.aespen-1.7" target="_blank">https://aclanthology.org/2020.aespen-1.7</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Classical and some deep learning techniques for Arabic text classification often depend on complex morphological analysis word segmentation and hand-crafted feature engineering. These could be eliminated by using character-level features. We propose a novel end-to-end Arabic document classification framework Arabic document image-based classifier (AraDIC) inspired by the work on image-based character embeddings. AraDIC consists of an image-based character encoder and a classifier. They are trained in an end-to-end fashion using the class balanced loss to deal with the long-tailed data distribution problem. To evaluate the effectiveness of AraDIC we created and published two datasets the Arabic Wikipedia title (AWT) dataset and the Arabic poetry (AraP) dataset. To the best of our knowledge this is the first image-based character embedding framework addressing the problem of Arabic text classification. We also present the first deep learning-based text classifier widely evaluated on modern standard Arabic colloquial Arabic and Classical Arabic. AraDIC shows performance improvement over classical and deep learning baselines by 12.29% and 23.05% for the micro and macro F-score respectively.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>AraDIC: Arabic Document Classification Using Image-Based Character Embeddings and Class-Balanced Loss</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-srw.29" target="_blank">https://aclanthology.org/2020.acl-srw.29</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately we show that the tasks contain complementary information and are best addressed jointly. We introduce Segment Pooling LSTM (S-LSTM) which is capable of jointly segmenting a document and labeling segments. In support of joint training we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average while also improving segment labeling.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Joint Model for Document Segmentation and Segment Labeling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.29" target="_blank">https://aclanthology.org/2020.acl-main.29</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Text classification is fundamental in natural language processing (NLP) and Graph Neural Networks (GNN) are recently applied in this task. However the existing graph-based works can neither capture the contextual word relationships within each document nor fulfil the inductive learning of new words. Therefore in this work to overcome such problems we propose TextING for inductive text classification via GNN. We first build individual graphs for each document and then use GNN to learn the fine-grained word representations based on their local structure which can also effectively produce embeddings for unseen words in the new document. Finally the word nodes are aggregated as the document embedding. Extensive experiments on four benchmark datasets show that our method outperforms state-of-the-art text classification methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.31" target="_blank">https://aclanthology.org/2020.acl-main.31</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Sequence-to-sequence models have lead to significant progress in keyphrase generation but it remains unknown whether they are reliable enough to be beneficial for document retrieval. This study provides empirical evidence that such models can significantly improve retrieval performance and introduces a new extrinsic evaluation framework that allows for a better understanding of the limitations of keyphrase generation models. Using this framework we point out and discuss the difficulties encountered with supplementing documents with -not present in text- keyphrases and generalizing models across domains. Our code is available at https://github.com/boudinfl/ir-using-kg</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Keyphrase Generation for Scientific Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.105" target="_blank">https://aclanthology.org/2020.acl-main.105</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Multi-document summarization (MDS) aims to compress the content in large document collections into short summaries and has important applications in story clustering for newsfeeds presentation of search results and timeline generation. However there is a lack of datasets that realistically address such use cases at a scale large enough for training supervised models for this task. This work presents a new dataset for MDS that is large both in the total number of document clusters and in the size of individual clusters. We build this dataset by leveraging the Wikipedia Current Events Portal (WCEP) which provides concise and neutral human-written summaries of news events with links to external source articles. We also automatically extend these source articles by looking for related articles in the Common Crawl archive. We provide a quantitative analysis of the dataset and empirical results for several state-of-the-art MDS techniques.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.120" target="_blank">https://aclanthology.org/2020.acl-main.120</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We study unsupervised multi-document summarization evaluation metrics which require neither human-written reference summaries nor human annotations (e.g. preferences ratings etc.). We propose SUPERT which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary i.e. selected salient sentences from the source documents using contextualized embeddings and soft token alignment techniques. Compared to the state-of-the-art unsupervised evaluation metrics SUPERT correlates better with human ratings by 18- 39%. Furthermore we use SUPERT as rewards to guide a neural-based reinforcement learning summarizer yielding favorable performance compared to the state-of-the-art unsupervised summarizers. All source code is available at https://github.com/yg211/acl20-ref-free-eval.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.124" target="_blank">https://aclanthology.org/2020.acl-main.124</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically our model achieves an F1 score of 59.05 on a large-scale document-level dataset (DocRED) significantly improving over the previous results and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore extensive analyses show that the model is able to discover more accurate inter-sentence relations.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Reasoning with Latent Structure Refinement for Document-Level Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.141" target="_blank">https://aclanthology.org/2020.acl-main.141</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Predicting reading time has been a subject of much previous work focusing on how different words affect human processing measured by reading time. However previous work has dealt with a limited number of participants as well as word level only predictions (i.e. predicting the time to read a single word). We seek to extend these works by examining whether or not document level predictions are effective given additional information such as subject matter font characteristics and readability metrics. We perform a novel experiment to examine how different features of text contribute to the time it takes to read distributing and collecting data from over a thousand participants. We then employ a large number of machine learning methods to predict a user&#39;s reading time. We find that despite extensive research showing that word level reading time can be most effectively predicted by neural networks larger scale text can be easily and most accurately predicted by one factor the number of words.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>You Don&#39;t Have Time to Read This: An Exploration of Document Reading Time Prediction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.162" target="_blank">https://aclanthology.org/2020.acl-main.162</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness which limits their document-level representation power. For applications on scientific documents such as classification and recommendation accurate embeddings of documents are a necessity. We propose SPECTER a new method to generate document-level embedding of scientific papers based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models Specter can be easily applied to downstream applications without task-specific fine-tuning. Additionally to encourage further research on document-level models we introduce SciDocs a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction to document classification and recommendation. We show that Specter outperforms a variety of competitive baselines on the benchmark.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SPECTER: Document-level Representation Learning using Citation-informed Transformers</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.207" target="_blank">https://aclanthology.org/2020.acl-main.207</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Most of the existing models for document-level machine translation adopt dual-encoder structures. The representation of the source sentences and the document-level contexts are modeled with two separate encoders. Although these models can make use of the document-level contexts they do not fully model the interaction between the contexts and the source sentences and can not directly adapt to the recent pre-training models (e.g. BERT) which encodes multiple sentences with a single encoder. In this work we propose a simple and effective unified encoder that can outperform the baseline models of dual-encoder models in terms of BLEU and METEOR scores. Moreover the pre-training models can further boost the performance of our proposed model.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Simple and Effective Unified Encoder for Document-Level Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.321" target="_blank">https://aclanthology.org/2020.acl-main.321</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In the literature existing studies always consider Aspect Sentiment Classification (ASC) as an independent sentence-level classification problem aspect by aspect which largely ignore the document-level sentiment preference information though obviously such information is crucial for alleviating the information deficiency problem in ASC. In this paper we explore two kinds of sentiment preference information inside a document i.e. contextual sentiment consistency w.r.t. the same aspect (namely intra-aspect sentiment consistency) and contextual sentiment tendency w.r.t. all the related aspects (namely inter-aspect sentiment tendency). On the basis we propose a Cooperative Graph Attention Networks (CoGAN) approach for cooperatively learning the aspect-related sentence representation. Specifically two graph attention networks are leveraged to model above two kinds of document-level sentiment preference information respectively followed by an interactive mechanism to integrate the two-fold preference. Detailed evaluation demonstrates the great advantage of the proposed approach to ASC over the state-of-the-art baselines. This justifies the importance of the document-level sentiment preference information to ASC and the effectiveness of our approach capturing such information.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Aspect Sentiment Classification with Document-level Sentiment Preference Modeling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.338" target="_blank">https://aclanthology.org/2020.acl-main.338</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document clustering requires a deep understanding of the complex structure of long-text; in particular the intra-sentential (local) and inter-sentential features (global). Existing representation learning models do not fully capture these features. To address this we present a novel graph-based representation for document clustering that builds a textitgraph autoencoder (GAE) on a Keyword Correlation Graph. The graph is constructed with topical keywords as nodes and multiple local and global features as edges. A GAE is employed to aggregate the two sets of features by learning a latent representation which can jointly reconstruct them. Clustering is then performed on the learned representations using vector dimensions as features for inducing document classes. Extensive experiments on two datasets show that the features learned by our approach can achieve better clustering performance than other existing features including term frequency-inverse document frequency and average embedding.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Autoencoding Keyword Correlation Graph for Document Clustering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.366" target="_blank">https://aclanthology.org/2020.acl-main.366</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We address the problem of extractive question answering using document-level distant super-vision pairing questions and relevant documents with answer strings. We compare previously used probability space and distant supervision assumptions (assumptions on the correspondence between the weak answer string labels and possible answer mention spans). We show that these assumptions interact and that different configurations provide complementary benefits. We demonstrate that a multi-objective model can efficiently combine the advantages of multiple assumptions and outperform the best individual formulation. Our approach outperforms previous state-of-the-art models by 4.3 points in F1 on TriviaQA-Wiki and 1.7 points in Rouge-L on NarrativeQA summaries.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Probabilistic Assumptions Matter: Improved Models for Distantly-Supervised Document-Level Question Answering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.501" target="_blank">https://aclanthology.org/2020.acl-main.501</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Measuring the scholarly impact of a document without citations is an important and challenging problem. Existing approaches such as Document Influence Model (DIM) are based on dynamic topic models which only consider the word frequency change. In this paper we use both frequency changes and word semantic shifts to measure document influence by developing a neural network framework. Our model has three steps. Firstly we train the word embeddings for different time periods. Subsequently we propose an unsupervised method to align vectors for different time periods. Finally we compute the influence value of documents. Our experimental results show that our model outperforms DIM.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Neural-DINF: A Neural Network based Framework for Measuring Document Influence</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.534" target="_blank">https://aclanthology.org/2020.acl-main.534</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>As a crucial step in extractive document summarization learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network which has a more complex structure for capturing inter-sentence relationships. In this paper we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH) which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Heterogeneous Graph Neural Networks for Extractive Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.553" target="_blank">https://aclanthology.org/2020.acl-main.553</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process which is beneficial for generating coherent and concise summaries. Furthermore pre-trained language models can be easily combined with our model which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Leveraging Graph to Improve Abstractive Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.555" target="_blank">https://aclanthology.org/2020.acl-main.555</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization which jointly learn semantic representations for words sentences and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.556" target="_blank">https://aclanthology.org/2020.acl-main.556</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We propose a novel approach using representation learning for tackling the problem of extracting structured information from form-like document images. We propose an extraction system that uses knowledge of the types of the target fields to generate extraction candidates and a neural network architecture that learns a dense representation of each candidate based on neighboring words in the document. These learned representations are not only useful in solving the extraction task for unseen document templates from two different domains but are also interpretable as we show using loss cases.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Representation Learning for Information Extraction from Form-like Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.580" target="_blank">https://aclanthology.org/2020.acl-main.580</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Natural Questions is a new challenging machine reading comprehension benchmark with two-grained answers which are a long answer (typically a paragraph) and a short answer (one or more entities inside the long answer). Despite the effectiveness of existing methods on this benchmark they treat these two sub-tasks individually during training while ignoring their dependencies. To address this issue we present a novel multi-grained machine reading comprehension framework that focuses on modeling documents at their hierarchical nature which are different levels of granularity: documents paragraphs sentences and tokens. We utilize graph attention networks to obtain different levels of representations so that they can be learned simultaneously. The long and short answers can be extracted from paragraph-level representation and token-level representation respectively. In this way we can model the dependencies between the two-grained answers to provide evidence for each other. We jointly train the two sub-tasks and our experiments show that our approach significantly outperforms previous systems at both long and short answer criteria.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.599" target="_blank">https://aclanthology.org/2020.acl-main.599</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a thorough comparison of two principal approaches to Cross-Lingual Information Retrieval: document translation (DT) and query translation (QT). Our experiments are conducted using the cross-lingual test collection produced within the CLEF eHealth information retrieval tasks in 2013--2015 containing English documents and queries in several European languages. We exploit the Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) paradigms and train several domain-specific and task-specific machine translation systems to translate the non-English queries into English (for the QT approach) and the English documents to all the query languages (for the DT approach). The results show that the quality of QT by SMT is sufficient enough to outperform the retrieval results of the DT approach for all the languages. NMT then further boosts translation quality and retrieval quality for both QT and DT for most languages but still QT provides generally better retrieval results than DT.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Translation vs. Query Translation for Cross-Lingual Information Retrieval in the Medical Domain</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.613" target="_blank">https://aclanthology.org/2020.acl-main.613</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Extracting information from full documents is an important problem in many domains but most previous work focus on identifying relationships within a sentence or a paragraph. It is challenging to create a large-scale information extraction (IE) dataset at the document level since it requires an understanding of the whole document to annotate entities and their document-level relationships that usually span beyond sentences or even sections. In this paper we introduce SciREX a document level IE dataset that encompasses multiple IE tasks including salient entity identification and document level N-ary relation identification from scientific articles. We annotate our dataset by integrating automatic and human annotations leveraging existing scientific knowledge resources. We develop a neural model as a strong baseline that extends previous state-of-the-art IE models to document-level IE. Analyzing the model performance shows a significant gap between human performance and current baselines inviting the community to use our dataset as a challenge to develop document-level IE models. Our data and code are publicly available at https://github.com/allenai/SciREX .</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SciREX: A Challenge Dataset for Document-Level Information Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.670" target="_blank">https://aclanthology.org/2020.acl-main.670</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction as well as how the length of context captured affects the models&#39; performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g. the sentence- and paragraph-level) we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.acl-main.714" target="_blank">https://aclanthology.org/2020.acl-main.714</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Large-scale pre-trained representations such as BERT have been widely used in many natural language understanding tasks. The methods of incorporating BERT into document-level machine translation are still being explored. BERT is able to understand sentence relationships since BERT is pre-trained using the next sentence prediction task. In our work we leverage this property to improve document-level machine translation. In our proposed model BERT performs as a context encoder to achieve document-level contextual information which is then integrated into both the encoder and decoder. Experiment results show that our proposed method can significantly outperform strong document-level machine translation baselines on BLEU score. Moreover the ablation study shows our method can capture document-level context information to boost translation performance.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Neural Machine Translation Using BERT as Context Encoder</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.aacl-srw.15" target="_blank">https://aclanthology.org/2020.aacl-srw.15</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Our analysis of large summarization datasets indicates that redundancy is a very serious problem when summarizing long documents. Yet redundancy reduction has not been thoroughly investigated in neural summarization. In this work we systematically explore and compare different ways to deal with redundancy when summarizing long documents. Specifically we organize existing methods into categories based on when and how the redundancy is considered. Then in the context of these categories we propose three additional methods balancing non-redundancy and importance in a general and flexible way. In a series of experiments we show that our proposed methods achieve the state-of-the-art with respect to ROUGE scores on two scientific paper datasets Pubmed and arXiv while reducing redundancy significantly.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Systematically Exploring Redundancy Reduction in Summarizing Long Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.aacl-main.51" target="_blank">https://aclanthology.org/2020.aacl-main.51</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2020</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. Such aligned data can be used for a variety of NLP tasks from training cross-lingual representations to mining parallel data for machine translation. In this paper we develop an unsupervised scoring function that leverages cross-lingual sentence embeddings to compute the semantic distance between documents in different languages. These semantic distances are then used to guide a document alignment algorithm to properly pair cross-lingual web documents across a variety of low mid and high-resource language pairs. Recognizing that our proposed scoring function and other state of the art methods are computationally intractable for long web documents we utilize a more tractable greedy algorithm that performs comparably. We experimentally demonstrate that our distance metric performs better alignment than current baselines outperforming them by 7% on high-resource language pairs 15% on mid-resource language pairs and 22% on low-resource language pairs.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Massively Multilingual Document Alignment with Cross-lingual Sentence-Mover&#39;s Distance</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2020.aacl-main.62" target="_blank">https://aclanthology.org/2020.aacl-main.62</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Keyphrase generation is the task of predicting a set of lexical units that conveys the main content of a source text. Existing datasets for keyphrase generation are only readily available for the scholarly domain and include non-expert annotations. In this paper we present KPTimes a large-scale dataset of news texts paired with editor-curated keyphrases. Exploring the dataset we show how editors tag documents and how their annotations differ from those found in existing datasets. We also train and evaluate state-of-the-art neural keyphrase generation models on KPTimes to gain insights on how well they perform on the news domain. The dataset is available online at https:// github.com/ygorg/KPTimes.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-8617" target="_blank">https://aclanthology.org/W19-8617</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We propose a data-to-document generator that can easily control the contents of output texts based on a neural language model. Conventional data-to-text model is useful when a reader seeks a global summary of data because it has only to describe an important part that has been extracted beforehand. However because depending on users it differs what they are interested in so it is necessary to develop a method to generate various summaries according to users&#39; interests. We develop a model to generate various summaries and to control their contents by providing the explicit targets for a reference to the model as controllable factors. In the experiments we used five-minute or one-hour charts of 9 indicators (e.g. Nikkei225) as time-series data and daily summaries of Nikkei Quick News as textual data. We conducted comparative experiments using two pieces of information: human-designed topic labels indicating the contents of a sentence and automatically extracted keywords as the referential information for generation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Controlling Contents in Data-to-Document Generation with Human-Designed Topic Labels</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-8640" target="_blank">https://aclanthology.org/W19-8640</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Robust Document Representations for Cross-Lingual Information Retrieval in Low-Resource Settings</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-6602" target="_blank">https://aclanthology.org/W19-6602</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Finance document Extraction Using Data Augmentation and Attention</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-6401" target="_blank">https://aclanthology.org/W19-6401</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The FinTOC-2019 Shared Task: Financial Document Structure Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-6407" target="_blank">https://aclanthology.org/W19-6407</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>UWB@FinTOC-2019 Shared Task: Financial Document Title Detection</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-6411" target="_blank">https://aclanthology.org/W19-6411</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Online Platform for Community-Based Language Description and Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-6001" target="_blank">https://aclanthology.org/W19-6001</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Developing without developers: choosing labor-saving tools for language documentation apps</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-6002" target="_blank">https://aclanthology.org/W19-6002</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Future Directions in Technological Support for Language Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-6003" target="_blank">https://aclanthology.org/W19-6003</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>aiai at FinSBD task: Sentence Boundary Detection in Noisy Texts From Financial Documents Using Deep Attention Model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-5514" target="_blank">https://aclanthology.org/W19-5514</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes the Microsoft Translator submissions to the WMT19 news translation shared task for English-German. Our main focus is document-level neural machine translation with deep transformer models. We start with strong sentence-level baselines trained on large-scale data created via data-filtering and noisy back-translation and find that back-translation seems to mainly help with translationese input. We explore fine-tuning techniques deeper models and different ensembling strategies to counter these effects. Using document boundaries present in the authentic and synthetic parallel data we create sequences of up to 1000 subword segments and train transformer translation models. We experiment with data augmentation techniques for the smaller authentic data with document-boundaries and for larger authentic data without boundaries. We further explore multi-task training for the incorporation of document-level source language monolingual data via the BERT-objective on the encoder and two-pass decoding for combinations of sentence-level and document-level systems. Based on preliminary human evaluation results evaluators strongly prefer the document-level systems over our comparable sentence-level system. The document-level systems also seem to score higher than the human references in source-based direct assessment.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Microsoft Translator at WMT 2019: Towards Large-Scale Document-Level Neural Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-5321" target="_blank">https://aclanthology.org/W19-5321</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We describe our NMT systems submitted to the WMT19 shared task in English→Czech news translation. Our systems are based on the Transformer model implemented in either Tensor2Tensor (T2T) or Marian framework. We aimed at improving the adequacy and coherence of translated documents by enlarging the context of the source and target. Instead of translating each sentence independently we split the document into possibly overlapping multi-sentence segments. In case of the T2T implementation this ``document-level&#39;&#39;-trained system achieves a +0.6 BLEU improvement (p textless 0.05) relative to the same system applied on isolated sentences. To assess the potential effect document-level models might have on lexical coherence we performed a semi-automatic analysis which revealed only a few sentences improved in this aspect. Thus we cannot draw any conclusions from this week evidence.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>English-Czech Systems in WMT19: Document-Level Transformer</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-5337" target="_blank">https://aclanthology.org/W19-5337</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We describe LMU Munich&#39;s machine translation system for English→German translation which was used to participate in the WMT19 shared task on supervised news translation. We specifically participated in the document-level MT track. The system used as a primary submission is a context-aware Transformer capable of both rich modeling of limited contextual information and integration of large-scale document-level context with a less rich representation. We train this model by fine-tuning a big Transformer baseline. Our experimental results show that document-level context provides for large improvements in translation quality and adding a rich representation of the previous sentence provides a small additional gain.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Combining Local and Document-Level Context: The LMU Munich Neural Machine Translation System at WMT19</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-5345" target="_blank">https://aclanthology.org/W19-5345</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>As the quality of machine translation rises and neural machine translation (NMT) is moving from sentence to document level translations it is becoming increasingly difficult to evaluate the output of translation systems. We provide a test suite for WMT19 aimed at assessing discourse phenomena of MT systems participating in the News Translation Task. We have manually checked the outputs and identified types of translation errors that are relevant to document-level translation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Test Suite and Manual Evaluation of Document-Level NMT at WMT19</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-5352" target="_blank">https://aclanthology.org/W19-5352</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We explore using multilingual document embeddings for nearest neighbor mining of parallel data. Three document-level representations are investigated: (i) document embeddings generated by simply averaging multilingual sentence embeddings; (ii) a neural bag-of-words (BoW) document encoding model; (iii) a hierarchical multilingual document encoder (HiDE) that builds on our sentence-level model. The results show document embeddings derived from sentence-level averaging are surprisingly effective for clean datasets but suggest models trained hierarchically at the document-level are more effective on noisy data. Analysis experiments demonstrate our hierarchical models are very robust to variations in the underlying sentence embedding quality. Using document embeddings trained with HiDE achieves the state-of-the-art on United Nations (UN) parallel document mining 94.9% P@1 for en-fr and 97.3% P@1 for en-es.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Hierarchical Document Encoder for Parallel Corpus Mining</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-5207" target="_blank">https://aclanthology.org/W19-5207</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents a high-quality multilingual dataset for the documentation domain to advance research on localization of structured text. Unlike widely-used datasets for translation of plain text we collect XML-structured parallel text segments from the online documentation for an enterprise software platform. These Web pages have been professionally translated from English into 16 languages and maintained by domain experts and around 100000 text segments are available for each language pair. We build and evaluate translation models for seven target languages from English with several different copy mechanisms and an XML-constrained beam search. We also experiment with a non-English pair to show that our dataset has the potential to explicitly enable 17 mboxtimes 16 translation settings. Our experiments show that learning to translate with the XML tags improves translation accuracy and the beam search accurately generates XML structures. We also discuss trade-offs of using the copy mechanisms by focusing on translation of numerical words and named entities. We further provide a detailed human analysis of gaps between the model output and human translations for real-world applications including suitability for post-editing.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A High-Quality Multilingual Dataset for Structured Documentation Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-5212" target="_blank">https://aclanthology.org/W19-5212</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Automatic Term Extraction (ATE) extracts terminology from domain-specific corpora. ATE is used in many NLP tasks including Computer Assisted Translation where it is typically applied to individual documents rather than the entire corpus. While corpus-level ATE has been extensively evaluated it is not obvious how the results transfer to document-level ATE. To fill this gap we evaluate 16 state-of-the-art ATE methods on full-length documents from three different domains on both corpus and document levels. Unlike existing studies our evaluation is more realistic as we take into account all gold terms. We show that no single method is best in corpus-level ATE but C-Value and KeyConceptRelatendess surpass others in document-level ATE.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Evaluating Automatic Term Extraction Methods on Individual Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-5118" target="_blank">https://aclanthology.org/W19-5118</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The text of clinical notes can be a valuable source of patient information and clinical assessments. Historically the primary approach for exploiting clinical notes has been information extraction: linking spans of text to concepts in a detailed domain ontology. However recent work has demonstrated the potential of supervised machine learning to extract document-level codes directly from the raw text of clinical notes. We propose to bridge the gap between the two approaches with two novel syntheses: (1) treating extracted concepts as features which are used to supplement or replace the text of the note; (2) treating extracted concepts as labels which are used to learn a better representation of the text. Unfortunately the resulting concepts do not yield performance gains on the document-level clinical coding task. We explore possible explanations and future research directions.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Clinical Concept Extraction for Document-Level Coding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-5028" target="_blank">https://aclanthology.org/W19-5028</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Segmentation is the first step in building practical discourse parsers and is often neglected in discourse parsing studies. The goal is to identify the minimal spans of text to be linked by discourse relations or to isolate explicit marking of discourse relations. Existing systems on English report F1 scores as high as 95% but they generally assume gold sentence boundaries and are restricted to English newswire texts annotated within the RST framework. This article presents a generic approach and a system ToNy a discourse segmenter developed for the DisRPT shared task where multiple discourse representation schemes languages and domains are represented. In our experiments we found that a straightforward sequence prediction architecture with pretrained contextual embeddings is sufficient to reach performance levels comparable to existing systems when separately trained on each corpus. We report performance between 81% and 96% in F1 score. We also observed that discourse segmentation models only display a moderate generalization capability even within the same language and discourse representation scheme.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ToNy: Contextual embeddings for accurate multilingual discourse segmentation of full documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-2715" target="_blank">https://aclanthology.org/W19-2715</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Mathematical expressions (ME) are widely used in scholar documents. In this paper we analyze characteristics of textual and visual MEs characteristics for the image-to-LaTeX translation task. While there are open data-sets of LaTeX files with MEs included it is very complicated to extract these MEs from a document and to compile the list of MEs. Therefore we release a corpus of open-access scholar documents with PDF and JATS-XML parallel files. The MEs in these documents are LaTeX encoded and are document independent. The data contains more than 1.2 million distinct annotated formulae and more than 80 million raw tokens of LaTeX MEs in more than 8 thousand documents. While the variety of textual lengths and visual sizes of MEs are not well defined we found that the task of analyzing MEs in scholar documents can be reduced to the subtask of a particular text length image width and height bounds and display MEs can be processed as arrays of partial MEs.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Textual and Visual Characteristics of Mathematical Expressions in Scholar Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-2610" target="_blank">https://aclanthology.org/W19-2610</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a portfolio of natural legal language processing and document curation services currently under development in a collaborative European project. First we give an overview of the project and the different use cases while in the main part of the article we focus upon the 13 different processing services that are being deployed in different prototype applications using a flexible and scalable microservices architecture. Their orchestration is operationalised using a content and document curation workflow manager.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Developing and Orchestrating a Portfolio of Natural Legal Language Processing and Document Curation Services</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-2207" target="_blank">https://aclanthology.org/W19-2207</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>NLP naturally puts a primary focus on leveraging document language occasionally considering user attributes as supplemental. However as we tackle more social scientific tasks it is possible user attributes might be of primary importance and the document supplemental. Here we systematically investigate the predictive power of user-level features alone versus document-level features for document-level tasks. We first show user attributes can sometimes carry more task-related information than the document itself. For example a tweet-level stance detection model using only 13 user-level attributes (i.e. features that did not depend on the specific tweet) was able to obtain a higher F1 than the top-performing SemEval participant. We then consider multiple tasks and a wider range of user attributes showing the performance of strong document-only models can often be improved (as in stance sentiment and sarcasm) with user attributes particularly benefiting tasks with stable ``trait-like&#39;&#39; outcomes (e.g. stance) most relative to frequently changing ``state-like&#39;&#39; outcomes (e.g. sentiment). These results not only support the growing work on integrating user factors into predictive systems but that some of our NLP tasks might be better cast primarily as user-level (or human) tasks.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Tweet Classification without the Tweet: An Empirical Examination of User versus Document Attributes</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-2103" target="_blank">https://aclanthology.org/W19-2103</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a very simple unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications it clearly outperforms the more complex system of the original authors. In addition our method is transparent because it provides explicit information about how a similarity score was computed and efficient because it is based on the aggregation of (pre-computable) word-level similarities.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Semantic Matching of Documents from Heterogeneous Collections: A Simple and Transparent Method for Practical Applications</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W19-0804" target="_blank">https://aclanthology.org/W19-0804</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Feature-guided Neural Model Training for Supervised Document Representation Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/U19-1007" target="_blank">https://aclanthology.org/U19-1007</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Despite the success of attention-based neural models for natural language generation and classification tasks they are unable to capture the discourse structure of larger documents. We hypothesize that explicit discourse representations have utility for NLP tasks over longer documents or document sequences which sequence-to-sequence models are unable to capture. For abstractive summarization for instance conventional neural models simply match source documents and the summary in a latent space without explicit representation of text structure or relations. In this paper we propose to use neural discourse representations obtained from a rhetorical structure theory (RST) parser to enhance document representations. Specifically document representations are generated for discourse spans known as the elementary discourse units (EDUs). We empirically investigate the benefit of the proposed approach on two different tasks: abstractive summarization and popularity prediction of online petitions. We find that the proposed approach leads to substantial improvements in all cases.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improved Document Modelling with a Neural Discourse Parser</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/U19-1010" target="_blank">https://aclanthology.org/U19-1010</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Discourse relations between sentences are often represented as a tree and the tree structure provides important information for summarizers to create a short and coherent summary. However current neural network-based summarizers treat the source document as just a sequence of sentences and ignore the tree-like discourse structure inherent in the document. To incorporate the information of a discourse tree structure into the neural network-based summarizers we propose a discourse-aware neural extractive summarizer which can explicitly take into account the discourse dependency tree structure of the source document. Our discourse-aware summarizer can jointly learn the discourse structure and the salience score of a sentence by using novel hierarchical attention modules which can be trained on automatically parsed discourse dependency trees. Experimental results showed that our model achieved competitive or better performances against state-of-the-art models in terms of ROUGE scores on the DailyMail dataset. We further conducted manual evaluations. The results showed that our approach also gained the coherence of the output summaries.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Discourse-Aware Hierarchical Attention Network for Extractive Single-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/R19-1059" target="_blank">https://aclanthology.org/R19-1059</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The paper presents an evaluation of word embedding models in clustering of texts in the Polish language. Authors verified six different embedding models starting from widely used word2vec across fastText with character n-grams embedding to deep learning-based ELMo and BERT. Moreover four standardisation methods three distance measures and four clustering methods were evaluated. The analysis was performed on two corpora of texts in Polish classified into subjects. The Adjusted Mutual Information (AMI) metric was used to verify the quality of clustering results. The performed experiments show that Skipgram models with n-grams character embedding built on KGR10 corpus and provided by Clarin-PL outperforms other publicly available models for Polish. Moreover presented results suggest that Yeo--Johnson transformation for document vectors standardisation and Agglomerative Clustering with a cosine distance should be used for grouping of text documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Evaluation of vector embedding models in clustering of text documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/R19-1149" target="_blank">https://aclanthology.org/R19-1149</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>With the increasing democratization of electronic media vast information resources are available in less-frequently-taught languages such as Swahili or Somali. That information which may be crucially important and not available elsewhere can be difficult for monolingual English speakers to effectively access. In this paper we present an end-to-end cross-lingual information retrieval (CLIR) and summarization system for low-resource languages that 1) enables English speakers to search foreign language repositories of text and audio using English queries 2) summarizes the retrieved documents in English with respect to a particular information need and 3) provides complete transcriptions and translations as needed. The SARAL system achieved the top end-to-end performance in the most recent IARPA MATERIAL CLIR+summarization evaluations. Our demonstration system provides end-to-end open query retrieval and summarization capability and presents the original source text or audio speech transcription and machine translation for two low resource languages.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SARAL: A Low-Resource Cross-Lingual Domain-Focused Information Retrieval System for Effective Rapid Document Triage</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-3004" target="_blank">https://aclanthology.org/P19-3004</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In document-level sentiment classification each document must be mapped to a fixed length vector. Document embedding models map each document to a dense low-dimensional vector in continuous vector space. This paper proposes training document embeddings using cosine similarity instead of dot product. Experiments on the IMDB dataset show that accuracy is improved when using cosine similarity compared to using dot product while using feature combination with Naive Bayes weighted bag of n-grams achieves a competitive accuracy of 93.68%. Code to reproduce all experiments is available at https://github.com/tanthongtan/dv-cosine.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Sentiment Classification Using Document Embeddings Trained with Cosine Similarity</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-2057" target="_blank">https://aclanthology.org/P19-2057</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document Grounded Conversations is a task to generate dialogue responses when chatting about the content of a given document. Obviously document knowledge plays a critical role in Document Grounded Conversations while existing dialogue models do not exploit this kind of knowledge effectively enough. In this paper we propose a novel Transformer-based architecture for multi-turn document grounded conversations. In particular we devise an Incremental Transformer to encode multi-turn utterances along with knowledge in related documents. Motivated by the human cognitive process we design a two-pass decoder (Deliberation Decoder) to improve context coherence and knowledge correctness. Our empirical study on a real-world Document Grounded Dataset proves that responses generated by our model significantly outperform competitive baselines on both context coherence and knowledge relevance.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Incremental Transformer with Deliberation Decoder for Document Grounded Conversations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-1002" target="_blank">https://aclanthology.org/P19-1002</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Multiple entities in a document generally exhibit complex inter-sentence relations and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE we introduce DocRED a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations and is the largest human-annotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data we also offer large-scale distantly supervised data which enables DocRED to be adopted for both supervised and weakly supervised scenarios. In order to verify the challenges of document-level RE we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED. Empirical results show that DocRED is challenging for existing RE methods which indicates that document-level RE remains an open problem and requires further efforts. Based on the detailed analysis on the experiments we discuss multiple promising directions for future research. We make DocRED and the code for our baselines publicly available at https://github.com/thunlp/DocRED.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DocRED: A Large-Scale Document-Level Relation Extraction Dataset</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-1074" target="_blank">https://aclanthology.org/P19-1074</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The most important obstacles facing multi-document summarization include excessive redundancy in source descriptions and the looming shortage of training data. These obstacles prevent encoder-decoder models from being used directly but optimization-based methods such as determinantal point processes (DPPs) are known to handle them well. In this paper we seek to strengthen a DPP-based method for extractive multi-document summarization by presenting a novel similarity measure inspired by capsule networks. The approach measures redundancy between a pair of sentences based on surface form and semantic information. We show that our DPP system with improved similarity measure performs competitively outperforming strong summarization baselines on benchmark datasets. Our findings are particularly meaningful for summarizing documents created by multiple authors containing redundant yet lexically diverse expressions.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving the Similarity Measure of Determinantal Point Processes for Extractive Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-1098" target="_blank">https://aclanthology.org/P19-1098</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper we introduce Multi-News the first large-scale MDS news dataset. Additionally we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-1102" target="_blank">https://aclanthology.org/P19-1102</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Modern entity linking systems rely on large collections of documents specifically annotated for the task (e.g. AIDA CoNLL). In contrast we propose an approach which exploits only naturally occurring information: unlabeled documents and Wikipedia. Our approach consists of two stages. First we construct a high recall list of candidate entities for each mention in an unlabeled document. Second we use the candidate lists as weak supervision to constrain our document-level entity linking model. The model treats entities as latent variables and when estimated on a collection of unlabelled texts learns to choose entities relying both on local context of each mention and on coherence with other entities in the document. The resulting approach rivals fully-supervised state-of-the-art systems on standard test sets. It also approaches their performance in the very challenging setting: when tested on a test set sampled from the data used to estimate the supervised systems. By comparing to Wikipedia-only training of our model we demonstrate that modeling unlabeled documents is beneficial.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Boosting Entity Linking Performance by Leveraging Unlabeled Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-1187" target="_blank">https://aclanthology.org/P19-1187</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Comprehensive document encoding and salient information selection are two major difficulties for generating summaries with adequate salient information. To tackle the above difficulties we propose a Transformer-based encoder-decoder framework with two novel extensions for abstractive document summarization. Specifically (1) to encode the documents comprehensively we design a focus-attention mechanism and incorporate it into the encoder. This mechanism models a Gaussian focal bias on attention scores to enhance the perception of local context which contributes to producing salient and informative summaries. (2) To distinguish salient information precisely we design an independent saliency-selection network which manages the information flow from encoder to decoder. This network effectively reduces the influences of secondary information on the generated summaries. Experimental results on the popular CNN/Daily Mail benchmark demonstrate that our model outperforms other state-of-the-art baselines on the ROUGE metrics.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Abstractive Document Summarization with Salient Information Modeling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-1205" target="_blank">https://aclanthology.org/P19-1205</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper focuses on the end-to-end abstractive summarization of a single product review without supervision. We assume that a review can be described as a discourse tree in which the summary is the root and the child sentences explain their parent in detail. By recursively estimating a parent from its children our model learns the latent discourse tree without an external parser and generates a concise summary. We also introduce an architecture that ranks the importance of each sentence on the tree to support summary generation focusing on the main review point. The experimental results demonstrate that our model is competitive with or outperforms other unsupervised approaches. In particular for relatively long reviews it achieves a competitive or better performance than supervised models. The induced tree shows that the child sentences provide additional information about their parent and the generated summary abstracts the entire review.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-1206" target="_blank">https://aclanthology.org/P19-1206</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper considers the reading comprehension task in which multiple documents are given as input. Prior work has shown that a pipeline of retriever reader and reranker can improve the overall performance. However the pipeline system is inefficient since the input is re-encoded within each module and is unable to leverage upstream components to help downstream training. In this work we present RE^3QA a unified question answering model that combines context retrieving reading comprehension and answer reranking to predict the final answer. Unlike previous pipelined approaches RE^3QA shares contextualized text representation across different components and is carefully designed to use high-quality upstream outputs (e.g. retrieved context or candidate answers) for directly supervising downstream modules (e.g. the reader or the reranker). As a result the whole network can be trained end-to-end to avoid the context inconsistency problem. Experiments show that our model outperforms the pipelined baseline and achieves state-of-the-art results on two versions of TriviaQA and two variants of SQuAD.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Retrieve Read Rerank: Towards End-to-End Multi-Document Reading Comprehension</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-1221" target="_blank">https://aclanthology.org/P19-1221</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set our HDE graph based single model delivers competitive result and the ensemble model achieves the state-of-the-art performance.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-1260" target="_blank">https://aclanthology.org/P19-1260</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we propose to boost low-resource cross-lingual document retrieval performance with deep bilingual query-document representations. We match queries and documents in both source and target languages with four components each of which is implemented as a term interaction-based deep neural network with cross-lingual word embeddings as input. By including query likelihood scores as extra features our model effectively learns to rerank the retrieved documents by using a small number of relevance labels for low-resource language pairs. Due to the shared cross-lingual word embedding space the model can also be directly applied to another language pair without any training label. Experimental results on the Material dataset show that our model outperforms the competitive translation-based baselines on English-Swahili English-Tagalog and English-Somali cross-lingual information retrieval tasks.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Low-Resource Cross-lingual Document Retrieval by Reranking with Deep Bilingual Representations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-1306" target="_blank">https://aclanthology.org/P19-1306</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Word embedding models typically learn two types of vectors: target word vectors and context word vectors. These vectors are normally learned such that they are predictive of some word co-occurrence statistic but they are otherwise unconstrained. However the words from a given language can be organized in various natural groupings such as syntactic word classes (e.g. nouns adjectives verbs) and semantic themes (e.g. sports politics sentiment). Our hypothesis in this paper is that embedding models can be improved by explicitly imposing a cluster structure on the set of context word vectors. To this end our model relies on the assumption that context word vectors are drawn from a mixture of von Mises-Fisher (vMF) distributions where the parameters of this mixture distribution are jointly optimized with the word vectors. We show that this results in word vectors which are qualitatively different from those obtained with existing word embedding models. We furthermore show that our embedding model can also be used to learn high-quality document representations.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Word and Document Embedding with vMF-Mixture Priors on Context Word Vectors</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-1321" target="_blank">https://aclanthology.org/P19-1321</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Language usage can change across periods of time but document classifiers models are usually trained and tested on corpora spanning multiple years without considering temporal variations. This paper describes two complementary ways to adapt classifiers to shifts across time. First we show that diachronic word embeddings which were originally developed to study language change can also improve document classification and we show a simple method for constructing this type of embedding. Second we propose a time-driven neural classification model inspired by methods for domain adaptation. Experiments on six corpora show how these methods can make classifiers more robust over time.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Neural Temporality Adaptation for Document Classification: Diachronic Word Embeddings and Domain Adaptation Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-1403" target="_blank">https://aclanthology.org/P19-1403</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recognizing coreferring events and entities across multiple texts is crucial for many NLP applications. Despite the task&#39;s importance research focus was given mostly to within-document entity coreference with rather little attention to the other variants. We propose a neural architecture for cross-document coreference resolution. Inspired by Lee et al. (2012) we jointly model entity and event coreference. We represent an event (entity) mention using its lexical span surrounding context and relation to entity (event) mentions via predicate-arguments structures. Our model outperforms the previous state-of-the-art event coreference model on ECB+ while providing the first entity coreference results on this corpus. Our analysis confirms that all our representation elements including the mention span itself its context and the relation to other mentions contribute to the model&#39;s success.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Revisiting Joint Modeling of Cross-document Entity and Event Coreference Resolution</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-1409" target="_blank">https://aclanthology.org/P19-1409</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Inter-sentence relation extraction deals with a number of complex semantic relationships in documents which require local non-local syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-1423" target="_blank">https://aclanthology.org/P19-1423</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Neural extractive summarization models usually employ a hierarchical encoder for document encoding and they are trained using sentence-level labels which are created heuristically using rule-based methods. Training the hierarchical encoder with these textitinaccurate labels is challenging. Inspired by the recent work on pre-training transformer sentence encoders (Devlin et al. 2018) we propose Hibert (as shorthand for textbfHIerachical textbfBidirectional textbfEncoder textbfRepresentations from textbfTransformers) for document encoding and a method to pre-train it using unlabeled data. We apply the pre-trained Hibert to our summarization model and it outperforms its randomly initialized counterpart by 1.25 ROUGE on the CNN/Dailymail dataset and by 2.0 ROUGE on a version of New York Times dataset. We also achieve the state-of-the-art performance on these two datasets.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-1499" target="_blank">https://aclanthology.org/P19-1499</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Hierarchical Transformers for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-1500" target="_blank">https://aclanthology.org/P19-1500</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We introduce a novel semantic parsing task based on Discourse Representation Theory (DRT; Kamp and Reyle 1993). Our model operates over Discourse Representation Tree Structures which we formally define for sentences and documents. We present a general framework for parsing discourse structures of arbitrary length and granularity. We achieve this with a neural model equipped with a supervised hierarchical attention mechanism and a linguistically-motivated copy strategy. Experimental results on sentence- and document-level benchmarks show that our model outperforms competitive baselines by a wide margin.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Discourse Representation Parsing for Sentences and Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-1629" target="_blank">https://aclanthology.org/P19-1629</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Automatic summarization is typically treated as a 1-to-1 mapping from document to summary. Documents such as news articles however are structured and often cover multiple topics or aspects; and readers may be interested in only some of them. We tackle the task of aspect-based summarization where given a document and a target aspect our models generate a summary centered around the aspect. We induce latent document structure jointly with an abstractive summarization objective and train our models in a scalable synthetic setup. In addition to improvements in summarization over topic-agnostic baselines we demonstrate the benefit of the learnt document structure: we show that our models (a) learn to accurately segment documents by aspect; (b) can leverage the structure to produce both abstractive and extractive aspect-based summaries; and (c) that structure is particularly advantageous for summarizing long documents. All results transfer from synthetic training documents to natural news articles from CNN/Daily Mail and RCV1.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Inducing Document Structure for Aspect-based Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P19-1630" target="_blank">https://aclanthology.org/P19-1630</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Visually rich documents (VRDs) are ubiquitous in daily business and life. Examples are purchase receipts insurance policy documents custom declaration forms and so on. In VRDs visual and layout information is critical for document understanding and texts in such documents cannot be serialized into the one-dimensional sequence without losing information. Classic information extraction models such as BiLSTM-CRF typically operate on text sequences and do not incorporate visual features. In this paper we introduce a graph convolution based model to combine textual and visual information presented in VRDs. Graph embeddings are trained to summarize the context of a text segment in the document and further combined with text embeddings for entity extraction. Extensive experiments have been conducted to show that our method outperforms BiLSTM-CRF baselines by significant margins on two real-world datasets. Additionally ablation studies are also performed to evaluate the effectiveness of each component of our model.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Graph Convolution for Multimodal Information Extraction from Visually Rich Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N19-2005" target="_blank">https://aclanthology.org/N19-2005</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we propose a novel representation for text documents based on aggregating word embedding vectors into document embeddings. Our approach is inspired by the Vector of Locally-Aggregated Descriptors used for image representation and it works as follows. First the word embeddings gathered from a collection of documents are clustered by k-means in order to learn a codebook of semnatically-related word embeddings. Each word embedding is then associated to its nearest cluster centroid (codeword). The Vector of Locally-Aggregated Word Embeddings (VLAWE) representation of a document is then computed by accumulating the differences between each codeword vector and each word vector (from the document) associated to the respective codeword. We plug the VLAWE representation which is learned in an unsupervised manner into a classifier and show that it is useful for a diverse set of text classification tasks. We compare our approach with a broad range of recent state-of-the-art methods demonstrating the effectiveness of our approach. Furthermore we obtain a considerable improvement on the Movie Review data set reporting an accuracy of 93.3% which represents an absolute gain of 10% over the state-of-the-art approach.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Vector of Locally-Aggregated Word Embeddings (VLAWE): A Novel Document-level Representation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N19-1033" target="_blank">https://aclanthology.org/N19-1033</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we propose a variational approach to weakly supervised document-level multi-aspect sentiment classification. Instead of using user-generated ratings or annotations provided by domain experts we use target-opinion word pairs as ``supervision.&#39;&#39; These word pairs can be extracted by using dependency parsers and simple rules. Our objective is to predict an opinion word given a target word while our ultimate goal is to learn a sentiment polarity classifier to predict the sentiment polarity of each aspect given a document. By introducing a latent variable i.e. the sentiment polarity to the objective function we can inject the sentiment polarity classifier to the objective via the variational lower bound. We can learn a sentiment polarity classifier by optimizing the lower bound. We show that our method can outperform weakly supervised baselines on TripAdvisor and BeerAdvocate datasets and can be comparable to the state-of-the-art supervised method with hundreds of labels per aspect.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Variational Approach to Weakly Supervised Document-Level Multi-Aspect Sentiment Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N19-1036" target="_blank">https://aclanthology.org/N19-1036</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Negation scope detection is widely performed as a supervised learning task which relies upon negation labels at word level. This suffers from two key drawbacks: (1) such granular annotations are costly and (2) highly subjective since due to the absence of explicit linguistic resolution rules human annotators often disagree in the perceived negation scopes. To the best of our knowledge our work presents the first approach that eliminates the need for world-level negation labels replacing it instead with document-level sentiment annotations. For this we present a novel strategy for learning fully interpretable negation rules via weak supervision: we apply reinforcement learning to find a policy that reconstructs negation rules from sentiment predictions at document level. Our experiments demonstrate that our approach for weak supervision can effectively learn negation rules. Furthermore an out-of-sample evaluation via sentiment analysis reveals consistent improvements (of up to 4.66%) over both a sentiment analysis with (i) no negation handling and (ii) the use of word-level annotations from humans. Moreover the inferred negation rules are fully interpretable.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learning Interpretable Negation Rules via Weak Supervision at Document Level: A Reinforcement Learning Approach</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N19-1038" target="_blank">https://aclanthology.org/N19-1038</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Concept map-based multi-document summarization has recently been proposed as a variant of the traditional summarization task with graph-structured summaries. As shown by previous work the grouping of coreferent concept mentions across documents is a crucial subtask of it. However while the current state-of-the-art method suggested a new grouping method that was shown to improve the summary quality its use of pairwise comparisons leads to polynomial runtime complexity that prohibits the application to large document collections. In this paper we propose two alternative grouping techniques based on locality sensitive hashing approximate nearest neighbor search and a fast clustering algorithm. They exhibit linear and log-linear runtime complexity making them much more scalable. We report experimental results that confirm the improved runtime behavior while also showing that the quality of the summary concept maps remains comparable.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Fast Concept Mention Grouping for Concept Map-based Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N19-1074" target="_blank">https://aclanthology.org/N19-1074</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>A standard word embedding algorithm such as word2vec and glove makes a strong assumption that words are likely to be semantically related only if they co-occur locally within a window of fixed size. However this strong assumption may not capture the semantic association between words that co-occur frequently but non-locally within documents. In this paper we propose a graph-based word embedding method named `word-node2vec&#39;. By relaxing the strong constraint of locality our method is able to capture both the local and non-local co-occurrences. Word-node2vec constructs a graph where every node represents a word and an edge between two nodes represents a combination of both local (e.g. word2vec) and document-level co-occurrences. Our experiments show that word-node2vec outperforms word2vec and glove on a range of different tasks such as predicting word-pair similarity word analogy and concept categorization.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Word-Node2Vec: Improving Word Embedding with Document-Level Non-Local Word Co-occurrences</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N19-1109" target="_blank">https://aclanthology.org/N19-1109</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we conceptualize single-document extractive summarization as a tree induction problem. In contrast to previous approaches which have relied on linguistically motivated document representations to generate summaries our model induces a multi-root dependency tree while predicting the output summary. Each root node in the tree is a summary sentence and the subtrees attached to it are sentences whose content relates to or explains the summary sentence. We design a new iterative refinement algorithm: it induces the trees through repeatedly refining the structures predicted by previous iterations. We demonstrate experimentally on two benchmark datasets that our summarizer performs competitively against state-of-the-art methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Single Document Summarization as Tree Induction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N19-1173" target="_blank">https://aclanthology.org/N19-1173</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We aim to comprehensively identify all the event causal relations in a document both within a sentence and across sentences which is important for reconstructing pivotal event structures. The challenges we identified are two: 1) event causal relations are sparse among all possible event pairs in a document in addition 2) few causal relations are explicitly stated. Both challenges are especially true for identifying causal relations between events across sentences. To address these challenges we model rich aspects of document-level causal structures for achieving comprehensive causal relation identification. The causal structures include heavy involvements of document-level main events in causal relations as well as several types of fine-grained constraints that capture implications from certain sentential syntactic relations and discourse relations as well as interactions between event causal relations and event coreference relations. Our experimental results show that modeling the global and fine-grained aspects of causal structures using Integer Linear Programming (ILP) greatly improves the performance of causal relation identification especially in identifying cross-sentence causal relations.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Modeling Document-level Causal Structures for Event Causal Relation Identification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N19-1179" target="_blank">https://aclanthology.org/N19-1179</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Learning to hash via generative model has become a powerful paradigm for fast similarity search in documents retrieval. To get binary representation (i.e. hash codes) the discrete distribution prior (i.e. Bernoulli Distribution) is applied to train the variational autoencoder (VAE). However the discrete stochastic layer is usually incompatible with the backpropagation in the training stage and thus causes a gradient flow problem because of non-differentiable operators. The reparameterization trick of sampling from a discrete distribution usually inc non-differentiable operators. In this paper we propose a method Doc2hash that solves the gradient flow problem of the discrete stochastic layer by using continuous relaxation on priors and trains the generative model in an end-to-end manner to generate hash codes. In qualitative and quantitative experiments we show the proposed model outperforms other state-of-art methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Doc2hash: Learning Discrete Latent variables for Documents Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N19-1232" target="_blank">https://aclanthology.org/N19-1232</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Most research in reading comprehension has focused on answering questions based on individual documents or even single paragraphs. We introduce a neural model which integrates and reasons relying on information spread within documents and across multiple documents. We frame it as an inference problem on a graph. Mentions of entities are nodes of this graph while edges encode relations between different mentions (e.g. within- and cross-document co-reference). Graph convolutional networks (GCNs) are applied to these graphs and trained to perform multi-step reasoning. Our Entity-GCN method is scalable and compact and it achieves state-of-the-art results on a multi-document question answering dataset WikiHop (Welbl et al. 2018).</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Question Answering by Reasoning Across Documents with Graph Convolutional Networks</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N19-1240" target="_blank">https://aclanthology.org/N19-1240</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Unsupervised document representation learning is an important task providing pre-trained features for NLP applications. Unlike most previous work which learn the embedding based on self-prediction of the surface of text we explicitly exploit the inter-document information and directly model the relations of documents in embedding space with a discriminative network and a novel objective. Extensive experiments on both small and large public datasets show the competitiveness of the proposed method. In evaluations on standard document classification our model has errors that are 5 to 13% lower than state-of-the-art unsupervised embedding models. The reduction in error is even more pronounced in scarce label setting.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Self-Discriminative Learning for Unsupervised Document Embedding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N19-1255" target="_blank">https://aclanthology.org/N19-1255</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level event factuality identification is an important subtask in event factuality and is crucial for discourse understanding in Natural Language Processing (NLP). Previous studies mainly suffer from the scarcity of suitable corpus and effective methods. To solve these two issues we first construct a corpus annotated with both document- and sentence-level event factuality information on both English and Chinese texts. Then we present an LSTM neural network based on adversarial training with both intra- and inter-sequence attentions to identify document-level event factuality. Experimental results show that our neural network model can outperform various baselines on the constructed corpus.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Event Factuality Identification via Adversarial Neural Network</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N19-1287" target="_blank">https://aclanthology.org/N19-1287</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The uncertainty measurement of classifiers&#39; predictions is especially important in applications such as medical diagnoses that need to ensure limited human resources can focus on the most uncertain predictions returned by machine learning models. However few existing uncertainty models attempt to improve overall prediction accuracy where human resources are involved in the text classification task. In this paper we propose a novel neural-network-based model that applies a new dropout-entropy method for uncertainty measurement. We also design a metric learning method on feature representations which can boost the performance of dropout-based uncertainty methods with smaller prediction variance in accurate prediction trials. Extensive experiments on real-world data sets demonstrate that our method can achieve a considerable improvement in overall prediction accuracy compared to existing approaches. In particular our model improved the accuracy from 0.78 to 0.92 when 30% of the most uncertain predictions were handed over to human experts in ``20NewsGroup&#39;&#39; data.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Mitigating Uncertainty in Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N19-1316" target="_blank">https://aclanthology.org/N19-1316</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Most information extraction methods focus on binary relations expressed within single sentences. In high-value domains however n-ary relations are of great demand (e.g. drug-gene-mutation interactions in precision oncology). Such relations often involve entity mentions that are far apart in the document yet existing work on cross-sentence relation extraction is generally confined to small text spans (e.g. three consecutive sentences) which severely limits recall. In this paper we propose a novel multiscale neural architecture for document-level n-ary relation extraction. Our system combines representations learned over various text spans throughout the document and across the subrelation hierarchy. Widening the system&#39;s purview to the entire document maximizes potential recall. Moreover by integrating weak signals across the document multiscale modeling increases precision even in the presence of noisy labels from distant supervision. Experiments on biomedical machine reading show that our approach substantially outperforms previous n-ary relation extraction methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level N-ary Relation Extraction with Multiscale Representation Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N19-1370" target="_blank">https://aclanthology.org/N19-1370</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a new neural model for text summarization that first extracts sentences from a document and then compresses them. The pro-posed model offers a balance that sidesteps thedifficulties in abstractive methods while gener-ating more concise summaries than extractivemethods. In addition our model dynamically determines the length of the output summary based on the gold summaries it observes during training and does not require length constraints typical to extractive summarization. The model achieves state-of-the-art results on the CNN/DailyMail and Newsroom datasets improving over current extractive and abstractive methods. Human evaluations demonstratethat our model generates concise and informa-tive summaries. We also make available a new dataset of oracle compressive summaries derived automatically from the CNN/DailyMailreference summaries.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Jointly Extracting and Compressing Documents with Summary State Representations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N19-1397" target="_blank">https://aclanthology.org/N19-1397</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Neural network models for many NLP tasks have grown increasingly complex in recent years making training and deployment more difficult. A number of recent papers have questioned the necessity of such architectures and found that well-executed simpler models are quite effective. We show that this is also the case for document classification: in a large-scale reproducibility study of several recent neural models we find that a simple BiLSTM architecture with appropriate regularization yields accuracy and F1 that are either competitive or exceed the state of the art on four standard benchmark datasets. Surprisingly our simple model is able to achieve these results without attention mechanisms. While these regularization techniques borrowed from language modeling are not novel to our knowledge we are the first to apply them in this context. Our work provides an open-source platform and the foundation for future work in document classification.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Rethinking Complex Neural Network Architectures for Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N19-1408" target="_blank">https://aclanthology.org/N19-1408</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We propose BeamSeg a joint model for segmentation and topic identification of documents from the same domain. The model assumes that lexical cohesion can be observed across documents meaning that segments describing the same topic use a similar lexical distribution over the vocabulary. The model implements lexical cohesion in an unsupervised Bayesian setting by drawing from the same language model segments with the same topic. Contrary to previous approaches we assume that language models are not independent since the vocabulary changes in consecutive segments are expected to be smooth and not abrupt. We achieve this by using a dynamic Dirichlet prior that takes into account data contributions from other topics. BeamSeg also models segment length properties of documents based on modality (textbooks slides textitetc.). The evaluation is carried out in three datasets. In two of them improvements of up to 4.8% and 7.3% are obtained in the segmentation and topic identifications tasks indicating that both tasks should be jointly modeled.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>BeamSeg: A Joint Model for Multi-Document Segmentation and Topic Identification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/K19-1054" target="_blank">https://aclanthology.org/K19-1054</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document embeddings created with methods ranging from simple heuristics to statistical and deep models are widely applicable. Bag-of-vectors models for documents include the mean and quadratic approaches (Torki 2018). We present evidence that quadratic statistics alone without the mean information can offer superior accuracy fast document comparison and compact document representations. In matching news articles to their comment threads low-rank representations of only 3-4 times the size of the mean vector give most accurate matching and in standard sentence comparison tasks results are state of the art despite faster computation. Similarity measures are discussed and the Frobenius product implicit in the proposed method is contrasted to Wasserstein or Bures metric from the transportation theory. We also shortly demonstrate matching of unordered word lists to documents to measure topicality or sentiment of documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Low-Rank Approximations of Second-Order Document Representations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/K19-1059" target="_blank">https://aclanthology.org/K19-1059</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level context has received lots of attention for compensating neural machine translation (NMT) of isolated sentences. However recent advances in document-level NMT focus on sophisticated integration of the context explaining its improvement with only a few selected examples or targeted test sets. We extensively quantify the causes of improvements by a document-level model in general test sets clarifying the limit of the usefulness of document-level context in NMT. We show that most of the improvements are not interpretable as utilizing the context. We also show that a minimal encoding is sufficient for the context modeling and very long context is not helpful for NMT.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>When and Why is Document-level Context Useful in Neural Machine Translation?</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-6503" target="_blank">https://aclanthology.org/D19-6503</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we investigate how different aspects of discourse context affect the performance of recent neural MT systems. We describe two popular datasets covering news and movie subtitles and we provide a thorough analysis of the distribution of various document-level features in their domains. Furthermore we train a set of context-aware MT models on both datasets and propose a comparative evaluation scheme that contrasts coherent context with artificially scrambled documents and absent context arguing that the impact of discourse-aware MT models will become visible in this way. Our results show that the models are indeed affected by the manipulation of the test data providing a different view on document-level translation quality than absolute sentence-level scores.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Analysing concatenation approaches to document-level NMT in two different domains</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-6506" target="_blank">https://aclanthology.org/D19-6506</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper discusses a cross-document coreference annotation schema that was developed to further automatic extraction of timelines in the clinical domain. Lexical senses and coreference choices are determined largely by context but cross-document work requires reasoning across contexts that are not necessarily coherent. We found that an annotation approach that relies less on context-guided annotator intuitions and more on schematic rules was most effective in creating meaningful and consistent cross-document relations.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-document coreference: An approach to capturing coreference without context</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-6201" target="_blank">https://aclanthology.org/D19-6201</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents our system details and results of participation in the RDoC Tasks of BioNLP-OST 2019. Research Domain Criteria (RDoC) construct is a multi-dimensional and broad framework to describe mental health disorders by combining knowledge from genomics to behaviour. Non-availability of RDoC labelled dataset and tedious labelling process hinders the use of RDoC framework to reach its full potential in Biomedical research community and Healthcare industry. Therefore Task-1 aims at retrieval and ranking of PubMed abstracts relevant to a given RDoC construct and Task-2 aims at extraction of the most relevant sentence from a given PubMed abstract. We investigate (1) attention based supervised neural topic model and SVM for retrieval and ranking of PubMed abstracts and further utilize BM25 and other relevance measures for re-ranking (2) supervised and unsupervised sentence ranking models utilizing multi-view representations comprising of query-aware attention-based sentence representation (QAR) bag-of-words (BoW) and TF-IDF. Our best systems achieved 1st rank and scored 0.86 mAP and 0.58 macro average accuracy in Task-1 and Task-2 respectively.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>BioNLP-OST 2019 RDoC Tasks: Multi-grain Neural Relevance Ranking Using Topics and Attention Based Query-Document-Sentence Interactions</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-5730" target="_blank">https://aclanthology.org/D19-5730</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Ambiguous user queries in search engines result in the retrieval of documents that often span multiple topics. One potential solution is for the search engine to generate multiple refined queries each of which relates to a subset of the documents spanning the same topic. A preliminary step towards this goal is to generate a question that captures common concepts of multiple documents. We propose a new task of generating common question from multiple documents and present simple variant of an existing multi-source encoder-decoder framework called the Multi-Source Question Generator (MSQG). We first train an RNN-based single encoder-decoder generator from (single document question) pairs. At test time given multiple documents the Distribute step of our MSQG model predicts target word distributions for each document using the trained model. The Aggregate step aggregates these distributions to generate a common question. This simple yet effective strategy significantly outperforms several existing baseline models applied to the new task when evaluated using automated metrics and human judgments on the MS-MARCO-QA dataset.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Generating a Common Question from Multiple Documents using Multi-source Encoder-Decoder Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-5604" target="_blank">https://aclanthology.org/D19-5604</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We propose a system that improves performance on single document summarization task using the CNN/DailyMail and Newsroom datasets. It follows the popular encoder-decoder paradigm but with an extra focus on the encoder. The intuition is that the probability of correctly decoding an information significantly lies in the pattern and correctness of the encoder. Hence we introduce encode --encode -- decode. A framework that encodes the source text first with a transformer then a sequence-to-sequence (seq2seq) model. We find that the transformer and seq2seq model complement themselves adequately making for a richer encoded vector representation. We also find that paying more attention to the vocabulary of target words during abstraction improves performance. We experiment our hypothesis and framework on the task of extractive and abstractive single document summarization and evaluate using the standard CNN/DailyMail dataset and the recently released Newsroom dataset.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Transformer-based Model for Single Documents Neural Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-5607" target="_blank">https://aclanthology.org/D19-5607</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The Insertion Transformer is well suited for long form text generation due to its parallel generation capabilities requiring O(log_2 n) generation steps to generate n tokens. However modeling long sequences is difficult as there is more ambiguity captured in the attention mechanism. This work proposes the Big Bidirectional Insertion Representations for Documents (Big BIRD) an insertion-based model for document-level translation tasks. We scale up the insertion-based models to long form documents. Our key contribution is introducing sentence alignment via sentence-positional embeddings between the source and target document. We show an improvement of +4.3 BLEU on the WMT&#39;19 English-textgreaterGerman document-level translation task compared with the Insertion Transformer baseline.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Big Bidirectional Insertion Representations for Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-5620" target="_blank">https://aclanthology.org/D19-5620</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We describe the work of Monash University for the shared task of Rotowire document translation organised by the 3rd Workshop on Neural Generation and Translation (WNGT 2019). We submitted systems for both directions of the English-German language pair. Our main focus is on employing an established document-level neural machine translation model for this task. We achieve a BLEU score of 39.83 (41.46 BLEU per WNGT evaluation) for En-De and 45.06 (47.39 BLEU per WNGT evaluation) for De-En translation directions on the Rotowire test set. All experiments conducted in the process are also described.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Monash University&#39;s Submissions to the WNGT 2019 Document Translation Task</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-5628" target="_blank">https://aclanthology.org/D19-5628</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The University of Edinburgh participated in all six tracks: NLG MT and MT+NLG with both English and German as targeted languages. For the NLG track we submitted a multilingual system based on the Content Selection and Planning model of Puduppully et al (2019). For the MT track we submitted Transformer-based Neural Machine Translation models where out-of-domain parallel data was augmented with in-domain data extracted from monolingual corpora. Our MT+NLG systems disregard the structured input data and instead rely exclusively on the source summaries.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>University of Edinburgh&#39;s submission to the Document-level Generation and Translation Shared Task</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-5630" target="_blank">https://aclanthology.org/D19-5630</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recently neural models led to significant improvements in both machine translation (MT) and natural language generation tasks (NLG). However generation of long descriptive summaries conditioned on structured data remains an open challenge. Likewise MT that goes beyond sentence-level context is still an open issue (e.g. document-level MT or MT with metadata). To address these challenges we propose to leverage data from both tasks and do transfer learning between MT NLG and MT with source-side metadata (MT+NLG). First we train document-based MT systems with large amounts of parallel data. Then we adapt these models to pure NLG and MT+NLG tasks by fine-tuning with smaller amounts of domain-specific data. This end-to-end NLG approach without data selection and planning outperforms the previous state of the art on the Rotowire NLG task. We participated to the ``Document Generation and Translation&#39;&#39; task at WNGT 2019 and ranked first in all tracks.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Naver Labs Europe&#39;s Systems for the Document-Level Generation and Translation Task at WNGT 2019</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-5631" target="_blank">https://aclanthology.org/D19-5631</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we report our system submissions to all 6 tracks of the WNGT 2019 shared task on Document-Level Generation and Translation. The objective is to generate a textual document from either structured data: generation task or a document in a different language: translation task. For the translation task we focused on adapting a large scale system trained on WMT data by fine tuning it on the RotoWire data. For the generation task we participated with two systems based on a selection and planning model followed by (a) a simple language model generation and (b) a GPT-2 pre-trained language model approach. The selection and planning module chooses a subset of table records in order and the language models produce text given such a subset.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Selecting Planning and Rewriting: A Modular Approach for Data-to-Document Generation and Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-5633" target="_blank">https://aclanthology.org/D19-5633</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In the context of document quality assessment previous work has mainly focused on predicting the quality of a document relative to a putative gold standard without paying attention to the subjectivity of this task. To imitate people&#39;s disagreement over inherently subjective tasks such as rating the quality of a Wikipedia article a document quality assessment system should provide not only a prediction of the article quality but also the uncertainty over its predictions. This motivates us to measure the uncertainty in document quality predictions in addition to making the label prediction. Experimental results show that both Gaussian processes (GPs) and random forests (RFs) can yield competitive results in predicting the quality of Wikipedia articles while providing an estimate of uncertainty when there is inconsistency in the quality labels from the Wikipedia contributors. We additionally evaluate our methods in the context of a semi-automated document quality class assignment decision-making process where there is asymmetric risk associated with overestimates and underestimates of document quality. Our experiments suggest that GPs provide more reliable estimates in this context.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Modelling Uncertainty in Collaborative Document Quality Assessment</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-5525" target="_blank">https://aclanthology.org/D19-5525</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Linking facts across documents is a challenging task as the language used to express the same information in a sentence can vary significantly which complicates the task of multi-document summarization. Consequently existing approaches heavily rely on hand-crafted features which are domain-dependent and hard to craft or additional annotated data which is costly to gather. To overcome these limitations we present a novel method which makes use of two types of sentence embeddings: universal embeddings which are trained on a large unrelated corpus and domain-specific embeddings which are learned during training. To this end we develop SemSentSum a fully data-driven model able to leverage both types of sentence embeddings by building a sentence semantic relation graph. SemSentSum achieves competitive results on two types of summary consisting of 665 bytes and 100 words. Unlike other state-of-the-art models neither hand-crafted features nor additional annotated data are necessary and the method is easily adaptable for other tasks. To our knowledge we are the first to use multiple sentence embeddings for the task of multi-document summarization.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-5404" target="_blank">https://aclanthology.org/D19-5404</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>User-generated reviews of products or services provide valuable information to customers. However it is often impossible to read each of the potentially thousands of reviews: it would therefore save valuable time to provide short summaries of their contents. We address opinion summarization a multi-document summarization task with an unsupervised abstractive summarization neural system. Our system is based on (i) a language model that is meant to encode reviews to a vector space and to generate fluent sentences from the same vector space (ii) a clustering step that groups together reviews about the same aspects and allows the system to generate summary sentences focused on these aspects. Our experiments on the Oposum dataset empirically show the importance of the clustering step.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Aspect-Based Multi-Document Abstractive Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-5405" target="_blank">https://aclanthology.org/D19-5405</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We suggest a new idea of Editorial Network -- a mixed extractive-abstractive summarization approach which is applied as a post-processing step over a given sequence of extracted sentences. We further suggest an effective way for training the ``editor&#39;&#39; based on a novel soft-labeling approach. Using the CNN/DailyMail dataset we demonstrate the effectiveness of our approach compared to state-of-the-art extractive-only or abstractive-only baselines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Editorial Network for Enhanced Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-5407" target="_blank">https://aclanthology.org/D19-5407</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Emerged as one of the best performing techniques for extractive summarization determinantal point processes select a most probable set of summary sentences according to a probabilistic measure defined by respectively modeling sentence prominence and pairwise repulsion. Traditionally both aspects are modelled using shallow and linguistically informed features but the rise of deep contextualized representations raises an interesting question. Whether and to what extent could contextualized sentence representations be used to improve the DPP framework? Our findings suggest that despite the success of deep semantic representations it remains necessary to combine them with surface indicators for effective identification of summary-worthy sentences.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Document Summarization with Determinantal Point Processes and Contextualized Representations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-5412" target="_blank">https://aclanthology.org/D19-5412</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recent advances in reading comprehension have resulted in models that surpass human performance when the answer is contained in a single continuous passage of text. However complex Question Answering (QA) typically requires multi-hop reasoning - i.e. the integration of supporting facts from different sources to infer the correct answer. This paper proposes Document Graph Network (DGN) a message passing architecture for the identification of supporting facts over a graph-structured representation of text. The evaluation on HotpotQA shows that DGN obtains competitive results when compared to a reading comprehension baseline operating on raw text confirming the relevance of structured representations for supporting multi-hop reasoning.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Identifying Supporting Facts for Multi-hop Question Answering with Document Graph Networks</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-5306" target="_blank">https://aclanthology.org/D19-5306</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In order to automate banking processes (e.g. payments money transfers foreign trade) we need to extract banking transactions from different types of mediums such as faxes e-mails and scanners. Banking orders may be considered as complex documents since they contain quite complex relations compared to traditional datasets used in relation extraction research. In this paper we present our method to extract intersentential nested and complex relations from banking orders and introduce a relation extraction method based on maximal clique factorization technique. We demonstrate 11% error reduction over previous methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extracting Complex Relations from Banking Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-5101" target="_blank">https://aclanthology.org/D19-5101</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present Birch a system that applies BERT to document retrieval via integration with the open-source Anserini information retrieval toolkit to demonstrate end-to-end search over large document collections. Birch implements simple ranking models that achieve state-of-the-art effectiveness on standard TREC newswire and social media test collections. This demonstration focuses on technical challenges in the integration of NLP and IR capabilities along with the design rationale behind our approach to tightly-coupled integration between Python (to support neural networks) and the Java Virtual Machine (to support document retrieval using the open-source Lucene search library). We demonstrate integration of Birch with an existing search interface as well as interactive notebooks that highlight its capabilities in an easy-to-understand manner.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Applying BERT to Document Retrieval with Birch</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-3004" target="_blank">https://aclanthology.org/D19-3004</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Legal Tech is developed to help people with legal services and solve legal problems via machines. To achieve this one of the key requirements for machines is to utilize legal knowledge and comprehend legal context. This can be fulfilled by natural language processing (NLP) techniques for instance text representation text categorization question answering (QA) and natural language inference etc. To this end we introduce a freely available Chinese Legal Tech system (IFlyLegal) that benefits from multiple NLP tasks. It is an integrated system that performs legal consulting multi-way law searching and legal document analysis by exploiting techniques such as deep contextual representations and various attention mechanisms. To our knowledge IFlyLegal is the first Chinese legal system that employs up-to-date NLP techniques and caters for needs of different user groups such as lawyers judges procurators and clients. Since Jan 2019 we have gathered 2349 users and 28238 page views (till June 23 2019).</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>IFlyLegal: A Chinese Legal System for Consultation Law Searching and Document Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-3017" target="_blank">https://aclanthology.org/D19-3017</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a novel system providing summaries for Computer Science publications. Through a qualitative user study we identified the most valuable scenarios for discovery exploration and understanding of scientific documents. Based on these findings we built a system that retrieves and summarizes scientific documents for a given information need either in form of a free-text query or by choosing categorized values such as scientific tasks datasets and more. Our system ingested 270000 papers and its summarization module aims to generate concise yet detailed summaries. We validated our approach with human experts.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Summarization System for Scientific Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-3036" target="_blank">https://aclanthology.org/D19-3036</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications such as finance legislation health etc. where event arguments always scatter across different sentences and even multiple such event mentions frequently co-exist in the same document. To address these challenges we propose a novel end-to-end model Doc2EDAG which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1032" target="_blank">https://aclanthology.org/D19-1032</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Multi-label text classification (MLTC) aims to tag most relevant labels for the given document. In this paper we propose a Label-Specific Attention Network (LSAN) to learn a label-specific document representation. LSAN takes advantage of label semantic information to determine the semantic connection between labels and document for constructing label-specific document representation. Meanwhile the self-attention mechanism is adopted to identify the label-specific document representation from document content information. In order to seamlessly integrate the above two parts an adaptive fusion strategy is proposed which can effectively output the comprehensive label-specific document representation to build multi-label text classifier. Extensive experimental results demonstrate that LSAN consistently outperforms the state-of-the-art methods on four different datasets especially on the prediction of low-frequency labels. The code and hyper-parameter settings are released to facilitate other researchers.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Label-Specific Document Representation for Multi-Label Text Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1044" target="_blank">https://aclanthology.org/D19-1044</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>ROUGE is widely used to automatically evaluate summarization systems. However ROUGE measures semantic overlap between a system summary and a human reference on word-string level much at odds with the contemporary treatment of semantic meaning. Here we present a suite of experiments on using distributed representations for evaluating summarizers both in reference-based and in reference-free setting. Our experimental results show that the max value over each dimension of the summary ELMo word embeddings is a good representation that results in high correlation with human ratings. Averaging the cosine similarity of all encoders we tested yields high correlation with manual scores in reference-free setting. The distributed representations outperform ROUGE in recent corpora for abstractive news summarization but are less good on test data used in past evaluations.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The Feasibility of Embedding Based Automatic Evaluation for Single Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1116" target="_blank">https://aclanthology.org/D19-1116</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Attention plays a key role in the improvement of sequence-to-sequence-based document summarization models. To obtain a powerful attention helping with reproducing the most salient information and avoiding repetitions we augment the vanilla attention model from both local and global aspects. We propose attention refinement unit paired with local variance loss to impose supervision on the attention model at each decoding step and we also propose a global variance loss to optimize the attention distributions of all decoding steps from the global perspective. The performances on CNN/Daily Mail dataset verify the effectiveness of our methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Attention Optimization for Abstractive Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1117" target="_blank">https://aclanthology.org/D19-1117</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Context modeling is essential to generate coherent and consistent translation for Document-level Neural Machine Translations. The widely used method for document-level translation usually compresses the context information into a representation via hierarchical attention networks. However this method neither considers the relationship between context words nor distinguishes the roles of context words. To address this problem we propose a query-guided capsule networks to cluster context information into different perspectives from which the target translation may concern. Experiment results show that our method can significantly outperform strong baselines on multiple data sets of different domains.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Enhancing Context Modeling with a Query-Guided Capsule Network for Document-level Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1164" target="_blank">https://aclanthology.org/D19-1164</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level machine translation (MT) remains challenging due to the difficulty in efficiently using document context for translation. In this paper we propose a hierarchical model to learn the global context for document-level neural machine translation (NMT). This is done through a sentence encoder to capture intra-sentence dependencies and a document encoder to model document-level inter-sentence consistency and coherence. With this hierarchical architecture we feedback the extracted global document context to each word in a top-down fashion to distinguish different translations of a word according to its specific surrounding context. In addition since large-scale in-domain document-level parallel corpora are usually unavailable we use a two-step training strategy to take advantage of a large-scale corpus with out-of-domain parallel sentence pairs and a small-scale corpus with in-domain parallel document pairs to achieve the domain adaptability. Experimental results on several benchmark corpora show that our proposed model can significantly improve document-level translation performance over several strong NMT baselines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1168" target="_blank">https://aclanthology.org/D19-1168</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Images and text co-occur constantly on the web but explicit links between images and sentences (or other intra-document textual units) are often not present. We present algorithms that discover image-sentence relationships without relying on explicit multimodal annotation in training. We experiment on seven datasets of varying difficulty ranging from documents consisting of groups of images captioned post hoc by crowdworkers to naturally-occurring user-generated multimodal documents. We find that a structured training objective based on identifying whether collections of images and sentences co-occur in documents can suffice to predict links between specific sentences and specific images within the same document at test time.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Discovery of Multimodal Links in Multi-image Multi-sentence Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1210" target="_blank">https://aclanthology.org/D19-1210</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we propose a novel neural single-document extractive summarization model for long documents incorporating both the global context of the whole document and the local context within the current topic. We evaluate the model on two datasets of scientific papers Pubmed and arXiv where it outperforms previous work both extractive and abstractive models on ROUGE-1 ROUGE-2 and METEOR scores. We also show that consistently with our goal the benefits of our method become stronger as we apply it to longer documents. Rather surprisingly an ablation study indicates that the benefits of our model seem to come exclusively from modeling the local context even for the longest documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extractive Summarization of Long Documents by Combining Global and Local Context</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1298" target="_blank">https://aclanthology.org/D19-1298</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In multi-document summarization a set of documents to be summarized is assumed to be on the same topic known as the underlying topic in this paper. That is the underlying topic can be collectively represented by all the documents in the set. Meanwhile different documents may cover various different subtopics and the same subtopic can be across several documents. Inspired by topic model the underlying topic of a document set can also be viewed as a collection of different subtopics of different importance. In this paper we propose a summarization model called STDS. The model generates the underlying topic representation from both document view and subtopic view in parallel. The learning objective is to minimize the distance between the representations learned from the two views. The contextual information is encoded through a hierarchical RNN architecture. Sentence salience is estimated in a hierarchical way with subtopic salience and relative sentence salience by considering the contextual information. Top ranked sentences are then extracted as a summary. Note that the notion of subtopic enables us to bring in additional information (e.g. comments to news articles) that is helpful for document summarization. Experimental results show that the proposed solution outperforms state-of-the-art methods on benchmark datasets.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Subtopic-driven Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1311" target="_blank">https://aclanthology.org/D19-1311</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The automated generation of information indicating the characteristics of articles such as headlines key phrases summaries and categories helps writers to alleviate their workload. Previous research has tackled these tasks using neural abstractive summarization and classification methods. However the outputs may be inconsistent if they are generated individually. The purpose of our study is to generate multiple outputs consistently. We introduce a multi-task learning model with a shared encoder and multiple decoders for each task. We propose a novel loss function called hierarchical consistency loss to maintain consistency among the attention weights of the decoders. To evaluate the consistency we employ a human evaluation. The results show that our model generates more consistent headlines key phrases and categories. In addition our model outperforms the baseline model on the ROUGE scores and generates more adequate and fluent headlines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Keeping Consistency of Sentence Generation and Document Classification with Multi-Task Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1315" target="_blank">https://aclanthology.org/D19-1315</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present 1) a work in progress method to visually segment key regions of scientific articles using an object detection technique augmented with contextual features and 2) a novel dataset of region-labeled articles. A continuing challenge in scientific literature mining is the difficulty of consistently extracting high-quality text from formatted PDFs. To address this we adapt the object-detection technique Faster R-CNN for document layout detection incorporating contextual information that leverages the inherently localized nature of article contents to improve the region detection performance. Due to the limited availability of high-quality region-labels for scientific articles we also contribute a novel dataset of region annotations the first version of which covers 9 region classes and 822 article pages. Initial experimental results demonstrate a 23.9% absolute improvement in mean average precision over the baseline model by incorporating contextual features and a processing speed 14x faster than a text-based technique. Ongoing work on further improvements is also discussed.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Visual Detection with Context for Document Layout Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1348" target="_blank">https://aclanthology.org/D19-1348</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper applies BERT to ad hoc document retrieval on news articles which requires addressing two challenges: relevance judgments in existing test collections are typically provided only at the document level and documents often exceed the length that BERT was designed to handle. Our solution is to aggregate sentence-level evidence to rank documents. Furthermore we are able to leverage passage-level relevance judgments fortuitously available in other domains to fine-tune BERT models that are able to capture cross-domain notions of relevance and can be directly used for ranking news articles. Our simple neural ranking models achieve state-of-the-art effectiveness on three standard test collections.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1352" target="_blank">https://aclanthology.org/D19-1352</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We consider a document classification problem where document labels are absent but only relevant keywords of a target class and unlabeled documents are given. Although heuristic methods based on pseudo-labeling have been considered theoretical understanding of this problem has still been limited. Moreover previous methods cannot easily incorporate well-developed techniques in supervised text classification. In this paper we propose a theoretically guaranteed learning framework that is simple to implement and has flexible choices of models e.g. linear models or neural networks. We demonstrate how to optimize the area under the receiver operating characteristic curve (AUC) effectively and also discuss how to adjust it to optimize other well-known evaluation metrics such as the accuracy and F1-measure. Finally we show the effectiveness of our framework using benchmark datasets.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learning Only from Relevant Keywords and Unlabeled Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1411" target="_blank">https://aclanthology.org/D19-1411</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Query-based open-domain NLP tasks require information synthesis from long and diverse web results. Current approaches extractively select portions of web text as input to Sequence-to-Sequence models using methods such as TF-IDF ranking. We propose constructing a local graph structured knowledge base for each query which compresses the web search information and reduces redundancy. We show that by linearizing the graph into a structured input sequence models can encode the graph representations within a standard Sequence-to-Sequence setting. For two generative tasks with very long text input long-form question answering and multi-document summarization feeding graph representations as input can achieve better performance than using retrieved text portions.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1428" target="_blank">https://aclanthology.org/D19-1428</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Neural machine translation which achieves near human-level performance in some languages strongly relies on the large amounts of parallel sentences which hinders its applicability to low-resource language pairs. Recent works explore the possibility of unsupervised machine translation with monolingual data only leading to much lower accuracy compared with the supervised one. Observing that weakly paired bilingual documents are much easier to collect than bilingual sentences e.g. from Wikipedia news websites or books in this paper we investigate training translation models with weakly paired bilingual documents. Our approach contains two components. 1) We provide a simple approach to mine implicitly bilingual sentence pairs from document pairs which can then be used as supervised training signals. 2) We leverage the topic consistency of two weakly paired documents and learn the sentence translation model by constraining the word distribution-level alignments. We evaluate our method on weakly paired documents from Wikipedia on six tasks the widely used WMT16 GermanleftrightarrowEnglish WMT13 SpanishleftrightarrowEnglish and WMT16 RomanianleftrightarrowEnglish translation tasks. We obtain 24.1/30.3 28.1/27.6 and 30.1/27.6 BLEU points separately outperforming previous results by more than 5 BLEU points in each direction and reducing the gap between unsupervised translation and supervised translation up to 50%.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Machine Translation With Weakly Paired Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1446" target="_blank">https://aclanthology.org/D19-1446</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Computing author intent from multimodal data like Instagram posts requires modeling a complex relationship between text and image. For example a caption might evoke an ironic contrast with the image so neither caption nor image is a mere transcript of the other. Instead they combine---via what has been called meaning multiplication (Bateman et al.)- to create a new meaning that has a more complex relation to the literal meanings of text and image. Here we introduce a multimodal dataset of 1299 Instagram posts labeled for three orthogonal taxonomies: the authorial intent behind the image-caption pair the contextual relationship between the literal meanings of the image and caption and the semiotic relationship between the signified meanings of the image and caption. We build a baseline deep multimodal classifier to validate the taxonomy showing that employing both text and image improves intent detection by 9.6 compared to using only the image modality demonstrating the commonality of non-intersective meaning multiplication. The gain with multimodality is greatest when the image and caption diverge semiotically. Our dataset offers a new resource for the study of the rich meanings that result from pairing text and image.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Integrating Text and Image: Determining Multimodal Document Intent in Instagram Posts</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1469" target="_blank">https://aclanthology.org/D19-1469</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Insightful findings in political science often require researchers to analyze documents of a certain subject or type yet these documents are usually contained in large corpora that do not distinguish between pertinent and non-pertinent documents. In contrast we can find corpora that label relevant documents but have limitations (e.g. from a single source or era) preventing their use for political science research. To bridge this gap we present adaptive ensembling an unsupervised domain adaptation framework equipped with a novel text classification model and time-aware training to ensure our methods work well with diachronic corpora. Experiments on an expert-annotated dataset show that our framework outperforms strong benchmarks. Further analysis indicates that our methods are more stable learn better representations and extract cleaner corpora for fine-grained analysis.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Adaptive Ensembling: Unsupervised Domain Adaptation for Political Document Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1478" target="_blank">https://aclanthology.org/D19-1478</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level relation extraction is a complex human process that requires logical inference to extract relationships between named entities in text. Existing approaches use graph-based neural models with words as nodes and edges as relations between them to encode relations across sentences. These models are node-based i.e. they form pair representations based solely on the two target node representations. However entity relations can be better expressed through unique edge representations formed as paths between nodes. We thus propose an edge-oriented graph neural model for document-level relation extraction. The model utilises different types of nodes and edges to create a document-level graph. An inference mechanism on the graph edges enables to learn intra- and inter-sentence relations using multi-instance learning internally. Experiments on two document-level biomedical datasets for chemical-disease and gene-disease associations show the usefulness of the proposed edge-oriented approach.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1498" target="_blank">https://aclanthology.org/D19-1498</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Management of collaborative documents can be difficult given the profusion of edits and comments that multiple authors make during a document&#39;s evolution. Reliably modeling the relationship between edits and comments is a crucial step towards helping the user keep track of a document in flux. A number of authoring tasks such as categorizing and summarizing edits detecting completed to-dos and visually rearranging comments could benefit from such a contribution. Thus in this paper we explore the relationship between comments and edits by defining two novel related tasks: Comment Ranking and Edit Anchoring. We begin by collecting a dataset with more than half a million comment-edit pairs based on Wikipedia revision histories. We then propose a hierarchical multi-layer deep neural-network to model the relationship between edits and comments. Our architecture tackles both Comment Ranking and Edit Anchoring tasks by encoding specific edit actions such as additions and deletions while also accounting for document context. In a number of evaluation settings our experimental results show that our approach outperforms several strong baselines significantly. We are able to achieve a precision@1 of 71.0% and a precision@3 of 94.4% for Comment Ranking while we achieve 74.4% accuracy on Edit Anchoring.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Modeling the Relationship between User Comments and Edits in Document Revision</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1505" target="_blank">https://aclanthology.org/D19-1505</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recently there has been a great interest in the development of small and accurate neural networks that run entirely on devices such as mobile phones smart watches and IoT. This enables user privacy consistent user experience and low latency. Although a wide range of applications have been targeted from wake word detection to short text classification yet there are no on-device networks for long text classification. We propose a novel projection attention neural network PRADO that combines trainable projections with attention and convolutions. We evaluate our approach on multiple large document text classification tasks. Our results show the effectiveness of the trainable projection model in finding semantically similar phrases and reaching high performance while maintaining compact size. Using this approach we train tiny neural networks just 200 Kilobytes in size that improve over prior CNN and LSTM models and achieve near state of the art performance on multiple long document classification tasks. We also apply our model for transfer learning show its robustness and ability to further improve the performance in limited data scenarios.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>PRADO: Projection Attention Networks for Document Classification On-Device</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1506" target="_blank">https://aclanthology.org/D19-1506</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Hashing is promising for large-scale information retrieval tasks thanks to the efficiency of distance evaluation between binary codes. Generative hashing is often used to generate hashing codes in an unsupervised way. However existing generative hashing methods only considered the use of simple priors like Gaussian and Bernoulli priors which limits these methods to further improve their performance. In this paper two mixture-prior generative models are proposed under the objective to produce high-quality hashing codes for documents. Specifically a Gaussian mixture prior is first imposed onto the variational auto-encoder (VAE) followed by a separate step to cast the continuous latent representation of VAE into binary code. To avoid the performance loss caused by the separate casting a model using a Bernoulli mixture prior is further developed in which an end-to-end training is admitted by resorting to the straight-through (ST) discrete gradient estimator. Experimental results on several benchmark datasets demonstrate that the proposed methods especially the one using Bernoulli mixture priors consistently outperform existing ones by a substantial margin.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Hashing with Mixture-Prior Generative Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1526" target="_blank">https://aclanthology.org/D19-1526</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recently neural networks have shown promising results on Document-level Aspect Sentiment Classification (DASC). However these approaches often offer little transparency w.r.t. their inner working mechanisms and lack interpretability. In this paper to simulating the steps of analyzing aspect sentiment in a document by human beings we propose a new Hierarchical Reinforcement Learning (HRL) approach to DASC. This approach incorporates clause selection and word selection strategies to tackle the data noise problem in the task of DASC. First a high-level policy is proposed to select aspect-relevant clauses and discard noisy clauses. Then a low-level policy is proposed to select sentiment-relevant words and discard noisy words inside the selected clauses. Finally a sentiment rating predictor is designed to provide reward signals to guide both clause and word selection. Experimental results demonstrate the impressive effectiveness of the proposed approach to DASC over the state-of-the-art baselines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Human-Like Decision Making: Document-level Aspect Sentiment Classification via Hierarchical Reinforcement Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D19-1560" target="_blank">https://aclanthology.org/D19-1560</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>結合類神經網路及文件概念圖之文件檢索研究(Document Retrieval based on Neural Network and Document Concept Graph)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2019.rocling-1.5" target="_blank">https://aclanthology.org/2019.rocling-1.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We propose in this paper a new hybrid document embedding approach in order to address the problem of document similarities with respect to the technical content. To do so we employ a state-of-the-art graph techniques to first extract the keyphrases (composite keywords) of documents and then use them to score the sentences. Using the ranked sentences we propose two approaches to embed documents and show their performances with respect to two baselines. With domain expert annotations we illustrate that the proposed methods can find more relevant documents and outperform the baselines up to 27% in terms of NDCG.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Terminology-based Text Embedding for Computing Document Similarities on Technical Content</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2019.jeptalnrecital-tia.3" target="_blank">https://aclanthology.org/2019.jeptalnrecital-tia.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Cet article pr&#39;esente la contribution de l&#39;&#39;equipe du Laboratoire de G&#39;enie Informatique et d&#39;Ing&#39;enierie de Production (LGI2P) d&#39;IMT Mines Al`es au D&#39;Efi Fouille de Textes (DEFT) 2019. Il d&#39;etaille en particulier deux approches propos&#39;ees pour les t^aches li&#39;ees `a (1) l&#39;indexation et `a (2) la similarit&#39;e de documents. Ces m&#39;ethodes reposent sur des techniques robustes et &#39;eprouv&#39;ees du domaine de la Recherche d&#39;Information et du Traitement Automatique du Langage Naturel qui ont &#39;et&#39;e adapt&#39;ees `a la nature sp&#39;ecifique du corpus (biom&#39;edical/clinique) et coupl&#39;ees `a des m&#39;ecanismes d&#39;evelopp&#39;es pour r&#39;epondre aux sp&#39;ecificit&#39;es des t^aches trait&#39;ees. Pour la t^ache 1 nous proposons une m&#39;ethode d&#39;indexation par extraction appliqu&#39;ee sur une version normalis&#39;ee du corpus (MAP de 048 `a l&#39;&#39;evaluation) ; les sp&#39;ecificit&#39;es de la phase de normalisation seront en particulier d&#39;etaill&#39;ees. Pour la t^ache 2 au-del`a de la pr&#39;esentation de l&#39;approche propos&#39;ee bas&#39;ee sur l&#39;&#39;evaluation de similarit&#39;es sur des repr&#39;esentations de documents (score de 091 `a l&#39;&#39;evaluation) nous proposons une &#39;etude comparative de l&#39;impact des choix de la distance et de la mani`ere de repr&#39;esenter les textes sur la performance de l&#39;approche.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>D&#39;Efi Fouille de Textes 2019 : indexation par extraction et appariement textuel (DEFT 2019 : extraction-based document indexing and textual document similarity matching )</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2019.jeptalnrecital-deft.4" target="_blank">https://aclanthology.org/2019.jeptalnrecital-deft.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Dans cet article nous pr&#39;esentons nos m&#39;ethodes pour les t^aches d&#39;indexation et d&#39;appariements du D&#39;efi Fouile de Textes (Deft) 2019. Pour la ta^che d&#39;indexation nous avons test&#39;e deux m&#39;ethodes une fond&#39;ee sur l&#39;appariemetn pr&#39;ealable des documents du jeu de tset avec les documents du jeu d&#39;entra^inement et une autre m&#39;ethode fond&#39;ee sur l&#39;annotation terminologique. Ces m&#39;ethodes ont malheureusement offert des r&#39;esultats assez faible. Pour la t^ache d&#39;appariement nous avons d&#39;evellop&#39;e une m&#39;ethode sans apprentissage fond&#39;ee sur des similarit&#39;es de cha^ines de caract`eres ainsi qu&#39;une m&#39;ethode exploitant des r&#39;eseaux siamois. L`a encore les r&#39;esultats ont &#39;et&#39;e plut^ot d&#39;ecevant m^eme si la m&#39;ethode non supervis&#39;ee atteint un score plut^ot honorable pour une m&#39;ethode non-supervis&#39;ee : 62% .</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Indexation et appariements de documents cliniques pour le Deft 2019 (Indexing and pairing texts of the medical domain )</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2019.jeptalnrecital-deft.5" target="_blank">https://aclanthology.org/2019.jeptalnrecital-deft.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Dans ce papier nous pr&#39;esentons la participation de Qwant Research aux t^aches 2 et 3 de l&#39;&#39;edition 2019 du d&#39;efi fouille de textes (DEFT) portant sur l&#39;analyse de documents cliniques r&#39;edig&#39;es en franccais. La t^ache 2 est une t^ache de similarit&#39;e s&#39;emantique qui demande d&#39;apparier cas cliniques et discussions m&#39;edicales. Pour r&#39;esoudre cette t^ache nous proposons une approche reposant sur des mod`eles de langue et &#39;evaluons l&#39;impact de diff&#39;erents pr&#39;e-traitements et de diff&#39;erentes techniques d&#39;appariement sur les r&#39;esultats. Pour la t^ache 3 nous avons d&#39;evelopp&#39;e un syst`eme d&#39;extraction d&#39;information qui produit des r&#39;esultats encourageants en termes de pr&#39;ecision. Nous avons exp&#39;eriment&#39;e deux approches diff&#39;erentes l&#39;une se fondant exclusivement sur l&#39;utilisation de r&#39;eseaux de neurones pour traiter la t^ache l&#39;autre reposant sur l&#39;exploitation des informations linguistiques issues d&#39;une analyse syntaxique.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Qwant Research @DEFT 2019 : appariement de documents et extraction d&#39;informations `a partir de cas cliniques (Document matching and information retrieval using clinical cases)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2019.jeptalnrecital-deft.7" target="_blank">https://aclanthology.org/2019.jeptalnrecital-deft.7</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Dans ce papier nous pr&#39;esentons les m&#39;ethodes que nous avons d&#39;evelopp&#39;ees pour participer aux t^aches 1 et 2 de l&#39;&#39;edition 2019 du d&#39;efi fouille de textes (DEFT 2019). Pour la premi`ere t^ache qui s&#39;int&#39;eresse `a l&#39;indexation de cas cliniques une m&#39;ethode utilisant la pond&#39;eration TF-IDF (term frequency -- inverse document frequency) a &#39;et&#39;e propos&#39;ee. Quant `a la seconde t^ache la m&#39;ethode propos&#39;ee repose sur le mod`ele vectoriel pour apparier des discussions aux cas cliniques correspondants ; pour cela le cosinus est utilis&#39;e comme mesure de similarit&#39;e. L&#39;indexation s&#39;emantique latente (latent semantic indexing -- LSI) est &#39;egalement exp&#39;eriment&#39;ee pour &#39;etendre cette m&#39;ethode. Pour chaque m&#39;ethode diff&#39;erentes configurations ont &#39;et&#39;e test&#39;ees et &#39;evalu&#39;ees sur les donn&#39;ees de test du DEFT 2019.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Indexation et appariement de documents cliniques avec le mod`ele vectoriel (Indexing and matching clinical documents using the vector space model)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2019.jeptalnrecital-deft.9" target="_blank">https://aclanthology.org/2019.jeptalnrecital-deft.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>La compr&#39;ehension automatique de texte est une t^ache faisant partie de la famille des syst`emes de Question/R&#39;eponse o`u les questions ne sont pas `a port&#39;ee g&#39;en&#39;erale mais sont li&#39;ees `a un document particulier. R&#39;ecemment de tr`es grand corpus (SQuAD MS MARCO) contenant des triplets (document question r&#39;eponse) ont &#39;et&#39;e mis `a la disposition de la communaut&#39;e scientifique afin de d&#39;evelopper des m&#39;ethodes supervis&#39;ees `a base de r&#39;eseaux de neurones profonds en obtenant des r&#39;esultats prometteurs. Ces m&#39;ethodes sont cependant tr`es gourmandes en donn&#39;ees d&#39;apprentissage donn&#39;ees qui n&#39;existent pour le moment que pour la langue anglaise. Le but de cette &#39;etude est de permettre le d&#39;eveloppement de telles ressources pour d&#39;autres langues `a moindre co^ut en proposant une m&#39;ethode g&#39;en&#39;erant de mani`ere semi-automatique des questions `a partir d&#39;une analyse s&#39;emantique d&#39;un grand corpus. La collecte de questions naturelle est r&#39;eduite `a un ensemble de validation/test. L&#39;application de cette m&#39;ethode sur le corpus CALOR-Frame a permis de d&#39;evelopper la ressource CALOR-QUEST pr&#39;esent&#39;ee dans cet article.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CALOR-QUEST : un corpus d&#39;entra^inement et d&#39;&#39;evaluation pour la compr&#39;ehension automatique de textes (Machine reading comprehension is a task related to Question-Answering where questions are not generic in scope but are related to a particular document)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2019.jeptalnrecital-court.4" target="_blank">https://aclanthology.org/2019.jeptalnrecital-court.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Cet article 1 propose une approche hybride pour la segmentation de documents bas&#39;ee sur l&#39;agr&#39;egation de diff&#39;erentes solutions. Divers algorithmes de segmentation peuvent ^etre utilis&#39;es dans le syst`eme ce qui permet la combinaison de strat&#39;egies multiples (sp&#39;ecifiques au domaine supervis&#39;ees et nonsupervis&#39;ees). Un ensemble de documents &#39;etiquet&#39;es segment&#39;es au pr&#39;ealable et repr&#39;esentatif du domaine cibl&#39;e doit ^etre fourni pour ^etre utilis&#39;e comme ensemble d&#39;entra^inement pour l&#39;apprentissage des m&#39;ethodes supervis&#39;ees et aussi comme ensemble de test pour l&#39;&#39;evaluation de la performance de chaque m&#39;ethode ce qui d&#39;eterminera leur poids lors de la phase d&#39;agr&#39;egation. L&#39;approche propos&#39;ee pr&#39;esente de bonnes performances dans un sc&#39;enario exp&#39;erimental issu d&#39;un corpus extrait du domaine juridique.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Une approche hybride pour la segmentation automatique de documents juridiques (A hybrid approach for automatic text segmentation)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2019.jeptalnrecital-court.31" target="_blank">https://aclanthology.org/2019.jeptalnrecital-court.31</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We describe our four NMT systems submitted to the IWSLT19 shared task in English→Czech text-to-text translation of TED talks. The goal of this study is to understand the interactions between document-level NMT and domain adaptation. All our systems are based on the Transformer model implemented in the Tensor2Tensor framework. Two of the systems serve as baselines which are not adapted to the TED talks domain: SENTBASE is trained on single sen- tences DOCBASE on multi-sentence (document-level) sequences. The other two submitted systems are adapted to TED talks: SENTFINE is fine-tuned on single sentences DOCFINE is fine-tuned on multi-sentence sequences. We present both automatic-metrics evaluation and manual analysis of the translation quality focusing on the differences between the four systems.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Domain Adaptation of Document-Level NMT in IWSLT19</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2019.iwslt-1.8" target="_blank">https://aclanthology.org/2019.iwslt-1.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2019</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In Machine Translation considering the document as a whole can help to resolve ambiguities and inconsistencies. In this paper we propose a simple yet promising approach to add contextual information in Neural Machine Translation. We present a method to add source context that capture the whole document with accurate boundaries taking every word into account. We provide this additional information to a Transformer model and study the impact of our method on three language pairs. The proposed approach obtains promising results in the English-German English-French and French-English document-level translation tasks. We observe interesting cross-sentential behaviors where the model learns to use document-level information to improve translation coherence.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using Whole Document Context in Neural Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2019.iwslt-1.21" target="_blank">https://aclanthology.org/2019.iwslt-1.21</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Study of Readability of Health Documents with Eye-tracking Approaches</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W18-7003" target="_blank">https://aclanthology.org/W18-7003</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper proposes an approach to NLG system design which focuses on generating output text which can be more easily processed by the reader. Ways in which cognitive theory might be combined with existing NLG techniques are discussed and two simple experiments in content ordering are presented.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Comprehension Driven Document Planning in Natural Language Generation Systems</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W18-6544" target="_blank">https://aclanthology.org/W18-6544</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Till now neural abstractive summarization methods have achieved great success for single document summarization (SDS). However due to the lack of large scale multi-document summaries such methods can be hardly applied to multi-document summarization (MDS). In this paper we investigate neural abstractive methods for MDS by adapting a state-of-the-art neural abstractive summarization model for SDS. We propose an approach to extend the neural abstractive model trained on large scale SDS data to the MDS task. Our approach only makes use of a small number of multi-document summaries for fine tuning. Experimental results on two benchmark DUC datasets demonstrate that our approach can outperform a variety of baseline neural models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Adapting Neural Single-Document Summarization Model for Abstractive Multi-Document Summarization: A Pilot Study</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W18-6545" target="_blank">https://aclanthology.org/W18-6545</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>For the WMT 2018 shared task of translating documents pertaining to the Biomedical domain we developed a scoring formula that uses an unsophisticated and effective method of weighting term frequencies and was integrated in a data selection pipeline. The method was applied on five language pairs and it performed best on Portuguese-English where a BLEU score of 41.84 placed it third out of seven runs submitted by three institutions. In this paper we describe our method and results with a special focus on Spanish-English where we compare it against a state-of-the-art method. Our contribution to the task lies in introducing a fast unsupervised method for selecting domain-specific data for training models which obtain good results using only 10% of the general domain data.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Translation of Biomedical Documents with Focus on Spanish-English</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W18-6444" target="_blank">https://aclanthology.org/W18-6444</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents the ColumbiaNLP submission for the FEVER Workshop Shared Task. Our system is an end-to-end pipeline that extracts factual evidence from Wikipedia and infers a decision about the truthfulness of the claim based on the extracted evidence. Our pipeline achieves significant improvement over the baseline for all the components (Document Retrieval Sentence Selection and Textual Entailment) both on the development set and the test set. Our team finished 6th out of 24 teams on the leader-board based on the preliminary results with a FEVER score of 49.06 on the blind test set compared to 27.45 of the baseline system.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Robust Document Retrieval and Individual Evidence Modeling for Fact Extraction and Verification.</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W18-5521" target="_blank">https://aclanthology.org/W18-5521</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This work aims to contribute to our understanding of textitwhen multi-task learning through parameter sharing in deep neural networks leads to improvements over single-task learning. We focus on the setting of learning from textitloosely related tasks for which no theoretical guarantees exist. We therefore approach the question empirically studying which properties of datasets and single-task learning characteristics correlate with improvements from multi-task learning. We are the first to study this in a text classification setting and across more than 500 different task pairs.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>When does deep multi-task learning work for loosely related document classification tasks?</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W18-5401" target="_blank">https://aclanthology.org/W18-5401</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present AUEB&#39;s submissions to the BioASQ 6 document and snippet retrieval tasks (parts of Task 6b Phase A). Our models use novel extensions to deep learning architectures that operate solely over the text of the query and candidate document/snippets. Our systems scored at the top or near the top for all batches of the challenge highlighting the effectiveness of deep learning for these tasks.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>AUEB at BioASQ 6: Document and Snippet Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W18-5304" target="_blank">https://aclanthology.org/W18-5304</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Morphological analysis of morphologically rich and low-resource languages is important to both descriptive linguistics and natural language processing. Field documentary efforts usually procure analyzed data in cooperation with native speakers who are capable of providing some level of linguistic information. Manually annotating such data is very expensive and the traditional process is arguably too slow in the face of language endangerment and loss. We report on a case study of learning to automatically gloss a Nakh-Daghestanian language Lezgi from a very small amount of seed data. We compare a conditional random field based sequence labeler and a neural encoder-decoder model and show that a nearly 0.9 F1-score on labeled accuracy of morphemes can be achieved with 3000 words of transcribed oral text. Errors are mostly limited to morphemes with high allomorphy. These results are potentially useful for developing rapid annotation and fieldwork tools to support documentation of morphologically rich endangered languages.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Glossing in a Low-Resource Setting for Language Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W18-4809" target="_blank">https://aclanthology.org/W18-4809</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Cross-document event chain co-referencing in corpora of news articles would achieve increased precision and generalizability from a method that consistently recognizes narrative discursive and phenomenological features such as tense mood tone canonicity and breach person hermeneutic composability speed and time. Current models that capture primarily linguistic data such as entities times and relations or causal relationships may only incidentally capture narrative framing features of events. That limits efforts at narrative and event chain segmentation among other predicate tasks for narrative search and narrative-based reasoning. It further limits research on audience engagement with journalism about complex subjects. This position paper explores the above proposition with respect to narrative theory and ongoing research on segmenting event chains into narrative units. Our own work in progress approaches this task using event segmentation word embeddings and variable length pattern matching in a corpus of 2000 articles describing environmental events. Our position is that narrative features may or may not be implicitly captured by current methods explicitly focused on events as linguistic phenomena that they are not explicitly captured and that further research is required.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-Document Narrative Alignment of Environmental News: A Position Paper on the Challenge of Using Event Chains to Proxy Narrative Features</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W18-4303" target="_blank">https://aclanthology.org/W18-4303</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Childhood acquisition of written language is not straightforward. Writing skills evolve differently depending on external factors such as the conditions in which children practice their productions and the quality of their instructors&#39; guidance. This can be challenging in low-income areas where schools may struggle to ensure ideal acquisition conditions. Developing computational tools to support the learning process may counterweight negative environmental influences; however few work exists on the use of information technologies to improve childhood literacy. This work centers around the computational study of Spanish word and syllable structure in documents written by 2nd and 3rd year elementary school students. The studied texts were compared against a corpus of short stories aimed at the same age group so as to observe whether the children tend to produce similar written patterns as the ones they are expected to interpret at their literacy level. The obtained results show some significant differences between the two kinds of texts pointing towards possible strategies for the implementation of new education software in support of written language acquisition.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Textual Features Indicative of Writing Proficiency in Elementary School Spanish Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W18-3716" target="_blank">https://aclanthology.org/W18-3716</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Despite the popularity of word embeddings the precise way by which they acquire semantic relations between words remain unclear. In the present article we investigate whether LSA and word2vec capacity to identify relevant semantic relations increases with corpus size. One intuitive hypothesis is that the capacity to identify relevant associations should increase as the amount of data increases. However if corpus size grows in topics which are not specific to the domain of interest signal to noise ratio may weaken. Here we investigate the effect of corpus specificity and size in word-embeddings and for this we study two ways for progressive elimination of documents: the elimination of random documents vs. the elimination of documents unrelated to a specific task. We show that word2vec can take advantage of all the documents obtaining its best performance when it is trained with the whole corpus. On the contrary the specialization (removal of out-of-domain documents) of the training corpus accompanied by a decrease of dimensionality can increase LSA word-representation quality while speeding up the processing time. From a cognitive-modeling point of view we point out that LSA&#39;s word-knowledge acquisitions may not be efficiently exploiting higher-order co-occurrences and global relations whereas word2vec does.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Corpus Specificity in LSA and Word2vec: The Role of Out-of-Domain Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W18-3001" target="_blank">https://aclanthology.org/W18-3001</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We explore representations for multi-word names in text classification tasks on Reuters (RCV1) topic and sector classification. We find that: the best way to treat names is to split them into tokens and use each token as a separate feature; NEs have more impact on sector classification than topic classification; replacing NEs with entity types is not an effective strategy; representing tokens by different embeddings for proper names vs. common nouns does not improve results. We highlight the improvements over state-of-the-art results that our CNN models yield.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Comparison of Representations of Named Entities for Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W18-3008" target="_blank">https://aclanthology.org/W18-3008</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we continue experiments where neural machine translation training is used to produce joint cross-lingual fixed-dimensional sentence embeddings. In this framework we introduce a simple method of adding a loss to the learning objective which penalizes distance between representations of bilingually aligned sentences. We evaluate cross-lingual transfer using two approaches cross-lingual similarity search on an aligned corpus (Europarl) and cross-lingual document classification on a recently published benchmark Reuters corpus and we find the similarity loss significantly improves performance on both. Furthermore we notice that while our Reuters results are very competitive our English results are not as competitive showing room for improvement in the current cross-lingual state-of-the-art. Our results are based on a set of 6 European languages.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilingual Seq2seq Training with Similarity Loss for Cross-Lingual Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W18-3023" target="_blank">https://aclanthology.org/W18-3023</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>It is common practice to adapt machine translation systems to novel domains but even a well-adapted system may be able to perform better on a particular document if it were to learn from a translator&#39;s corrections within the document itself. We focus on adaptation within a single document -- appropriate for an interactive translation scenario where a model adapts to a human translator&#39;s input over the course of a document. We propose two methods: single-sentence adaptation (which performs online adaptation one sentence at a time) and dictionary adaptation (which specifically addresses the issue of translating novel words). Combining the two models results in improvements over both approaches individually and over baseline systems even on short documents. On WMT news test data we observe an improvement of +1.8 BLEU points and +23.3% novel word translation accuracy and on EMEA data (descriptions of medications) we observe an improvement of +2.7 BLEU points and +49.2% novel word translation accuracy.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Adaptation for Neural Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W18-2708" target="_blank">https://aclanthology.org/W18-2708</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In recent years the journalists and computer sciences speak to each other to identify useful technologies which would help them in extracting useful information. This is called ``computational Journalism&#39;&#39;. In this paper we present a method that will enable the journalists to automatically identifies and annotates entities such as names of people organizations role and functions of people in legal documents; the relationship between these entities are also explored. The system uses a combination of both statistical and rule based technique. The statistical method used is Conditional Random Fields and for the rule based technique document and language specific regular expressions are used.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Extraction of Entities and Relation from Legal Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W18-2401" target="_blank">https://aclanthology.org/W18-2401</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this study we investigate learning-to-rank and query refinement approaches for information retrieval in the pharmacogenomic domain. The goal is to improve the information retrieval process of biomedical curators who manually build knowledge bases for personalized medicine. We study how to exploit the relationships between genes variants drugs diseases and outcomes as features for document ranking and query refinement. For a supervised approach we are faced with a small amount of annotated data and a large amount of unannotated data. Therefore we explore ways to use a neural document auto-encoder in a semi-supervised approach. We show that a combination of established algorithms feature-engineering and a neural auto-encoder model yield promising results in this setting.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Neural Autoencoder Approach for Document Ranking and Query Refinement in Pharmacogenomic Information Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W18-2310" target="_blank">https://aclanthology.org/W18-2310</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Information as Side Constraints for Improved Neural Patent Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W18-1802" target="_blank">https://aclanthology.org/W18-1802</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Contrary to the traditional Bag-of-Words approach we consider the Graph-of-Words(GoW) model in which each document is represented by a graph that encodes relationships between the different terms. Based on this formulation the importance of a term is determined by weighting the corresponding node in the document collection and label graphs using node centrality criteria. We also introduce novel graph-based weighting schemes by enriching graphs with word-embedding similarities in order to reward or penalize semantic relationships. Our methods produce more discriminative feature weights for text categorization outperforming existing frequency-based criteria.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Fusing Document Collection and Label Graph-based Representations with Word Embeddings for Text Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W18-1707" target="_blank">https://aclanthology.org/W18-1707</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence paragraph or document. Enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods but currently no resources exist to train and test this capability. We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods. In our task a model learns to seek and combine evidence --- effectively performing multihop alias multi-step inference. We devise a methodology to produce datasets for this task given a collection of query-answer pairs and thematically linked documents. Two datasets from different domains are induced and we identify potential pitfalls and devise circumvention strategies. We evaluate two previously proposed competitive models and find that one can integrate information across documents. However both models struggle to select relevant information; and providing documents guaranteed to be relevant greatly improves their performance. While the models outperform several strong baselines their best accuracy reaches 54.5% on an annotated test set compared to human performance at 85.0% leaving ample room for improvement.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Constructing Datasets for Multi-hop Reading Comprehension Across Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Q18-1021" target="_blank">https://aclanthology.org/Q18-1021</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present an event extraction framework to detect event mentions and extract events from the document-level financial news. Up to now methods based on supervised learning paradigm gain the highest performance in public datasets (such as ACE2005 KBP2015). These methods heavily depend on the manually labeled training data. However in particular areas such as financial medical and judicial domains there is no enough labeled data due to the high cost of data labeling process. Moreover most of the current methods focus on extracting events from one sentence but an event is usually expressed by multiple sentences in one document. To solve these problems we propose a Document-level Chinese Financial Event Extraction (DCFEE) system which can automatically generate a large scaled labeled data and extract events from the whole document. Experimental results demonstrate the effectiveness of it</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DCFEE: A Document-level Chinese Financial Event Extraction System based on Automatically Labeled Training Data</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P18-4009" target="_blank">https://aclanthology.org/P18-4009</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The availability of huge amount of biomedical literature have opened up new possibilities to apply Information Retrieval and NLP for mining documents from them. In this work we are focusing on biomedical document retrieval from literature for clinical decision support systems. We compare statistical and NLP based approaches of query reformulation for biomedical document retrieval. Also we have modeled the biomedical document retrieval as a learning to rank problem. We report initial results for statistical and NLP based query reformulation approaches and learning to rank approach with future direction of research.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Biomedical Document Retrieval for Clinical Decision Support System</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P18-3012" target="_blank">https://aclanthology.org/P18-3012</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level information is very important for event detection even at sentence level. In this paper we propose a novel Document Embedding Enhanced Bi-RNN model called DEEB-RNN to detect events in sentences. This model first learns event detection oriented embeddings of documents through a hierarchical and supervised attention based RNN which pays word-level attention to event triggers and sentence-level attention to those sentences containing events. It then uses the learned document embedding to enhance another bidirectional RNN model to identify event triggers and their types in sentences. Through experiments on the ACE-2005 dataset we demonstrate the effectiveness and merits of the proposed DEEB-RNN model via comparison with state-of-the-art methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P18-2066" target="_blank">https://aclanthology.org/P18-2066</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>For extracting meaningful topics from texts their structures should be considered properly. In this paper we aim to analyze structured time-series documents such as a collection of news articles and a series of scientific papers wherein topics evolve along time depending on multiple topics in the past and are also related to each other at each time. To this end we propose a dynamic and static topic model which simultaneously considers the dynamic structures of the temporal topic evolution and the static structures of the topic hierarchy at each time. We show the results of experiments on collections of scientific papers in which the proposed method outperformed conventional models. Moreover we show an example of extracted topic structures which we found helpful for analyzing research activities.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Dynamic and Static Topic Model for Analyzing Time-Series Document Collections</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P18-2082" target="_blank">https://aclanthology.org/P18-2082</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we address the problem of finding a novel document descriptor based on the covariance matrix of the word vectors of a document. Our descriptor has a fixed length which makes it easy to use in many supervised and unsupervised applications. We tested our novel descriptor in different tasks including supervised and unsupervised settings. Our evaluation shows that our document covariance descriptor fits different tasks with competitive performance against state-of-the-art methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Document Descriptor using Covariance of Word Vectors</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P18-2084" target="_blank">https://aclanthology.org/P18-2084</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Attention-based long short-term memory (LSTM) networks have proven to be useful in aspect-level sentiment classification. However due to the difficulties in annotating aspect-level data existing public datasets for this task are all relatively small which largely limits the effectiveness of those neural models. In this paper we explore two approaches that transfer knowledge from document-level data which is much less expensive to obtain to improve the performance of aspect-level sentiment classification. We demonstrate the effectiveness of our approaches on 4 public datasets from SemEval 2014 2015 and 2016 and we show that attention-based LSTM benefits from document-level knowledge in multiple ways.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploiting Document Knowledge for Aspect-level Sentiment Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P18-2092" target="_blank">https://aclanthology.org/P18-2092</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Many corpora span broad periods of time. Language processing models trained during one time period may not work well in future time periods and the best model may depend on specific times of year (e.g. people might describe hotels differently in reviews during the winter versus the summer). This study investigates how document classifiers trained on documents from certain time intervals perform on documents from other time intervals considering both seasonal intervals (intervals that repeat across years e.g. winter) and non-seasonal intervals (e.g. specific years). We show experimentally that classification performance varies over time and that performance can be improved by using a standard domain adaptation approach to adjust for changes in time.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Examining Temporality in Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P18-2110" target="_blank">https://aclanthology.org/P18-2110</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The behavior of deep neural networks (DNNs) is hard to understand. This makes it necessary to explore post hoc explanation methods. We conduct the first comprehensive evaluation of explanation methods for NLP. To this end we design two novel evaluation paradigms that cover two important classes of NLP problems: small context and large context problems. Both paradigms require no manual annotation and are therefore broadly applicable. We also introduce LIMSSE an explanation method inspired by LIME that is designed for NLP. We show empirically that LIMSSE LRP and DeepLIFT are the most effective explanation methods and recommend them for explaining DNNs in NLP.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P18-1032" target="_blank">https://aclanthology.org/P18-1032</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper proposes a novel approach for event coreference resolution that models correlations between event coreference chains and document topical structures through an Integer Linear Programming formulation. We explicitly model correlations between the main event chains of a document with topic transition sentences inter-coreference chain correlations event mention distributional characteristics and sub-event structure and use them with scores obtained from a local coreference relation classifier for jointly resolving multiple event chains in a document. Our experiments across KBP 2016 and 2017 datasets suggest that each of the structures contribute to improving event coreference resolution performance.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Event Coreference Resolution by Modeling Correlations between Event Coreference Chains and Document Topic Structures</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P18-1045" target="_blank">https://aclanthology.org/P18-1045</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Sentence scoring and sentence selection are two main steps in extractive document summarization systems. However previous works treat them as two separated subtasks. In this paper we present a novel end-to-end neural network framework for extractive document summarization by jointly learning to score and select sentences. It first reads the document sentences with a hierarchical encoder to obtain the representation of sentences. Then it builds the output summary by extracting sentences one by one. Different from previous methods our approach integrates the selection strategy into the scoring model which directly predicts the relative importance given previously selected sentences. Experiments on the CNN/Daily Mail dataset show that the proposed framework significantly outperforms the state-of-the-art extractive summarization models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Neural Document Summarization by Jointly Learning to Score and Select Sentences</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P18-1061" target="_blank">https://aclanthology.org/P18-1061</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a document-level neural machine translation model which takes both source and target document context into account using memory networks. We model the problem as a structured prediction problem with interdependencies among the observed and hidden variables i.e. the source sentences and their unobserved target translations in the document. The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components one each for the source and target side to capture the documental interdependencies. We train the model end-to-end and propose an iterative decoding algorithm based on block coordinate descent. Experimental results of English translations from French German and Estonian documents show that our model is effective in exploiting both source and target document context and statistically significantly outperforms the previous work in terms of BLEU and METEOR.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Context Neural Machine Translation with Memory Networks</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P18-1118" target="_blank">https://aclanthology.org/P18-1118</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document date is essential for many important tasks such as document retrieval summarization event detection etc. While existing approaches for these tasks assume accurate knowledge of the document date this is not always available especially for arbitrary documents from the Web. Document Dating is a challenging problem which requires inference over the temporal structure of the document. Prior document dating systems have largely relied on handcrafted features while ignoring such document-internal structures. In this paper we propose NeuralDater a Graph Convolutional Network (GCN) based document dating approach which jointly exploits syntactic and temporal graph structures of document in a principled way. To the best of our knowledge this is the first application of deep learning for the problem of document dating. Through extensive experiments on real-world datasets we find that NeuralDater significantly outperforms state-of-the-art baseline by 19% absolute (45% relative) accuracy points.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Dating Documents using Graph Convolution Networks</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P18-1149" target="_blank">https://aclanthology.org/P18-1149</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Neural models for question answering (QA) over documents have achieved significant performance improvements. Although effective these models do not scale to large corpora due to their complex modeling of interactions between the document and the question. Moreover recent work has shown that such models are sensitive to adversarial inputs. In this paper we study the minimal context required to answer the question and find that most questions in existing datasets can be answered with a small set of sentences. Inspired by this observation we propose a simple sentence selector to select the minimal set of sentences to feed into the QA model. Our overall system achieves significant reductions in training (up to 15 times) and inference times (up to 13 times) with accuracy comparable to or better than the state-of-the-art on SQuAD NewsQA TriviaQA and SQuAD-Open. Furthermore our experimental results and analyses show that our approach is more robust to adversarial inputs.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Efficient and Robust Question Answering from Minimal Context over Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P18-1160" target="_blank">https://aclanthology.org/P18-1160</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document modeling is essential to a variety of natural language understanding tasks. We propose to use external information to improve document modeling for problems that can be framed as sentence extraction. We develop a framework composed of a hierarchical document encoder and an attention-based extractor with attention over external information. We evaluate our model on extractive document summarization (where the external information is image captions and the title of the document) and answer selection (where the external information is a question). We show that our model consistently outperforms strong baselines in terms of both informativeness and fluency (for CNN document summarization) and achieves state-of-the-art results for answer selection on WikiQA and NewsQA.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Modeling with External Attention for Sentence Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P18-1188" target="_blank">https://aclanthology.org/P18-1188</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Most real-world document collections involve various types of metadata such as author source and date and yet the most commonly-used approaches to modeling text corpora ignore this information. While specialized models have been developed for particular applications few are widely used in practice as customization typically requires derivation of a custom inference algorithm. In this paper we build on recent advances in variational inference methods and propose a general neural framework based on topic models to enable flexible incorporation of metadata and allow for rapid exploration of alternative models. Our approach achieves strong performance with a manageable tradeoff between perplexity coherence and sparsity. Finally we demonstrate the potential of our framework through an exploration of a corpus of articles about US immigration.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Neural Models for Documents with Metadata</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P18-1189" target="_blank">https://aclanthology.org/P18-1189</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In the era of big data focused analysis for diverse topics with a short response time becomes an urgent demand. As a fundamental task information filtering therefore becomes a critical necessity. In this paper we propose a novel deep relevance model for zero-shot document filtering named DAZER. DAZER estimates the relevance between a document and a category by taking a small set of seed words relevant to the category. With pre-trained word embeddings from a large external corpus DAZER is devised to extract the relevance signals by modeling the hidden feature interactions in the word embedding space. The relevance signals are extracted through a gated convolutional process. The gate mechanism controls which convolution filters output the relevance signals in a category dependent manner. Experiments on two document collections of two different tasks (i.e. topic categorization and sentiment analysis) demonstrate that DAZER significantly outperforms the existing alternative solutions including the state-of-the-art deep relevance ranking models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Deep Relevance Model for Zero-Shot Document Filtering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P18-1214" target="_blank">https://aclanthology.org/P18-1214</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths such as a long document and its summary. This is because of the lexical contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper we present a document matching approach to bridge this gap by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Similarity for Texts of Varying Lengths via Hidden Topics</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P18-1218" target="_blank">https://aclanthology.org/P18-1218</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Hypertext documents such as web pages and academic papers are of great importance in delivering information in our daily life. Although being effective on plain documents conventional text embedding methods suffer from information loss if directly adapted to hyper-documents. In this paper we propose a general embedding approach for hyper-documents namely hyperdoc2vec along with four criteria characterizing necessary information that hyper-document embedding models should preserve. Systematic comparisons are conducted between hyperdoc2vec and several competitors on two tasks i.e. paper classification and citation recommendation in the academic paper domain. Analyses and experiments both validate the superiority of hyperdoc2vec to other models w.r.t. the four criteria.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>hyperdoc2vec: Distributed Representations of Hypertext Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P18-1222" target="_blank">https://aclanthology.org/P18-1222</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We propose Object-oriented Neural Programming (OONP) a framework for semantically parsing documents in specific domains. Basically OONP reads a document and parses it into a predesigned object-oriented data structure that reflects the domain-specific semantics of the document. An OONP parser models semantic parsing as a decision process: a neural net-based Reader sequentially goes through the document and builds and updates an intermediate ontology during the process to summarize its partial understanding of the text. OONP supports a big variety of forms (both symbolic and differentiable) for representing the state and the document and a rich family of operations to compose the representation. An OONP parser can be trained with supervision of different forms and strength including supervised learning (SL) reinforcement learning (RL) and hybrid of the two. Our experiments on both synthetic and real-world document parsing tasks have shown that OONP can learn to handle fairly complicated ontology with training data of modest sizes.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Object-oriented Neural Programming (OONP) for Document Understanding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P18-1253" target="_blank">https://aclanthology.org/P18-1253</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>基於基因演算法的組合式多文件摘要方法 (An Ensemble Approach for Multi-document Summarization using Genetic Algorithms) [In Chinese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O18-1008" target="_blank">https://aclanthology.org/O18-1008</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using Statistical and Semantic Models for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O18-1018" target="_blank">https://aclanthology.org/O18-1018</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>A medical scribe is a clinical professional who charts patient--physician encounters in real time relieving physicians of most of their administrative burden and substantially increasing productivity and job satisfaction. We present a complete implementation of an automated medical scribe. Our system can serve either as a scalable standardized and economical alternative to human scribes; or as an assistive tool for them providing a first draft of a report along with a convenient means to modify it. This solution is to our knowledge the first automated scribe ever presented and relies upon multiple speech and language technologies including speaker diarization medical speech recognition knowledge extraction and natural language generation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An automated medical scribe for documenting clinical encounters</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N18-5003" target="_blank">https://aclanthology.org/N18-5003</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Job boards and professional social networks heavily use recommender systems in order to better support users in exploring job advertisements. Detecting the similarity between job advertisements is important for job recommendation systems as it allows for example the application of item-to-item based recommendations. In this work we research the usage of dense vector representations to enhance a large-scale job recommendation system and to rank German job advertisements regarding their similarity. We follow a two-folded evaluation scheme: (1) we exploit historic user interactions to automatically create a dataset of similar jobs that enables an offline evaluation. (2) In addition we conduct an online A/B test and evaluate the best performing method on our platform reaching more than 1 million users. We achieve the best results by combining job titles with full-text job descriptions. In particular this method builds dense document representation using words of the titles to weigh the importance of words of the full-text description. In the online evaluation this approach allows us to increase the click-through rate on job recommendations for active users by 8.0%.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-based Recommender System for Job Postings using Dense Representations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N18-3027" target="_blank">https://aclanthology.org/N18-3027</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Neural abstractive summarization models have led to promising results in summarizing relatively short documents. We propose the first model for abstractive summarization of single longer-form documents (e.g. research papers). Our approach consists of a new hierarchical encoder that models the discourse structure of a document and an attentive discourse-aware decoder to generate the summary. Empirical results on two large-scale datasets of scientific papers show that our model significantly outperforms state-of-the-art models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N18-2097" target="_blank">https://aclanthology.org/N18-2097</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Preserving Workflow Reproducibility: The RePlay-DH Client as a Tool for Process Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L18-1089" target="_blank">https://aclanthology.org/L18-1089</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Transc&amp;Anno: A Graphical Tool for the Transcription and On-the-Fly Annotation of Handwritten Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L18-1112" target="_blank">https://aclanthology.org/L18-1112</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>PDFAnno: a Web-based Linguistic Annotation Tool for PDF Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L18-1175" target="_blank">https://aclanthology.org/L18-1175</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards Processing of the Oral History Interviews and Related Printed Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L18-1331" target="_blank">https://aclanthology.org/L18-1331</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Introducing the CLARIN Knowledge Centre for Linguistic Diversity and Language Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L18-1370" target="_blank">https://aclanthology.org/L18-1370</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Low Resource Methods for Medieval Document Sections Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L18-1371" target="_blank">https://aclanthology.org/L18-1371</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Auto-hMDS: Automatic Construction of a Large Heterogeneous Multilingual Multi-Document Summarization Corpus</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L18-1510" target="_blank">https://aclanthology.org/L18-1510</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Evaluation Phonemic Transcription of Low-Resource Tonal Languages for Language Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L18-1530" target="_blank">https://aclanthology.org/L18-1530</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Very Low Resource Language Speech Corpus for Computational Language Documentation Experiments</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L18-1531" target="_blank">https://aclanthology.org/L18-1531</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>BULBasaa: A Bilingual Basaa-French Speech Corpus for the Evaluation of Language Documentation Tools</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L18-1533" target="_blank">https://aclanthology.org/L18-1533</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-Document Cross-Language Event Coreference Annotation Using Event Hoppers</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L18-1558" target="_blank">https://aclanthology.org/L18-1558</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>TAP-DLND 1.0 : A Corpus for Document Level Novelty Detection</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L18-1559" target="_blank">https://aclanthology.org/L18-1559</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Corpus for Multilingual Document Classification in Eight Languages</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L18-1560" target="_blank">https://aclanthology.org/L18-1560</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automating Document Discovery in the Systematic Review Process: How to Use Chaff to Extract Wheat</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L18-1582" target="_blank">https://aclanthology.org/L18-1582</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ASR for Documenting Acutely Under-Resourced Indigenous Languages</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L18-1657" target="_blank">https://aclanthology.org/L18-1657</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The MonPaGe_HA Database for the Documentation of Spoken French Throughout Adulthood</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L18-1679" target="_blank">https://aclanthology.org/L18-1679</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Czech Text Document Corpus v 2.0</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L18-1687" target="_blank">https://aclanthology.org/L18-1687</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Although the Transformer translation model (Vaswani et al. 2017) has achieved state-of-the-art performance in a variety of translation tasks how to use document-level context to deal with discourse phenomena problematic for Transformer still remains a challenge. In this work we extend the Transformer model with a new context encoder to represent document-level context which is then incorporated into the original encoder and decoder. As large-scale document-level parallel corpora are usually not available we introduce a two-step training method to take full advantage of abundant sentence-level parallel corpora and limited document-level parallel corpora. Experiments on the NIST Chinese-English datasets and the IWSLT French-English datasets show that our approach improves over Transformer significantly.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving the Transformer Translation Model with Document-Level Context</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D18-1049" target="_blank">https://aclanthology.org/D18-1049</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We formalize a new modular variant of current question answering tasks by enforcing complete independence of the document encoder from the question encoder. This formulation addresses a key challenge in machine comprehension by building a standalone representation of the document discourse. It additionally leads to a significant scalability advantage since the encoding of the answer candidate phrases in the document can be pre-computed and indexed offline for efficient retrieval. We experiment with baseline models for the new task which achieve a reasonable accuracy but significantly underperform unconstrained QA models. We invite the QA research community to engage in Phrase-Indexed Question Answering (PIQA pika) for closing the gap. The leaderboard is at: urlnlp.cs.washington.edu/piqa</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Phrase-Indexed Question Answering: A New Challenge for Scalable Document Comprehension</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D18-1052" target="_blank">https://aclanthology.org/D18-1052</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>State-of-the-art systems in deep question answering proceed as follows: (1)an initial document retrieval selects relevant documents which (2) are then processed by a neural network in order to extract the final answer. Yet the exact interplay between both components is poorly understood especially concerning the number of candidate documents that should be retrieved. We show that choosing a static number of documents - as used in prior research - suffers from a noise-information trade-off and yields suboptimal results. As a remedy we propose an adaptive document retrieval model. This learns the optimal candidate number for document retrieval conditional on the size of the corpus and the query. We report extensive experimental results showing that our adaptive approach outperforms state-of-the-art methods on multiple benchmark datasets as well as in the context of corpora with variable sizes.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Adaptive Document Retrieval for Deep Question Answering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D18-1055" target="_blank">https://aclanthology.org/D18-1055</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper introduces a document grounded dataset for conversations. We define ``Document Grounded Conversations&#39;&#39; as conversations that are about the contents of a specified document. In this dataset the specified documents were Wikipedia articles about popular movies. The dataset contains 4112 conversations with an average of 21.43 turns per conversation. This positions this dataset to not only provide a relevant chat history while generating responses but also provide a source of information that the models could use. We describe two neural architectures that provide benchmark performance on the task of generating the next response. We also evaluate our models for engagement and fluency and find that the information from the document helps in generating more engaging and fluent responses.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Dataset for Document Grounded Conversations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D18-1076" target="_blank">https://aclanthology.org/D18-1076</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Extractive summarization models need sentence level labels which are usually created with rule-based methods since most summarization datasets only have document summary pairs. These labels might be suboptimal. We propose a latent variable extractive model where sentences are viewed as latent variables and sentences with activated variables are used to infer gold summaries. During training the loss can come directly from gold summaries. Experiments on CNN/Dailymail dataset show our latent extractive model outperforms a strong extractive baseline trained on rule-based labels and also performs competitively with several recent models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Neural Latent Extractive Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D18-1088" target="_blank">https://aclanthology.org/D18-1088</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Many modern neural document summarization systems based on encoder-decoder networks are designed to produce abstractive summaries. We attempted to verify the degree of abstractiveness of modern neural abstractive summarization systems by calculating overlaps in terms of various types of units. Upon the observation that many abstractive systems tend to be near-extractive in practice we also implemented a pure copy system which achieved comparable results as abstractive summarizers while being far more computationally efficient. These findings suggest the possibility for future efforts towards more efficient systems that could better utilize the vocabulary in the original document.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>On the Abstractiveness of Neural Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D18-1089" target="_blank">https://aclanthology.org/D18-1089</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The text in many web documents is organized into a hierarchy of section titles and corresponding prose content a structure which provides potentially exploitable information on discourse structure and topicality. However this organization is generally discarded during text collection and collecting it is not straightforward: the same visual organization can be implemented in a myriad of different ways in the underlying HTML. To remedy this we present a flexible system for automatically extracting the hierarchical section titles and prose organization of web documents irrespective of differences in HTML representation. This system uses features from syntax semantics discourse and markup to build two models which classify HTML text into section titles and prose text. When tested on three different domains of web text our domain-independent system achieves an overall precision of 0.82 and a recall of 0.98. The domain-dependent variation produces very high precision (0.99) at the expense of recall (0.75). These results exhibit a robust level of accuracy suitable for enhancing question answering information extraction and summarization.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Supervised and Unsupervised Methods for Robust Separation of Section Titles and Prose Text in Web Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D18-1099" target="_blank">https://aclanthology.org/D18-1099</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Information selection is the most important component in document summarization task. In this paper we propose to extend the basic neural encoding-decoding framework with an information selection layer to explicitly model and optimize the information selection process in abstractive document summarization. Specifically our information selection layer consists of two parts: gated global information filtering and local sentence selection. Unnecessary information in the original document is first globally filtered then salient sentences are selected locally while generating each summary sentence sequentially. To optimize the information selection process directly distantly-supervised training guided by the golden summary is also imported. Experimental results demonstrate that the explicit modeling and optimizing of the information selection process improves document summarization performance significantly which enables our model to generate more informative and concise summaries and thus significantly outperform state-of-the-art neural abstractive methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Neural Abstractive Document Summarization with Explicit Information Selection Modeling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D18-1205" target="_blank">https://aclanthology.org/D18-1205</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We explore several new models for document relevance ranking building upon the Deep Relevance Matching Model (DRMM) of Guo et al. (2016). Unlike DRMM which uses context-insensitive encodings of terms and query-document term interactions we inject rich context-sensitive encodings throughout our models inspired by PACRR&#39;s (Hui et al. 2017) convolutional n-gram matching features but extended in several ways including multiple views of query and document inputs. We test our models on datasets from the BIOASQ question answering challenge (Tsatsaronis et al. 2015) and TREC ROBUST 2004 (Voorhees 2005) showing they outperform BM25-based baselines DRMM and PACRR.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Deep Relevance Ranking Using Enhanced Document-Query Interactions</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D18-1211" target="_blank">https://aclanthology.org/D18-1211</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Knowledge of the creation date of documents facilitates several tasks such as summarization event extraction temporally focused information extraction etc. Unfortunately for most of the documents on the Web the time-stamp metadata is either missing or can&#39;t be trusted. Thus predicting creation time from document content itself is an important task. In this paper we propose Attentive Deep Document Dater (AD3) an attention-based neural document dating system which utilizes both context and temporal information in documents in a flexible and principled manner. We perform extensive experimentation on multiple real-world datasets to demonstrate the effectiveness of AD3 over neural and non-neural baselines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>AD3: Attentive Deep Document Dater</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D18-1213" target="_blank">https://aclanthology.org/D18-1213</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Neural Machine Translation (NMT) can be improved by including document-level contextual information. For this purpose we propose a hierarchical attention model to capture the context in a structured and dynamic manner. The model is integrated in the original NMT architecture as another level of abstraction conditioning on the NMT model&#39;s own previous hidden states. Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods and that both the encoder and decoder benefit from context in complementary ways.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Neural Machine Translation with Hierarchical Attention Networks</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D18-1325" target="_blank">https://aclanthology.org/D18-1325</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Background research is an essential part of document writing. Search engines are great for retrieving information once we know what to look for. However the bigger challenge is often identifying topics for further research. Automated tools could help significantly in this discovery process and increase the productivity of the writer. In this paper we formulate the problem of recommending topics to a writer. We consider this as a supervised learning problem and run a user study to validate this approach. We propose an evaluation metric and perform an empirical comparison of state-of-the-art models for extreme multi-label classification on a large data set. We demonstrate how a simple modification of the cross-entropy loss function leads to improved results of the deep learning models.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Content Explorer: Recommending Novel Entities for a Document Writer</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D18-1374" target="_blank">https://aclanthology.org/D18-1374</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recent neural sequence-to-sequence models have shown significant progress on short text summarization. However for document summarization they fail to capture the long-term structure of both documents and multi-sentence summaries resulting in information loss and repetitions. In this paper we propose to leverage the structural information of both documents and multi-sentence summaries to improve the document summarization performance. Specifically we import both structural-compression and structural-coverage regularization into the summarization process in order to capture the information compression and information coverage properties which are the two most important structural properties of document summarization. Experimental results demonstrate that the structural regularization improves the document summarization performance significantly which enables our model to generate more informative and concise summaries and thus significantly outperforms state-of-the-art neural abstractive methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Neural Abstractive Document Summarization with Structural Regularization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D18-1441" target="_blank">https://aclanthology.org/D18-1441</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we introduce Iterative Text Summarization (ITS) an iteration-based model for supervised extractive text summarization inspired by the observation that it is often necessary for a human to read an article multiple times in order to fully understand and summarize its contents. Current summarization approaches read through a document only once to generate a document representation resulting in a sub-optimal representation. To address this issue we introduce a model which iteratively polishes the document representation on many passes through the document. As part of our model we also introduce a selective reading mechanism that decides more accurately the extent to which each sentence in the model should be updated. Experimental results on the CNN/DailyMail and DUC2002 datasets demonstrate that our model significantly outperforms state-of-the-art extractive systems when evaluated by machines and by humans.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Iterative Document Representation Learning Towards Summarization with Polishing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D18-1442" target="_blank">https://aclanthology.org/D18-1442</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Generating a text abstract from a set of documents remains a challenging task. The neural encoder-decoder framework has recently been exploited to summarize single documents but its success can in part be attributed to the availability of large parallel data automatically acquired from the Web. In contrast parallel data for multi-document summarization are scarce and costly to obtain. There is a pressing need to adapt an encoder-decoder model trained on single-document summarization data to work with multiple-document input. In this paper we present an initial investigation into a novel adaptation method. It exploits the maximal marginal relevance method to select representative sentences from multi-document input and leverages an abstractive encoder-decoder model to fuse disparate sentences to an abstractive summary. The adaptation method is robust and itself requires no training data. Our system compares favorably to state-of-the-art extractive and abstractive approaches judged by automatic metrics and human assessors.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D18-1446" target="_blank">https://aclanthology.org/D18-1446</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We introduce a novel type of text representation that preserves the 2D layout of a document. This is achieved by encoding each document page as a two-dimensional grid of characters. Based on this representation we present a generic document understanding pipeline for structured documents. This pipeline makes use of a fully convolutional encoder-decoder network that predicts a segmentation mask and bounding boxes. We demonstrate its capabilities on an information extraction task from invoices and show that it significantly outperforms approaches based on sequential text or document images.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Chargrid: Towards Understanding 2D Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D18-1476" target="_blank">https://aclanthology.org/D18-1476</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>While the celebrated Word2Vec technique yields semantically rich representations for individual words there has been relatively less success in extending to generate unsupervised sentences or documents embeddings. Recent work has demonstrated that a distance measure between documents called Word Mover&#39;s Distance (WMD) that aligns semantically similar words yields unprecedented KNN classification accuracy. However WMD is expensive to compute and it is hard to extend its use beyond a KNN classifier. In this paper we propose the Word Mover&#39;s Embedding (WME) a novel approach to building an unsupervised document (sentence) embedding from pre-trained word embeddings. In our experiments on 9 benchmark text classification datasets and 22 textual similarity tasks the proposed technique consistently matches or outperforms state-of-the-art techniques with significantly higher accuracy on problems of short length.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Word Mover&#39;s Embedding: From Word2Vec to Document Embedding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D18-1482" target="_blank">https://aclanthology.org/D18-1482</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Prevalence estimation is the task of inferring the relative frequency of classes of unlabeled examples in a group---for example the proportion of a document collection with positive sentiment. Previous work has focused on aggregating and adjusting discriminative individual classifiers to obtain prevalence point estimates. But imperfect classifier accuracy ought to be reflected in uncertainty over the predicted prevalence for scientifically valid inference. In this work we present (1) a generative probabilistic modeling approach to prevalence estimation and (2) the construction and evaluation of prevalence confidence intervals; in particular we demonstrate that an off-the-shelf discriminative classifier can be given a generative re-interpretation by backing out an implicit individual-level likelihood function which can be used to conduct fast and simple group-level Bayesian inference. Empirically we demonstrate our approach provides better confidence interval coverage than an alternative and is dramatically more robust to shifts in the class prior between training and testing.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Uncertainty-aware generative models for inferring document class prevalence</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D18-1487" target="_blank">https://aclanthology.org/D18-1487</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation protocols contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D18-1512" target="_blank">https://aclanthology.org/D18-1512</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Abbreviations and acronyms are a part of textual communication in most domains. However abbreviations are not necessarily defined in documents that employ them. Understanding all abbreviations used in a given document often requires extensive knowledge of the target domain and the ability to disambiguate based on context. This creates considerable entry barriers to newcomers and difficulties in automated document processing. Existing abbreviation expansion systems or tools require substantial technical knowledge for set up or make strong assumptions which limit their use in practice. Here we present Abbreviation Expander a system that builds on state of the art methods for identification of abbreviations acronyms and their definitions and a novel disambiguator for abbreviation expansion in an easily accessible web-based solution.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Abbreviation Expander - a Web-based System for Easy Reading of Technical Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C18-2001" target="_blank">https://aclanthology.org/C18-2001</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We tackle the problem of generating a diagrammatic summary of a set of documents each of which pertains to loosely related topics. In particular we aim at visualizing the medical histories of patients. In medicine choosing relevant reports from a patient&#39;s past exams for comparison provide valuable information for precise treatment planning. Manually finding the relevant reports for comparison studies from a large database is time-consuming which could result overlooking of some critical information. This task can be automated by defining similarity among documents which is a nontrivial task since these documents are often stored in an unstructured text format. To facilitate this we have used a representation learning algorithm that creates a semantic representation space for documents where the clinically related documents lie close to each other. We have utilized referral information to weakly supervise a LSTM network to learn this semantic space. The abstract representations within this semantic space are not only useful to visualize disease progressions corresponding to the relevant report groups of a patient but are also beneficial to analyze diseases at the population level. The proposed key tool here is clustering of documents based on the document similarity whose metric is learned from corpora.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Representation Learning for Patient History Visualization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C18-2007" target="_blank">https://aclanthology.org/C18-2007</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Automatically highlighting a text aims at identifying key portions that are the most important to a reader. In this paper we present a web-based framework designed to efficiently and scalably crowdsource two independent but related tasks: collecting highlight annotations and comparing the performance of automated highlighting systems. The first task is necessary to understand human preferences and train supervised automated highlighting systems. The second task yields a more accurate and fine-grained evaluation than existing automated performance metrics.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Web-based Framework for Collecting and Assessing Highlighted Sentences in a Document</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C18-2017" target="_blank">https://aclanthology.org/C18-2017</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Reading comprehension models are based on recurrent neural networks that sequentially process the document tokens. As interest turns to answering more complex questions over longer documents sequential reading of large portions of text becomes a substantial bottleneck. Inspired by how humans use document structure we propose a novel framework for reading comprehension. We represent documents as trees and model an agent that learns to interleave quick navigation through the document tree with more expensive answer extraction. To encourage exploration of the document tree we propose a new algorithm based on Deep Q-Network (DQN) which strategically samples tree nodes at training time. Empirically we find our algorithm improves question answering performance compared to DQN and a strong information-retrieval (IR) baseline and that ensembling our model with the IR baseline results in further gains in performance.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learning to Search in Long Documents Using Document Structure</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C18-1014" target="_blank">https://aclanthology.org/C18-1014</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we describe TextEnt a neural network model that learns distributed representations of entities and documents directly from a knowledge base (KB). Given a document in a KB consisting of words and entity annotations we train our model to predict the entity that the document describes and map the document and its target entity close to each other in a continuous vector space. Our model is trained using a large number of documents extracted from Wikipedia. The performance of the proposed model is evaluated using two tasks namely fine-grained entity typing and multiclass text classification. The results demonstrate that our model achieves state-of-the-art performance on both tasks. The code and the trained representations are made available online for further academic research.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Representation Learning of Entities and Documents from Knowledge Base Descriptions</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C18-1016" target="_blank">https://aclanthology.org/C18-1016</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper proposes an extractive multi-document summarization approach based on an ant colony system to optimize the information coverage of summary sentences. The implemented system was evaluated on both English and Arabic versions of the corpus of the Text Analysis Conference 2011 MultiLing Pilot by using ROUGE metrics. The evaluation results are promising in comparison to those of the participating systems. Indeed our system achieved the best scores based on several ROUGE metrics.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Ant Colony System for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C18-1062" target="_blank">https://aclanthology.org/C18-1062</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level multi-aspect sentiment classification aims to predict user&#39;s sentiment polarities for different aspects of a product in a review. Existing approaches mainly focus on text information. However the authors (i.e. users) and overall ratings of reviews are ignored both of which are proved to be significant on interpreting the sentiments of different aspects in this paper. Therefore we propose a model called Hierarchical User Aspect Rating Network (HUARN) to consider user preference and overall ratings jointly. Specifically HUARN adopts a hierarchical architecture to encode word sentence and document level information. Then user attention and aspect attention are introduced into building sentence and document level representation. The document representation is combined with user and overall rating information to predict aspect ratings of a review. Diverse aspects are treated differently and a multi-task framework is adopted. Empirical results on two real-world datasets show that HUARN achieves state-of-the-art performances.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-level Multi-aspect Sentiment Classification by Jointly Modeling Users Aspects and Overall Ratings</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C18-1079" target="_blank">https://aclanthology.org/C18-1079</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Generating an abstract from a collection of documents is a desirable capability for many real-world applications. However abstractive approaches to multi-document summarization have not been thoroughly investigated. This paper studies the feasibility of using Abstract Meaning Representation (AMR) a semantic representation of natural language grounded in linguistic theory as a form of content representation. Our approach condenses source documents to a set of summary graphs following the AMR formalism. The summary graphs are then transformed to a set of summary sentences in a surface realization step. The framework is fully data-driven and flexible. Each component can be optimized independently using small-scale in-domain training data. We perform experiments on benchmark summarization datasets and report promising results. We also describe opportunities and challenges for advancing this line of research.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Abstract Meaning Representation for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C18-1101" target="_blank">https://aclanthology.org/C18-1101</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this work we aim at developing an unsupervised abstractive summarization system in the multi-document setting. We design a paraphrastic sentence fusion model which jointly performs sentence fusion and paraphrasing using skip-gram word embedding model at the sentence level. Our model improves the information coverage and at the same time abstractiveness of the generated sentences. We conduct our experiments on the human-generated multi-sentence compression datasets and evaluate our system on several newly proposed Machine Translation (MT) evaluation metrics. Furthermore we apply our sentence level model to implement an abstractive multi-document summarization system where documents usually contain a related set of sentences. We also propose an optimal solution for the classical summary length limit problem which was not addressed in the past research. For the document level summary we conduct experiments on the datasets of two different domains (e.g. news article and user reviews) which are well suited for multi-document abstractive summarization. Our experiments demonstrate that the methods bring significant improvements over the state-of-the-art methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Abstractive Unsupervised Multi-Document Summarization using Paraphrastic Sentence Fusion</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C18-1102" target="_blank">https://aclanthology.org/C18-1102</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The rapid growth of documents across the web has necessitated finding means of discarding redundant documents and retaining novel ones. Capturing redundancy is challenging as it may involve investigating at a deep semantic level. Techniques for detecting such semantic redundancy at the document level are scarce. In this work we propose a deep Convolutional Neural Networks (CNN) based model to classify a document as novel or redundant with respect to a set of relevant documents already seen by the system. The system is simple and do not require any manual feature engineering. Our novel scheme encodes relevant and relative information from both source and target texts to generate an intermediate representation which we coin as the Relative Document Vector (RDV). The proposed method outperforms the existing state-of-the-art on a document-level novelty detection dataset by a margin of ∼5% in terms of accuracy. We further demonstrate the effectiveness of our approach on a standard paraphrase detection dataset where paraphrased passages closely resemble to semantically redundant documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Novelty Goes Deep. A Deep Neural Solution To Document Level Novelty Detection</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C18-1237" target="_blank">https://aclanthology.org/C18-1237</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Texts from the Internet serve as important data sources for financial market modeling. Early statistical approaches rely on manually defined features to capture lexical sentiment and event information which suffers from feature sparsity. Recent work has considered learning dense representations for news titles and abstracts. Compared to news titles full documents can contain more potentially helpful information but also noise compared to events and sentences which has been less investigated in previous work. To fill this gap we propose a novel target-specific abstract-guided news document representation model. The model uses a target-sensitive representation of the news abstract to weigh sentences in the news content so as to select and combine the most informative sentences for market modeling. Results show that document representations can give better performance for estimating cumulative abnormal returns of companies when compared to titles and abstracts. Our model is especially effective when it used to combine information from multiple document sources compared to the sentence-level baselines.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learning Target-Specific Representations of Financial News Documents For Cumulative Abnormal Return Prediction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C18-1239" target="_blank">https://aclanthology.org/C18-1239</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Le domaine m&#39;edical fait partie de la vie quotidienne pour des raisons de sant&#39;e mais la disponibilit&#39;e des informations m&#39;edicales ne garantit pas leur compr&#39;ehension correcte par les patients. Plusieurs &#39;etudes ont d&#39;emontr&#39;e qu&#39;il existe une difficult&#39;e r&#39;eelle dans la compr&#39;ehension de contenus m&#39;edicaux par les patients. Nous proposons d&#39;exploiter les m&#39;ethodes d&#39;oculom&#39;etrie pour &#39;etudier ces questions et pour d&#39;etecter quelles unit&#39;es linguistiques posent des difficult&#39;es de compr&#39;ehension. Pour cela des textes m&#39;edicaux en version originale et simplifi&#39;ee sont exploit&#39;es. L&#39;oculom&#39;etrie permet de suivre le regard des participants de l&#39;&#39;etude et de r&#39;ev&#39;eler les indicateurs de lecture comme la dur&#39;ee des fixations les r&#39;egressions et les saccades. Les r&#39;esultats indiquent qu&#39;il existe une diff&#39;erence statistiquement significative lors de la lecture des versions originales et simplifi&#39;ees des documents de sant&#39;e test&#39;es.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>&#39;Etude de la lisibilit&#39;e des documents de sant&#39;e avec des m&#39;ethodes d&#39;oculom&#39;etrie (Study of readability of health documents with eye-tracking methods)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2018.jeptalnrecital-long.1" target="_blank">https://aclanthology.org/2018.jeptalnrecital-long.1</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>L&#39;absence de donn&#39;ees annot&#39;ees peut ^etre une difficult&#39;e majeure lorsque l&#39;on s&#39;int&#39;eresse `a l&#39;analyse de documents manuscrits anciens. Pour contourner cette difficult&#39;e nous proposons de diviser le probl`eme en deux afin de pouvoir s&#39;appuyer sur des donn&#39;ees plus facilement accessibles. Dans cet article nous pr&#39;esentons la partie d&#39;ecodeur d&#39;un encodeur-d&#39;ecodeur multimodal utilisant l&#39;apprentissage par transfert de connaissances pour la transcription des titres de pi`eces de la Com&#39;edie Italienne. Le d&#39;ecodeur transforme un vecteur de n-grammes au niveau caract`eres en une s&#39;equence de caract`eres correspondant `a un mot. L&#39;apprentissage par transfert de connaissances est r&#39;ealis&#39;e principalement `a partir d&#39;une nouvelle ressource inexploit&#39;ee contemporaine `a la Com&#39;edie-Italienne et th&#39;ematiquement proche ; ainsi que d&#39;autres ressources couvrant d&#39;autres domaines des langages diff&#39;erents et m^eme des p&#39;eriodes diff&#39;erentes. Nous obtenons 9727% de caract`eres bien reconnus sur les donn&#39;ees de la Com&#39;edie-Italienne ainsi que 8657% de mots correctement g&#39;en&#39;er&#39;es malgr&#39;e une couverture de 6758% uniquement entre la Com&#39;edie-Italienne et l&#39;ensemble d&#39;apprentissage. Les exp&#39;eriences montrent qu&#39;un tel syst`eme peut ^etre une approche efficace dans le cadre d&#39;apprentissage par transfert.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>D&#39;ecodeur neuronal pour la transcription de documents manuscrits anciens (Neural decoder for the transcription of historical handwritten documents)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2018.jeptalnrecital-long.14" target="_blank">https://aclanthology.org/2018.jeptalnrecital-long.14</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Dans ce papier nous d&#39;ecrivons notre participation au d&#39;efi d&#39;analyse de texte DEFT 2018. Nous avons particip&#39;e `a deux t^aches : (i) classification transport/non-transport et (ii) analyse de polarit&#39;e globale des tweets : positifs negatifs neutres et mixtes. Nous avons exploit&#39;e un r&#39;eseau de neurone bas&#39;e sur un perceptron multicouche mais utilisant une seule couche cach&#39;ee.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>LIRMM@DEFT-2018 -- Mod`ele de classification de la vectorisation des documents (LIRMM DEFT-2018 -- Document Vectorization Classification model )</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2018.jeptalnrecital-deft.11" target="_blank">https://aclanthology.org/2018.jeptalnrecital-deft.11</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Le traitement `a posteriori de transcriptions OCR cherche `a d&#39;etecter les erreurs dans les sorties d&#39;OCR pour tenter de les corriger deux t^aches &#39;evalu&#39;ees par la comp&#39;etition ICDAR-2017 Post-OCR Text Correction. Nous pr&#39;esenterons dans ce papier un syst`eme de d&#39;etection d&#39;erreurs bas&#39;e sur un mod`ele `a r&#39;eseaux r&#39;ecurrents combinant une analyse du texte au niveau des mots et des caract`eres en deux temps. Ce syst`eme a &#39;et&#39;e class&#39;e second dans trois cat&#39;egories &#39;evalu&#39;ees parmi 11 candidats lors de la comp&#39;etition.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>D&#39;etection d&#39;erreurs dans des transcriptions OCR de documents historiques par r&#39;eseaux de neurones r&#39;ecurrents multi-niveau (Combining character level and word level RNNs for post-OCR error detection)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2018.jeptalnrecital-court.5" target="_blank">https://aclanthology.org/2018.jeptalnrecital-court.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Human language evolves with the passage of time. This makes historical documents to be hard to comprehend by contemporary people and thus limits their accessibility to scholars specialized in the time period in which a certain document was written. Modernization aims at breaking this language barrier and increase the accessibility of historical documents to a broader audience. To do so it generates a new version of a historical document written in the modern version of the document&#39;s original language. In this work we propose several machine translation approaches for modernizing historical documents. We tested these approaches in different scenarios obtaining very encouraging results.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Machine Translation Approach for Modernizing Historical Documents Using Backtranslation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2018.iwslt-1.6" target="_blank">https://aclanthology.org/2018.iwslt-1.6</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2018</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The meaning of a sentence in a document is more easily determined if its constituent words exhibit cohesion with respect to their individual semantics. This paper explores the degree of cohesion among a document&#39;s words using lexical chains as a semantic representation of its meaning. Using a combination of diverse types of lexical chains we develop a text document representation that can be used for semantic document retrieval. For our approach we develop two kinds of lexical chains: (i) a multilevel flexible chain representation of the extracted semantic values which is used to construct a fixed segmentation of these chains and constituent words in the text; and (ii) a fixed lexical chain obtained directly from the initial semantic representation from a document. The extraction and processing of concepts is performed using WordNet as a lexical database. The segmentation then uses these lexical chains to model the dispersion of concepts in the document. Representing each document as a high-dimensional vector we use spherical k-means clustering to demonstrate that our approach performs better than previous techniques.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Semantic Feature Structure Extraction From Documents Based on Extended Lexical Chains</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2018.gwc-1.11" target="_blank">https://aclanthology.org/2018.gwc-1.11</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Categorization of Tagalog Documents Using Support Vector Machines</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y17-1046" target="_blank">https://aclanthology.org/Y17-1046</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Whenever employed on large datasets information retrieval works by isolating a subset of documents from the larger dataset and then proceeding with low-level processing of the text. This is usually carried out by means of adding index-terms to each document in the collection. In this paper we deal with automatic document classification and index-term detection applied on large-scale medical corpora. In our methodology we employ a linear classifier and we test our results on the BioASQ training corpora which is a collection of 12 million MeSH-indexed medical abstracts. We cover both term-indexing result retrieval and result ranking based on distributed word representations.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document retrieval and question answering in medical documents. A large-scale corpus challenge.</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://doi.org/10.26615/978-954-452-044-1_001" target="_blank">https://doi.org/10.26615/978-954-452-044-1_001</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Level Novelty Detection: Textual Entailment Lends a Helping Hand</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-7517" target="_blank">https://aclanthology.org/W17-7517</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Embedding Generation for Cyber-Aggressive Comment Detection using Supervised Machine Learning Approach</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-7543" target="_blank">https://aclanthology.org/W17-7543</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The Effect of Negative Sampling Strategy on Capturing Semantic Similarity in Document Embeddings</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-7301" target="_blank">https://aclanthology.org/W17-7301</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>TBX in ODD: Schema-agnostic specification and documentation for TermBase eXchange</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-7006" target="_blank">https://aclanthology.org/W17-7006</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Influ^encia de T&#39;ecnicas N~ao-supervisionadas de Reducc~ao de Dimensionalidade para Organizacc~ao Flex&#39;ivel de Documentos (The Unsupervised Dimensionality Reduction Weight on Flexible Document Organization)[In Portuguese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-6614" target="_blank">https://aclanthology.org/W17-6614</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper challenges a cross-genre document retrieval task where the queries are in formal writing and the target documents are in conversational writing. In this task a query is a sentence extracted from either a summary or a plot of an episode in a TV show and the target document consists of transcripts from the corresponding episode. To establish a strong baseline we employ the current state-of-the-art search engine to perform document retrieval on the dataset collected for this work. We then introduce a structure reranking approach to improve the initial ranking by utilizing syntactic and semantic structures generated by NLP tools. Our evaluation shows an improvement of more than 4% when the structure reranking is applied which is very promising.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-genre Document Retrieval: Matching between Conversational and Formal Writings</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-5407" target="_blank">https://aclanthology.org/W17-5407</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We propose a method for the annotation of Japanese civil judgment documents with the purpose of creating flexible summaries of these. The first step described in the current paper concerns content selection i.e. the question of which material should be extracted initially for the summary. In particular we utilize the hierarchical argument structure of the judgment documents. Our main contributions are a) the design of an annotation scheme that stresses the connection between legal points (called issue topics) and argument structure b) an adaptation of rhetorical status to suit the Japanese legal system and c) the definition of a linked argument structure based on legal sub-arguments. In this paper we report agreement between two annotators on several aspects of the overall task.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Annotation of argument structure in Japanese legal documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-5103" target="_blank">https://aclanthology.org/W17-5103</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Characterizing the content of a technical document in terms of its learning utility can be useful for applications related to education such as generating reading lists from large collections of documents. We refer to this learning utility as the ``pedagogical value&#39;&#39; of the document to the learner. While pedagogical value is an important concept that has been studied extensively within the education domain there has been little work exploring it from a computational i.e. natural language processing (NLP) perspective. To allow a computational exploration of this concept we introduce the notion of ``pedagogical roles&#39;&#39; of documents (e.g. Tutorial and Survey) as an intermediary component for the study of pedagogical value. Given the lack of available corpora for our exploration we create the first annotated corpus of pedagogical roles and use it to test baseline techniques for automatic prediction of such roles.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Investigation into the Pedagogical Features of Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-5012" target="_blank">https://aclanthology.org/W17-5012</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we describe the approach of the ItaliaNLP Lab team to native language identification and discuss the results we submitted as participants to the essay track of NLI Shared Task 2017. We introduce for the first time a 2-stacked sentence-document architecture for native language identification that is able to exploit both local sentence information and a wide set of general-purpose features qualifying the lexical and grammatical structure of the whole document. When evaluated on the official test set our sentence-document stacked architecture obtained the best result among all the participants of the essay track with an F1 score of 0.8818.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Stacked Sentence-Document Classifier Approach for Improving Native Language Identification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-5049" target="_blank">https://aclanthology.org/W17-5049</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Although coherence is an important aspect of any text generation system it has received little attention in the context of machine translation (MT) so far. We hypothesize that the quality of document-level translation can be improved if MT models take into account the semantic relations among sentences during translation. We integrate the graph-based coherence model proposed by Mesgar and Strube (2016) with Docent (Hardmeier et al. 2012 Hardmeier 2014) a document-level machine translation system. The application of this graph-based coherence modeling approach is novel in the context of machine translation. We evaluate the coherence model and its effects on the quality of the machine translation. The result of our experiments shows that our coherence model slightly improves the quality of translation in terms of the average Meteor score.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using a Graph-based Coherence Model in Document-Level Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-4803" target="_blank">https://aclanthology.org/W17-4803</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Currently under review for EMNLP 2017 The phrase-based Statistical Machine Translation (SMT) approach deals with sentences in isolation making it difficult to consider discourse context in translation. This poses a challenge for ambiguous words that need discourse knowledge to be correctly translated. We propose a method that benefits from the semantic similarity in lexical chains to improve SMT output by integrating it in a document-level decoder. We focus on word embeddings to deal with the lexical chains contrary to the traditional approach that uses lexical resources. Experimental results on German-to-English show that our method produces correct translations in up to 88% of the changes improving the translation in 36%-48% of them over the baseline.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Lexical Chains meet Word Embeddings in Document-level Statistical Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-4813" target="_blank">https://aclanthology.org/W17-4813</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Vast amounts of speech data collected for language documentation and research remain untranscribed and unsearchable but often a small amount of speech may have text translations available. We present a method for partially labeling additional speech with translations in this scenario. We modify an unsupervised speech-to-translation alignment model and obtain prototype speech segments that match the translation words which are in turn used to discover terms in the unlabelled data. We evaluate our method on a Spanish-English speech translation corpus and on two corpora of endangered languages Arapaho and Ainu demonstrating its appropriateness and applicability in an actual very-low-resource scenario.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Spoken Term Discovery for Language Documentation using Translations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-4607" target="_blank">https://aclanthology.org/W17-4607</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Sequence-to-sequence models with attention have been successful for a variety of NLP problems but their speed does not scale well for tasks with long source sequences such as document summarization. We propose a novel coarse-to-fine attention model that hierarchically reads a document using coarse attention to select top-level chunks of text and fine attention to read the words of the chosen chunks. While the computation for training standard attention models scales linearly with source sequence length our method scales with the number of top-level chunks and can handle much longer sequences. Empirically we find that while coarse-to-fine attention models lag behind state-of-the-art baselines our method achieves the desired behavior of sparsely attending to subsets of the document for generation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Coarse-to-Fine Attention Models for Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-4505" target="_blank">https://aclanthology.org/W17-4505</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The centroid-based model for extractive document summarization is a simple and fast baseline that ranks sentences based on their similarity to a centroid vector. In this paper we apply this ranking to possible summaries instead of sentences and use a simple greedy algorithm to find the best summary. Furthermore we show possibilities to scale up to larger input document collections by selecting a small number of sentences from each document prior to constructing the summary. Experiments were done on the DUC2004 dataset for multi-document summarization. We observe a higher performance over the original model on par with more complex state-of-the-art methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Revisiting the Centroid-based Method: A Strong Baseline for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-4511" target="_blank">https://aclanthology.org/W17-4511</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. To conduct evaluation for summarization performance we prepare a new dataset. We describe the methods for data collection aspect annotation and summary writing as well as scrutinizing by experts. Experimental results show that reader comments can improve the summarization performance which also demonstrates the usefulness of the proposed dataset.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-4512" target="_blank">https://aclanthology.org/W17-4512</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Technical documents contain a fair amount of unnatural language such as tables formulas and pseudo-code. Unnatural language can bean important factor of confusing existing NLP tools. This paper presents an effective method of distinguishing unnatural language from natural language and evaluates the impact of un-natural language detection on NLP tasks such as document clustering. We view this problem as an information extraction task and build a multiclass classification model identifying unnatural language components into four categories. First we create a new annotated corpus by collecting slides and papers in various for-mats PPT PDF and HTML where unnatural language components are annotated into four categories. We then explore features available from plain text to build a statistical model that can handle any format as long as it is converted into plain text. Our experiments show that re-moving unnatural language components gives an absolute improvement in document cluster-ing by up to 15%. Our corpus and tool are publicly available</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Document Clustering by Removing Unnatural Language</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-4416" target="_blank">https://aclanthology.org/W17-4416</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Tagging news articles or blog posts with relevant tags from a collection of predefined ones is coined as document tagging in this work. Accurate tagging of articles can benefit several downstream applications such as recommendation and search. In this work we propose a novel yet simple approach called DocTag2Vec to accomplish this task. We substantially extend Word2Vec and Doc2Vec -- two popular models for learning distributed representation of words and documents. In DocTag2Vec we simultaneously learn the representation of words documents and tags in a joint vector space during training and employ the simple k-nearest neighbor search to predict tags for unseen documents. In contrast to previous multi-label learning methods DocTag2Vec directly deals with raw text instead of provided feature vector and in addition enjoys advantages like the learning of tag representation and the ability of handling newly created tags. To demonstrate the effectiveness of our approach we conduct experiments on several datasets and show promising results against state-of-the-art methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DocTag2Vec: An Embedding Based Multi-label Learning Approach for Document Tagging</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-2614" target="_blank">https://aclanthology.org/W17-2614</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We investigate the pertinence of methods from algebraic topology for text data analysis. These methods enable the development of mathematically-principled isometric-invariant mappings from a set of vectors to a document embedding which is stable with respect to the geometry of the document in the selected metric space. In this work we evaluate the utility of these topology-based document representations in traditional NLP tasks specifically document clustering and sentiment classification. We find that the embeddings do not benefit text analysis. In fact performance is worse than simple techniques like tf-idf indicating that the geometry of the document does not provide enough variability for classification on the basis of topic or sentiment in the chosen datasets.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Does the Geometry of Word Embeddings Help Document Classification? A Case Study on Persistent Homology-Based Representations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-2628" target="_blank">https://aclanthology.org/W17-2628</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this work we aim at developing an extractive summarizer in the multi-document setting. We implement a rank based sentence selection using continuous vector representations along with key-phrases. Furthermore we propose a model to tackle summary coherence for increasing readability. We conduct experiments on the Document Understanding Conference (DUC) 2004 datasets using ROUGE toolkit. Our experiments demonstrate that the methods bring significant improvements over the state of the art methods in terms of informativity and coherence.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extract with Order for Coherent Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-2407" target="_blank">https://aclanthology.org/W17-2407</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we present pilot work on characterising the documentation of electronic cigarettes (e-cigarettes) in the United States Veterans Administration Electronic Health Record. The Veterans Health Administration is the largest health care system in the United States with 1233 health care facilities nationwide serving 8.9 million veterans per year. We identified a random sample of 2000 Veterans Administration patients coded as current tobacco users from 2008 to 2014. Using simple keyword matching techniques combined with qualitative analysis we investigated the prevalence and distribution of e-cigarette terms in these clinical notes discovering that for current smokers 11.9% of patient records contain an e-cigarette related term.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Investigating the Documentation of Electronic Cigarette Use in the Veteran Affairs Electronic Health Record: A Pilot Study</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-2335" target="_blank">https://aclanthology.org/W17-2335</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we present ongoing work for developing language resources and basic NLP tools for an undocumented variety of Romansh in the context of a language documentation and language acquisition project. Our tools are meant to improve the speed and reliability of corpus annotations for noisy data involving large amounts of code-switching occurrences of child-speech and orthographic noise. Being able to increase the efficiency of language resource development for language documentation and acquisition research also constitutes a step towards solving the data sparsity issues with which researchers have been struggling.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Speeding up corpus development for linguistic research: language documentation and acquisition in Romansh Tuatschin</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-2212" target="_blank">https://aclanthology.org/W17-2212</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We investigate animal recognition models learned from wildlife video documentaries by using the weak supervision of the textual subtitles. This is a particularly challenging setting since i) the animals occur in their natural habitat and are often largely occluded and ii) subtitles are to a large degree complementary to the visual content providing a very weak supervisory signal. This is in contrast to most work on integrated vision and language in the literature where textual descriptions are tightly linked to the image content and often generated in a curated fashion for the task at hand. In particular we investigate different image representations and models including a support vector machine on top of activations of a pretrained convolutional neural network as well as a Naive Bayes framework on a `bag-of-activations&#39; image representation where each element of the bag is considered separately. This representation allows key components in the image to be isolated in spite of largely varying backgrounds and image clutter without an object detection or image segmentation step. The methods are evaluated based on how well they transfer to unseen camera-trap images captured across diverse topographical regions under different environmental conditions and illumination settings involving a large domain shift.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learning to Recognize Animals by Watching Documentaries: Using Subtitles as Weak Supervision</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-2003" target="_blank">https://aclanthology.org/W17-2003</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents a language identification system designed to detect the language of each word in its context in a multilingual documents as generated in social media by bilingual/multilingual communities in our case speakers of Algerian Arabic. We frame the task as a sequence tagging problem and use supervised machine learning with standard methods like HMM and Ngram classification tagging. We also experiment with a lexicon-based method. Combining all the methods in a fall-back mechanism and introducing some linguistic rules to deal with unseen tokens and ambiguous words gives an overall accuracy of 93.14%. Finally we introduced rules for language identification from sequences of recognised words.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Identification of Languages in Algerian Arabic Multilingual Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-1301" target="_blank">https://aclanthology.org/W17-1301</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Abstractive document summarization seeks to automatically generate a summary for a document based on some abstract ``understanding&#39;&#39; of the original document. State-of-the-art techniques traditionally use attentive encoder--decoder architectures. However due to the large number of parameters in these models they require large training datasets and long training times. In this paper we propose decoupling the encoder and decoder networks and training them separately. We encode documents using an unsupervised document encoder and then feed the document vector to a recurrent neural network decoder. With this decoupled architecture we decrease the number of parameters in the decoder substantially and shorten its training time. Experiments show that the decoupled model achieves comparable performance with state-of-the-art models for in-domain documents but less well for out-of-domain documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Decoupling Encoder and Decoder Networks for Abstractive Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-1002" target="_blank">https://aclanthology.org/W17-1002</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Instant Annotations -- Applying NLP Methods to the Annotation of Spoken Language Documentation Corpora</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-0604" target="_blank">https://aclanthology.org/W17-0604</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Docforia: A Multilayer Document Model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-0227" target="_blank">https://aclanthology.org/W17-0227</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A System for Identifying and Exploring Text Repetition in Large Historical Document Corpora</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-0249" target="_blank">https://aclanthology.org/W17-0249</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Connecting Documentation and Revitalization: A New Approach to Language Apps</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-0120" target="_blank">https://aclanthology.org/W17-0120</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Developing a Suite of Mobile Applications for Collaborative Language Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-0121" target="_blank">https://aclanthology.org/W17-0121</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A case study on using speech-to-translation alignments for language documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W17-0123" target="_blank">https://aclanthology.org/W17-0123</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Joint Sentence-Document Model for Manifesto Text Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/U17-1003" target="_blank">https://aclanthology.org/U17-1003</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents our relation extraction system for subtask C of SemEval-2017 Task 10: ScienceIE. Assuming that the keyphrases are already annotated in the input data our work explores a wide range of linguistic features applies various feature selection techniques optimizes the hyper parameters and class weights and experiments with different problem formulations (single classification model vs individual classifiers for each keyphrase type single-step classifier vs pipeline classifier for hyponym relations). Performance of five popular classification algorithms are evaluated for each problem formulation along with feature selection. The best setting achieved an F1 score of 71.0% for synonym and 30.0% for hyponym relation on the test data.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>NTNU-2 at SemEval-2017 Task 10: Identifying Synonym and Hyponym Relations among Keyphrases in Scientific Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/S17-2168" target="_blank">https://aclanthology.org/S17-2168</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>A multi-document summarizer finds the key topics from multiple textual sources and organizes information around them. In this paper we propose a summarization method for Persian text using paragraph vectors that can represent textual units of arbitrary lengths. We use these vectors to calculate the semantic relatedness between documents cluster them to a number of predetermined groups weight them based on their distance to the centroids and the intra-cluster homogeneity and take out the key paragraphs. We compare the final summaries with the gold-standard summaries of 21 digital topics using the ROUGE evaluation metric. Experimental results show the advantages of using paragraph vectors over earlier attempts at developing similar methods for a low resource language like Persian.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Document Summarization of Persian Text using Paragraph Vectors</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://doi.org/10.26615/issn.1314-9156.2017_005" target="_blank">https://doi.org/10.26615/issn.1314-9156.2017_005</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Nowadays search for documents on the Internet is becoming increasingly difficult. The reason is the amount of content published by users (articles comments blogs reviews). How to facilitate that the users can find their required documents? What would be necessary to provide useful document meta-data for supporting search engines? In this article we present a study of some Natural Language Processing (NLP) technologies that can be useful for facilitating the proper identification of documents according to the user needs. For this purpose it is designed a document profile that will be able to represent semantic meta-data extracted from documents by using NLP technologies. The research is basically focused on the study of different NLP technologies in order to support the creation our novel document profile proposal from semantic perspectives.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Natural Language Processing Technologies for Document Profiling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://doi.org/10.26615/978-954-452-049-6_039" target="_blank">https://doi.org/10.26615/978-954-452-049-6_039</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we analyze and evaluate word embeddings for representation of longer texts in the multi-label classification scenario. The embeddings are used in three convolutional neural network topologies. The experiments are realized on the Czech vCTK and English Reuters-21578 standard corpora. We compare the results of word2vec static and trainable embeddings with randomly initialized word vectors. We conclude that initialization does not play an important role for classification. However learning of word vectors is crucial to obtain good results.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Word Embeddings for Multi-label Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://doi.org/10.26615/978-954-452-049-6_057" target="_blank">https://doi.org/10.26615/978-954-452-049-6_057</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Text-based Speaker Identification on Multiparty Dialogues Using Multi-document Convolutional Neural Networks</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P17-3009" target="_blank">https://aclanthology.org/P17-3009</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Discourse segmentation is a crucial step in building end-to-end discourse parsers. However discourse segmenters only exist for a few languages and domains. Typically they only detect intra-sentential segment boundaries assuming gold standard sentence and token segmentation and relying on high-quality syntactic parses and rich heuristics that are not generally available across languages and domains. In this paper we propose statistical discourse segmenters for five languages and three domains that do not rely on gold pre-annotations. We also consider the problem of learning discourse segmenters when no labeled data is available for a language. Our fully supervised system obtains 89.5% F1 for English newswire with slight drops in performance on other domains and we report supervised and unsupervised (cross-lingual) results for five languages in total.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-lingual and cross-domain discourse segmentation of entire documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P17-2037" target="_blank">https://aclanthology.org/P17-2037</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a framework for question answering that can efficiently scale to longer documents while maintaining or even improving performance of state-of-the-art models. While most successful approaches for reading comprehension rely on recurrent neural networks (RNNs) running them over long documents is prohibitively slow because it is difficult to parallelize over sequences. Inspired by how people first skim the document identify relevant parts and carefully read these parts to produce an answer we combine a coarse fast model for selecting relevant sentences and a more expensive RNN for producing the answer from those sentences. We treat sentence selection as a latent variable trained jointly from the answer only using reinforcement learning. Experiments demonstrate state-of-the-art performance on a challenging subset of the WikiReading dataset and on a new dataset while speeding up the model by 3.5x-6.7x.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Coarse-to-Fine Question Answering for Long Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P17-1020" target="_blank">https://aclanthology.org/P17-1020</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a new supervised framework that learns to estimate automatic Pyramid scores and uses them for optimization-based extractive multi-document summarization. For learning automatic Pyramid scores we developed a method for automatic training data generation which is based on a genetic algorithm using automatic Pyramid as the fitness function. Our experimental evaluation shows that our new framework significantly outperforms strong baselines regarding automatic Pyramid and that there is much room for improvement in comparison with the upper-bound for automatic Pyramid.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Supervised Learning of Automatic Pyramid for Optimization-Based Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P17-1100" target="_blank">https://aclanthology.org/P17-1100</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The large and growing amounts of online scholarly data present both challenges and opportunities to enhance knowledge discovery. One such challenge is to automatically extract a small set of keyphrases from a document that can accurately describe the document&#39;s content and can facilitate fast information processing. In this paper we propose PositionRank an unsupervised model for keyphrase extraction from scholarly documents that incorporates information from all positions of a word&#39;s occurrences into a biased PageRank. Our model obtains remarkable improvements in performance over PageRank models that do not take into account word positions as well as over strong baselines for this task. Specifically on several datasets of research papers PositionRank achieves improvements as high as 29.09%.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>PositionRank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P17-1102" target="_blank">https://aclanthology.org/P17-1102</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Abstractive summarization is the ultimate goal of document summarization research but previously it is less investigated due to the immaturity of text generation techniques. Recently impressive progress has been made to abstractive sentence summarization using neural models. Unfortunately attempts on abstractive document summarization are still in a primitive stage and the evaluation results are worse than extractive methods on benchmark datasets. In this paper we review the difficulties of neural abstractive document summarization and propose a novel graph-based attention mechanism in the sequence-to-sequence framework. The intuition is to address the saliency factor of summarization which has been overlooked by prior works. Experimental results demonstrate our model is able to achieve considerable improvement over previous neural abstractive models. The data-driven neural abstractive method is also competitive with state-of-the-art extractive methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Abstractive Document Summarization with a Graph-Based Attentional Neural Model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P17-1108" target="_blank">https://aclanthology.org/P17-1108</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we propose an extractive multi-document summarization (MDS) system using joint optimization and active learning for content selection grounded in user feedback. Our method interactively obtains user feedback to gradually improve the results of a state-of-the-art integer linear programming (ILP) framework for MDS. Our methods complement fully automatic methods in producing high-quality summaries with a minimum number of iterations and feedbacks. We conduct multiple simulation-based experiments and analyze the effect of feedback-based concept selection in the ILP setup in order to maximize the user-desired content in the summary.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Joint Optimization of User-desired Content in Multi-document Summaries by Learning from User Feedback</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P17-1124" target="_blank">https://aclanthology.org/P17-1124</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We consider the problem of translating high-level textual descriptions to formal representations in technical documentation as part of an effort to model the meaning of such documentation. We focus specifically on the problem of learning translational correspondences between text descriptions and grounded representations in the target documentation such as formal representation of functions or code templates. Our approach exploits the parallel nature of such documentation or the tight coupling between high-level text and the low-level representations we aim to learn. Data is collected by mining technical documents for such parallel text-representation pairs which we use to train a simple semantic parsing model. We report new baseline results on sixteen novel datasets including the standard library documentation for nine popular programming languages across seven natural languages and a small collection of Unix utility manuals.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learning Semantic Correspondences in Technical Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P17-1148" target="_blank">https://aclanthology.org/P17-1148</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>語音文件檢索使用類神經網路技術 (On the Use of Neural Network Modeling Techniques for Spoken Document Retrieval) [In Chinese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O17-3002" target="_blank">https://aclanthology.org/O17-3002</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>使用查詢意向探索與類神經網路於語音文件檢索之研究 (Exploring Query Intent and Neural Network modeling Techniques for Spoken Document Retrieval) [In Chinese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O17-1015" target="_blank">https://aclanthology.org/O17-1015</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Topic models jointly learn topics and document-level topic distribution. Extrinsic evaluation of topic models tends to focus exclusively on topic-level evaluation e.g. by assessing the coherence of topics. We demonstrate that there can be large discrepancies between topic- and document-level model quality and that basing model evaluation on topic-level analysis can be highly misleading. We propose a method for automatically predicting topic model quality based on analysis of document-level topic allocations and provide empirical evidence for its robustness.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Automatic Approach for Document-level Topic Model Evaluation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/K17-1022" target="_blank">https://aclanthology.org/K17-1022</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We propose a neural multi-document summarization system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences that avoid redundancy. In our experiments on DUC 2004 we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon other traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph and it achieves competitive results against other state-of-the-art multi-document summarization systems.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Graph-based Neural Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/K17-1045" target="_blank">https://aclanthology.org/K17-1045</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Neural vector representations are now ubiquitous in all subfields of natural language processing and text mining. While methods such as word2vec and GloVe are well-known this tutorial focuses on multilingual and cross-lingual vector representations of words but also of sentences and documents as well.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilingual Vector Representations of Words Sentences and Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I17-5002" target="_blank">https://aclanthology.org/I17-5002</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We introduce MASSAlign: a Python library for the alignment and annotation of monolingual comparable documents. MASSAlign offers easy-to-use access to state of the art algorithms for paragraph and sentence-level alignment as well as novel algorithms for word-level annotation of transformation operations between aligned sentences. In addition MASSAlign provides a visualization module to display and analyze the alignments and annotations performed.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MASSAlign: Alignment and Annotation of Comparable Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I17-3001" target="_blank">https://aclanthology.org/I17-3001</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Automated documentation of programming source code and automated code generation from natural language are challenging tasks of both practical and scientific interest. Progress in these areas has been limited by the low availability of parallel corpora of code and natural language descriptions which tend to be small and constrained to specific domains. In this work we introduce a large and diverse parallel corpus of a hundred thousands Python functions with their documentation strings (``docstrings&#39;&#39;) generated by scraping open source repositories on GitHub. We describe baseline results for the code documentation and code generation tasks obtained by neural machine translation. We also experiment with data augmentation techniques to further increase the amount of training data. We release our datasets and processing scripts in order to stimulate research in these areas.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I17-2053" target="_blank">https://aclanthology.org/I17-2053</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We propose a submodular function-based summarization system which integrates three important measures namely importance coverage and non-redundancy to detect the important sentences for the summary. We design monotone and submodular functions which allow us to apply an efficient and scalable greedy algorithm to obtain informative and well-covered summaries. In addition we integrate two abstraction-based methods namely sentence compression and merging for generating an abstractive sentence set. We design our summarization models for both generic and query-focused summarization. Experimental results on DUC-2004 and DUC-2007 datasets show that our generic and query-focused summarizers have outperformed the state-of-the-art summarization systems in terms of ROUGE-1 and ROUGE-2 recall and F-measure.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards Abstractive Multi-Document Summarization Using Submodular Function-Based Framework Sentence Compression and Merging</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I17-2071" target="_blank">https://aclanthology.org/I17-2071</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper tackles the task of event detection which involves identifying and categorizing events. The previous work mainly exist two problems: (1) the traditional feature-based methods apply cross-sentence information yet need taking a large amount of human effort to design complicated feature sets and inference rules; (2) the representation-based methods though overcome the problem of manually extracting features while just depend on local sentence representation. Considering local sentence context is insufficient to resolve ambiguities in identifying particular event types therefore we propose a novel document level Recurrent Neural Networks (DLRNN) model which can automatically extract cross-sentence clues to improve sentence level event detection without designing complex reasoning rules. Experiment results show that our approach outperforms other state-of-the-art methods on ACE 2005 dataset without external knowledge base.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploiting Document Level Information to Improve Event Detection via Recurrent Neural Networks</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I17-1036" target="_blank">https://aclanthology.org/I17-1036</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level sentiment classification aims to assign the user reviews a sentiment polarity. Previous methods either just utilized the document content without consideration of user and product information or did not comprehensively consider what roles the three kinds of information play in text modeling. In this paper to reasonably use all the information we present the idea that user product and their combination can all influence the generation of attentions to words and sentences when judging the sentiment of a document. With this idea we propose a cascading multiway attention (CMA) model where multiple ways of using user and product information are cascaded to influence the generation of attentions on the word and sentence layers. Then sentences and documents are well modeled by multiple representation vectors which provide rich information for sentiment classification. Experiments on IMDB and Yelp datasets demonstrate the effectiveness of our model.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cascading Multiway Attentions for Document-level Sentiment Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I17-1064" target="_blank">https://aclanthology.org/I17-1064</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we study domain adaptation with a state-of-the-art hierarchical neural network for document-level sentiment classification. We first design a new auxiliary task based on sentiment scores of domain-independent words. We then propose two neural network architectures to respectively induce document embeddings and sentence embeddings that work well for different domains. When these document and sentence embeddings are used for sentiment classification we find that with both pseudo and external sentiment lexicons our proposed methods can perform similarly to or better than several highly competitive domain adaptation methods on a benchmark dataset of product reviews.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Leveraging Auxiliary Tasks for Document-Level Cross-Domain Sentiment Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I17-1066" target="_blank">https://aclanthology.org/I17-1066</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Concept-map-based multi-document summarization is a variant of traditional summarization that produces structured summaries in the form of concept maps. In this work we propose a new model for the task that addresses several issues in previous methods. It learns to identify and merge coreferent concepts to reduce redundancy determines their importance with a strong supervised model and finds an optimal summary concept map via integer linear programming. It is also computationally more efficient than previous methods allowing us to summarize larger document sets. We evaluate the model on two datasets finding that it outperforms several approaches from previous work.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Concept-Map-Based Multi-Document Summarization using Concept Coreference Resolution and Global Importance Optimization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I17-1081" target="_blank">https://aclanthology.org/I17-1081</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Existing work for abstractive multidocument summarization utilise existing phrase structures directly extracted from input documents to generate summary sentences. These methods can suffer from lack of consistence and coherence in merging phrases. We introduce a novel approach for abstractive multidocument summarization through partial dependency tree extraction recombination and linearization. The method entrusts the summarizer to generate its own topically coherent sequential structures from scratch for effective communication. Results on TAC 2011 DUC-2004 and 2005 show that our system gives competitive results compared with state of the art abstractive summarization approaches in the literature. We also achieve competitive results in linguistic quality assessed by human evaluators.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Abstractive Multi-document Summarization by Partial Tree Extraction Recombination and Linearization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I17-1082" target="_blank">https://aclanthology.org/I17-1082</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we model the document revision detection problem as a minimum cost branching problem that relies on computing document distances. Furthermore we propose two new document distance measures word vector-based Dynamic Time Warping (wDTW) and word vector-based Tree Edit Distance (wTED). Our revision detection system is designed for a large scale corpus and implemented in Apache Spark. We demonstrate that our system can more precisely detect revisions than state-of-the-art methods by utilizing the Wikipedia revision dumps and simulated data sets.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Semantic Document Distance Measures and Unsupervised Document Revision Detection</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I17-1095" target="_blank">https://aclanthology.org/I17-1095</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Hierarchical attention networks have recently achieved remarkable performance for document classification in a given language. However when multilingual document collections are considered training such models separately for each language entails linear parameter growth and lack of cross-language transfer. Learning a single multilingual model with fewer parameters is therefore a challenging but potentially beneficial objective. To this end we propose multilingual hierarchical attention networks for learning document structures with shared encoders and/or shared attention mechanisms across languages using multi-task learning and an aligned semantic space as input. We evaluate the proposed models on multilingual document classification with disjoint label sets on a large dataset which we provide with 600k news documents in 8 languages and 5k labels. The multilingual models outperform monolingual ones in low-resource as well as full-resource settings and use fewer parameters thus confirming their computational efficiency and the utility of cross-language transfer.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilingual Hierarchical Attention Networks for Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I17-1102" target="_blank">https://aclanthology.org/I17-1102</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Meaningful conclusions about the relative performance of NLP systems are only possible if the gold standard employed in a given evaluation is both valid and reliable. In this paper we explore the validity of human annotations currently employed in the evaluation of document-level quality estimation for machine translation (MT). We demonstrate the degree to which MT system rankings are dependent on weights employed in the construction of the gold standard before proposing direct human assessment as a valid alternative. Experiments show direct assessment (DA) scores for documents to be highly reliable achieving a correlation of above 0.9 in a self-replication experiment in addition to a substantial estimated cost reduction through quality controlled crowd-sourcing. The original gold standard based on post-edits incurs a 10--20 times greater cost than DA.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Evaluation of Document-level Machine Translation Quality Estimation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/E17-2057" target="_blank">https://aclanthology.org/E17-2057</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recently there has been a lot of activity in learning distributed representations of words in vector spaces. Although there are models capable of learning high-quality distributed representations of words how to generate vector representations of the same quality for phrases or documents still remains a challenge. In this paper we propose to model each document as a multivariate Gaussian distribution based on the distributed representations of its words. We then measure the similarity between two documents based on the similarity of their distributions. Experiments on eight standard text categorization datasets demonstrate the effectiveness of the proposed approach in comparison with state-of-the-art methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multivariate Gaussian Document Representation from Word Embeddings for Text Categorization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/E17-2072" target="_blank">https://aclanthology.org/E17-2072</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In many natural language processing (NLP) tasks a document is commonly modeled as a bag of words using the term frequency-inverse document frequency (TF-IDF) vector. One major shortcoming of the frequency-based TF-IDF feature vector is that it ignores word orders that carry syntactic and semantic relationships among the words in a document. This paper proposes a novel distributed vector representation of a document which will be labeled as DV-LSTM and is derived from the result of adapting a long short-term memory recurrent neural network language model by the document. DV-LSTM is expected to capture some high-level sequential information in the document which other current document representations fail to do. It was evaluated in document genre classification in the Brown Corpus and the BNC Baby Corpus. The results show that DV-LSTM significantly outperforms TF-IDF vector and paragraph vector (PV-DM) in most cases and their combinations may further improve the classification performance.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Derivation of Document Vectors from Adaptation of LSTM Language Model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/E17-2073" target="_blank">https://aclanthology.org/E17-2073</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a robust approach for detecting intrinsic sentence importance in news by training on two corpora of document-summary pairs. When used for single-document summarization our approach combined with the ``beginning of document&#39;&#39; heuristic outperforms a state-of-the-art summarizer and the beginning-of-article baseline in both automatic and manual evaluations. These results represent an important advance because in the absence of cross-document repetition single document summarizers for news have not been able to consistently outperform the strong beginning-of-article baseline.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Detecting (Un)Important Content for Single-Document News Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/E17-2112" target="_blank">https://aclanthology.org/E17-2112</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Descriptive document clustering aims to automatically discover groups of semantically related documents and to assign a meaningful label to characterise the content of each cluster. In this paper we present a descriptive clustering approach that employs a distributed representation model namely the paragraph vector model to capture semantic similarities between documents and phrases. The proposed method uses a joint representation of phrases and documents (i.e. a co-embedding) to automatically select a descriptive phrase that best represents each document cluster. We evaluate our method by comparing its performance to an existing state-of-the-art descriptive clustering method that also uses co-embedding but relies on a bag-of-words representation. Results obtained on benchmark datasets demonstrate that the paragraph vector-based method obtains superior performance over the existing approach in both identifying clusters and assigning appropriate descriptive labels to them.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Distributed Document and Phrase Co-embeddings for Descriptive Clustering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/E17-1093" target="_blank">https://aclanthology.org/E17-1093</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Graphs have long been proposed as a tool to browse and navigate in a collection of documents in order to support exploratory search. Many techniques to automatically extract different types of graphs showing for example entities or concepts and different relationships between them have been suggested. While experimental evidence that they are indeed helpful exists for some of them it is largely unknown which type of graph is most helpful for a specific exploratory task. However carrying out experimental comparisons with human subjects is challenging and time-consuming. Towards this end we present the textitGraphDocExplore framework. It provides an intuitive web interface for graph-based document exploration that is optimized for experimental user studies. Through a generic graph interface different methods to extract graphs from text can be plugged into the system. Hence they can be compared at minimal implementation effort in an environment that ensures controlled comparisons. The system is publicly available under an open-source license.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>GraphDocExplore: A Framework for the Experimental Comparison of Graph-based Document Exploration Techniques</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D17-2004" target="_blank">https://aclanthology.org/D17-2004</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Multi-document summarization provides users with a short text that summarizes the information in a set of related documents. This paper introduces affinity-preserving random walk to the summarization task which preserves the affinity relations of sentences by an absorbing random walk model. Meanwhile we put forward adjustable affinity-preserving random walk to enforce the diversity constraint of summarization in the random walk process. The ROUGE evaluations on DUC 2003 topic-focused summarization task and DUC 2004 generic summarization task show the good performance of our method which has the best ROUGE-2 recall among the graph-based ranking methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Affinity-Preserving Random Walk for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D17-1020" target="_blank">https://aclanthology.org/D17-1020</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level sentiment classification is a fundamental problem which aims to predict a user&#39;s overall sentiment about a product in a document. Several methods have been proposed to tackle the problem whereas most of them fail to consider the influence of users who express the sentiment and products which are evaluated. To address the issue we propose a deep memory network for document-level sentiment classification which could capture the user and product information at the same time. To prove the effectiveness of our algorithm we conduct experiments on IMDB and Yelp datasets and the results indicate that our model can achieve better performance than several existing methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Capturing User and Product Information for Document Level Sentiment Analysis with Deep Memory Network</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D17-1054" target="_blank">https://aclanthology.org/D17-1054</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a feature vector formation technique for documents - Sparse Composite Document Vector (SCDV) - which overcomes several shortcomings of the current distributional paragraph vector representations that are widely used for text representation. In SCDV word embeddings are clustered to capture multiple semantic contexts in which words occur. They are then chained together to form document topic-vectors that can express complex multi-topic documents. Through extensive experiments on multi-class and multi-label classification tasks we outperform the previous state-of-the-art method NTSG. We also show that SCDV embeddings perform well on heterogeneous tasks like Topic Coherence context-sensitive Learning and Information Retrieval. Moreover we achieve a significant reduction in training and prediction times compared to other representation methods. SCDV achieves best of both worlds - better performance with lower time and space complexity.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SCDV : Sparse Composite Document Vectors using soft clustering over distributional representations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D17-1069" target="_blank">https://aclanthology.org/D17-1069</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we present a novel document similarity measure based on the definition of a graph kernel between pairs of documents. The proposed measure takes into account both the terms contained in the documents and the relationships between them. By representing each document as a graph-of-words we are able to model these relationships and then determine how similar two documents are by using a modified shortest-path graph kernel. We evaluate our approach on two tasks and compare it against several baseline approaches using various performance metrics such as DET curves and macro-average F1-score. Experimental results on a range of datasets showed that our proposed approach outperforms traditional techniques and is capable of measuring more accurately the similarity between two documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Shortest-Path Graph Kernels for Document Similarity</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D17-1202" target="_blank">https://aclanthology.org/D17-1202</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We introduce a hierarchical architecture for machine reading capable of extracting precise information from long documents. The model divides the document into small overlapping windows and encodes all windows in parallel with an RNN. It then attends over these window encodings reducing them to a single encoding which is decoded into an answer using a sequence decoder. This hierarchical approach allows the model to scale to longer documents without increasing the number of sequential steps. In a supervised setting our model achieves state of the art accuracy of 76.8 on the WikiReading dataset. We also evaluate the model in a semi-supervised setting by downsampling the WikiReading training set to create increasingly smaller amounts of supervision while leaving the full unlabeled document corpus to train a sequence autoencoder on document windows. We evaluate models that can reuse autoencoder states and outputs without fine-tuning their weights allowing for more efficient training and inference.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Accurate Supervised and Semi-Supervised Machine Reading for Long Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D17-1214" target="_blank">https://aclanthology.org/D17-1214</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document-level multi-aspect sentiment classification is an important task for customer relation management. In this paper we model the task as a machine comprehension problem where pseudo question-answer pairs are constructed by a small number of aspect-related keywords and aspect ratings. A hierarchical iterative attention model is introduced to build aspectspecific representations by frequent and repeated interactions between documents and aspect questions. We adopt a hierarchical architecture to represent both word level and sentence level information and use the attention operations for aspect questions and documents alternatively with the multiple hop mechanism. Experimental results on the TripAdvisor and BeerAdvocate datasets show that our model outperforms classical baselines. We will release our code and data for the method replicability.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Multi-Aspect Sentiment Classification as Machine Comprehension</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D17-1217" target="_blank">https://aclanthology.org/D17-1217</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The need for automatic document summarization that can be used for practical applications is increasing rapidly. In this paper we propose a general framework for summarization that extracts sentences from a document using externally related information. Our work is aimed at single document summarization using small amounts of reference summaries. In particular we address document summarization in the framework of multi-task learning using curriculum learning for sentence extraction and document classification. The proposed framework enables us to obtain better feature representations to extract sentences from documents. We evaluate our proposed summarization method on two datasets: financial report and news corpus. Experimental results demonstrate that our summarizers achieve performance that is comparable to state-of-the-art systems.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extractive Summarization Using Multi-Task Learning with Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D17-1223" target="_blank">https://aclanthology.org/D17-1223</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work we suggest a slightly more difficult data-to-text generation task and investigate how effective current approaches are on this task. In particular we introduce a new large-scale corpus of data records paired with descriptive documents propose a series of extractive evaluation methods for analyzing performance and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text but fail to convincingly approximate human-generated documents. Moreover even templated baselines exceed the performance of these neural models on some metrics though copy- and reconstruction-based extensions lead to noticeable improvements.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Challenges in Data-to-Document Generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D17-1239" target="_blank">https://aclanthology.org/D17-1239</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>A document outlier is a document that substantially deviates in semantics from the majority ones in a corpus. Automatic identification of document outliers can be valuable in many applications such as screening health records for medical mistakes. In this paper we study the problem of mining semantically deviating document outliers in a given corpus. We develop a generative model to identify frequent and characteristic semantic regions in the word embedding space to represent the given corpus and a robust outlierness measure which is resistant to noisy content in documents. Experiments conducted on two real-world textual data sets show that our method can achieve an up to 135% improvement over baselines in terms of recall at top-1% of the outlier ranking.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Identifying Semantically Deviating Outlier Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D17-1291" target="_blank">https://aclanthology.org/D17-1291</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we present a novel approach to infer significance of various textual edits to documents. An author may make several edits to a document; each edit varies in its impact to the content of the document. While some edits are surface changes and introduce negligible change other edits may change the content/tone of the document significantly. In this paper we perform an analysis on the human perceptions of edit importance while reviewing documents from one version to the next. We identify linguistic features that influence edit importance and model it in a regression based setting. We show that the predicted importance by our approach is highly correlated with the human perceived importance established by a Mechanical Turk study.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Empirical Analysis of Edit Importance between Document Versions</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D17-1295" target="_blank">https://aclanthology.org/D17-1295</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Traditional supervised learning makes the closed-world assumption that the classes appeared in the test data must have appeared in training. This also applies to text learning or text classification. As learning is used increasingly in dynamic open environments where some new/test documents may not belong to any of the training classes identifying these novel documents during classification presents an important problem. This problem is called open-world classification or open classification. This paper proposes a novel deep learning based approach. It outperforms existing state-of-the-art techniques dramatically.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DOC: Deep Open Classification of Text Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D17-1314" target="_blank">https://aclanthology.org/D17-1314</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>L&#39;un des objectifs de nos travaux `a terme est de transformer un corpus de documents m&#39;edicaux en donn&#39;ees structur&#39;ees pour en faciliter l&#39;exploitation. Ainsi il est n&#39;ecessaire non seulement de d&#39;etecter les concepts m&#39;edicaux &#39;evoqu&#39;es mais aussi d&#39;int&#39;egrer un processus capable d&#39;identifier le contexte dans lequel est &#39;evoqu&#39;e chaque concept m&#39;edical. Dans cet article nous revenons principalement sur les syst`emes par apprentissage supervis&#39;e qui ont &#39;et&#39;e propos&#39;e pour la d&#39;etection de l&#39;incertitude et de la n&#39;egation. Ces dix derni`eres ann&#39;ees les travaux pour d&#39;etecter l&#39;incertitude et la n&#39;egation dans les textes en anglais ont donn&#39;e des r&#39;esultats satisfaisants. Cependant il existe encore une marge de progression non-n&#39;egligeable.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>D&#39;etection de l&#39;incertitude et de la n&#39;egation : un &#39;etat de l&#39;art (Identifying uncertainty and negation&#39;s cues and scope : State of the art One of the goals of our endeavours is to turn a corpus of medical documents into more easily readable structured data)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2017.jeptalnrecital-recital.8" target="_blank">https://aclanthology.org/2017.jeptalnrecital-recital.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Cet article pr&#39;esente un syst`eme original de traduction de documents num&#39;eris&#39;es en arabe. Deux modules sont cascad&#39;es : un syst`eme de reconnaissance optique de caract`eres (OCR) en arabe et un syst`eme de traduction automatique (TA) arabe-franccais. Le couplage OCR-TA a &#39;et&#39;e peu abord&#39;e dans la litt&#39;erature et l&#39;originalit&#39;e de cette &#39;etude consiste `a proposer un couplage &#39;etroit entre OCR et TA ainsi qu&#39;un traitement sp&#39;ecifique des mots hors vocabulaire (MHV) engendr&#39;es par les erreurs d&#39;OCRisation. Le couplage OCR-TA par treillis et notre traitement des MHV par remplacement selon une mesure composite qui prend en compte forme de surface et contexte du mot permettent une am&#39;elioration significative des performances de traduction. Les exp&#39;erimentations sont r&#39;ealis&#39;es sur un corpus de journaux num&#39;eris&#39;es en arabe et permettent d&#39;obtenir des am&#39;eliorations en score BLEU de 373 et 55 sur les corpus de d&#39;eveloppement et de test respectivement.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Traitement des Mots Hors Vocabulaire pour la Traduction Automatique de Document OCRis&#39;es en Arabe (This article presents a new system that automatically translates images of Arabic documents)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2017.jeptalnrecital-long.5" target="_blank">https://aclanthology.org/2017.jeptalnrecital-long.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2017</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>r automatiquement en ligne : d&#39;emonstration d&#39;un service web de r&#39;esum&#39;e multidocument Valentin Nyzam Nathan Gatto Aur&#39;elien Bossard LIASD Universit&#39;e Paris 8 - IUT de Montreuil 140 rue de la Nouvelle France 93100 Montreuil France valentin.nyzam@iut.univ-paris8.fr nathan.gatto@free.fr aurelien.bossard@iut.univ-paris8.fr R &#39;ESUM&#39;E Nous proposons une d&#39;emonstration d&#39;un webservice de r&#39;esum&#39;e automatique multidocument. Ce webservice s&#39;appuie sur un outil ouvert qui impl&#39;emente plusieurs algorithmes reconnus de r&#39;esum&#39;e automatique et permet de r&#39;esumer des documents en utilisant des configurations diff&#39;erentes.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>R&#39;esumer automatiquement en ligne : d&#39;emonstration d&#39;un service web de r&#39;esum&#39;e multidocument (Summarizing Automatically Online : We propose a demonstration of an automatic multidocument summarization web service)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2017.jeptalnrecital-demo.10" target="_blank">https://aclanthology.org/2017.jeptalnrecital-demo.10</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Document Ranking using Query Expansion and Classification Techniques for Mixed Script Information Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-6311" target="_blank">https://aclanthology.org/W16-6311</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Human versus Machine Attention in Document Classification: A Dataset with Crowdsourced Annotations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-6213" target="_blank">https://aclanthology.org/W16-6213</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Resolution of Acronyms and Abbreviations in Nursing Notes Using Document-Level Context Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-6107" target="_blank">https://aclanthology.org/W16-6107</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Machine Translation (MT) plays a critical role in expanding capacity in the translation industry. However many valuable documents including digital documents are encoded in non-accessible formats for machine processing (e.g. Historical or Legal documents). Such documents must be passed through a process of Optical Character Recognition (OCR) to render the text suitable for MT. No matter how good the OCR is this process introduces recognition errors which often renders MT ineffective. In this paper we propose a new OCR to MT framework based on adding a new OCR error correction module to enhance the overall quality of translation. Experimentation shows that our new system correction based on the combination of Language Modeling and Translation methods outperforms the baseline system by nearly 30% relative improvement.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Integrating Optical Character Recognition and Machine Translation of Historical Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-4015" target="_blank">https://aclanthology.org/W16-4015</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The SemEval-2010 benchmark dataset has brought renewed attention to the task of automatic keyphrase extraction. This dataset is made up of scientific articles that were automatically converted from PDF format to plain text and thus require careful preprocessing so that irrevelant spans of text do not negatively affect keyphrase extraction performance. In previous work a wide range of document preprocessing techniques were described but their impact on the overall performance of keyphrase extraction models is still unexplored. Here we re-assess the performance of several keyphrase extraction models and measure their robustness against increasingly sophisticated levels of document preprocessing.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>How Document Pre-processing affects Keyphrase Extraction Performance</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-3917" target="_blank">https://aclanthology.org/W16-3917</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Processing Document Collections to Automatically Extract Linked Data: Semantic Storytelling Technologies for Smart Curation Workflows</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-3503" target="_blank">https://aclanthology.org/W16-3503</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Analysing the Integration of Semantic Web Features for Document Planning across Genres</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-3513" target="_blank">https://aclanthology.org/W16-3513</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Portable Method for Parallel and Comparable Document Alignment</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-3412" target="_blank">https://aclanthology.org/W16-3412</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Document Classification with Informed Topic Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-2911" target="_blank">https://aclanthology.org/W16-2911</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using Centroids of Word Embeddings and Word Mover&#39;s Distance for Biomedical Document Retrieval in Question Answering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-2915" target="_blank">https://aclanthology.org/W16-2915</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Findings of the WMT 2016 Bilingual Document Alignment Shared Task</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-2347" target="_blank">https://aclanthology.org/W16-2347</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DOCAL - Vicomtech&#39;s Participation in the WMT16 Shared Task on Bilingual Document Alignment</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-2364" target="_blank">https://aclanthology.org/W16-2364</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Quick and Reliable Document Alignment via TF/IDF-weighted Cosine Distance</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-2365" target="_blank">https://aclanthology.org/W16-2365</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>YODA System for WMT16 Shared Task: Bilingual Document Alignment</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-2366" target="_blank">https://aclanthology.org/W16-2366</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Bitextor&#39;s participation in WMT&#39;16: shared task on document alignment</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-2367" target="_blank">https://aclanthology.org/W16-2367</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Bilingual Document Alignment with Latent Semantic Indexing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-2368" target="_blank">https://aclanthology.org/W16-2368</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>First Steps Towards Coverage-Based Document Alignment</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-2369" target="_blank">https://aclanthology.org/W16-2369</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>BAD LUC@WMT 2016: a Bilingual Document Alignment Platform Based on Lucene</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-2370" target="_blank">https://aclanthology.org/W16-2370</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using Term Position Similarity and Language Modeling for Bilingual Document Alignment</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-2371" target="_blank">https://aclanthology.org/W16-2371</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The ADAPT Bilingual Document Alignment system at WMT16</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-2372" target="_blank">https://aclanthology.org/W16-2372</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>WMT2016: A Hybrid Approach to Bilingual Document Alignment</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-2373" target="_blank">https://aclanthology.org/W16-2373</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>English-French Document Alignment Based on Keywords and Statistical Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-2374" target="_blank">https://aclanthology.org/W16-2374</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The ILSP/ARC submission to the WMT 2016 Bilingual Document Alignment Shared Task</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-2375" target="_blank">https://aclanthology.org/W16-2375</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Word Clustering Approach to Bilingual Document Alignment (WMT 2016 Shared Task)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-2376" target="_blank">https://aclanthology.org/W16-2376</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Searching Four-Millenia-Old Digitized Documents: A Text Retrieval System for Egyptologists</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-2103" target="_blank">https://aclanthology.org/W16-2103</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Semi-automated annotation of page-based documents within the Genre and Multimodality framework</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-2109" target="_blank">https://aclanthology.org/W16-2109</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Building a Cross-document Event-Event Relation Corpus</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-1701" target="_blank">https://aclanthology.org/W16-1701</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Phrase Generalization: a Corpus Study in Multi-Document Abstracts and Original News Alignments</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-1717" target="_blank">https://aclanthology.org/W16-1717</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-1609" target="_blank">https://aclanthology.org/W16-1609</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Incorporating Satellite Documents into Co-citation Networks for Scientific Paper Searches</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-1504" target="_blank">https://aclanthology.org/W16-1504</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Network Motifs May Improve Quality Assessment of Text Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W16-1404" target="_blank">https://aclanthology.org/W16-1404</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CDE-IIITH at SemEval-2016 Task 12: Extraction of Temporal Information from Clinical documents using Machine Learning techniques</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/S16-1192" target="_blank">https://aclanthology.org/S16-1192</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MDSWriter: Annotation Tool for Creating High-Quality Multi-Document Summarization Corpora</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P16-4017" target="_blank">https://aclanthology.org/P16-4017</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-level Sentiment Inference with Social Faction and Discourse Context</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P16-1032" target="_blank">https://aclanthology.org/P16-1032</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DocChat: An Information Retrieval Approach for Chatbot Engines Using Unstructured Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P16-1049" target="_blank">https://aclanthology.org/P16-1049</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Generative Topic Embedding: a Continuous Representation of Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P16-1063" target="_blank">https://aclanthology.org/P16-1063</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Discriminative Topic Model using Document Network Structure</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P16-1065" target="_blank">https://aclanthology.org/P16-1065</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Multi-Author Document Decomposition Based on Hidden Markov Model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P16-1067" target="_blank">https://aclanthology.org/P16-1067</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>ALTO: Active Learning with Topic Overviews for Speeding Label Induction and Document Labeling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P16-1110" target="_blank">https://aclanthology.org/P16-1110</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-Lingual Sentiment Classification with Bilingual Document Representation Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P16-1133" target="_blank">https://aclanthology.org/P16-1133</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Optimizing an Approximation of ROUGE - a Problem-Reduction Approach to Extractive Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P16-1172" target="_blank">https://aclanthology.org/P16-1172</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>How to Train Dependency Parsers with Inexact Search for Joint Sentence Boundary Detection and Parsing of Entire Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P16-1181" target="_blank">https://aclanthology.org/P16-1181</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P16-1188" target="_blank">https://aclanthology.org/P16-1188</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Joint Extraction of Events and Entities within a Document Context</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N16-1033" target="_blank">https://aclanthology.org/N16-1033</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Unsupervised Model of Orthographic Variation for Historical Document Transcription</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N16-1055" target="_blank">https://aclanthology.org/N16-1055</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Inter-document Contextual Language model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N16-1090" target="_blank">https://aclanthology.org/N16-1090</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Hierarchical Attention Networks for Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N16-1174" target="_blank">https://aclanthology.org/N16-1174</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N16-1177" target="_blank">https://aclanthology.org/N16-1177</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a successfully implemented document repository REST service for flexible SCRUD (search crate read update delete) storage of social media conversations using a GATE/TIPSTER-like document object model and providing a query language for document features. This software is currently being used in the SENSEI research project and will be published as open-source software before the project ends. It is to the best of our knowledge the first freely available general purpose data repository to support large-scale multimodal (i.e. speech or text) conversation analytics.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Document Repository for Social Media and Speech Conversations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L16-1070" target="_blank">https://aclanthology.org/L16-1070</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we describe the organization and the implementation of the CAMOMILE collaborative annotation framework for multimodal multimedia multilingual (3M) data. Given the versatile nature of the analysis which can be performed on 3M data the structure of the server was kept intentionally simple in order to preserve its genericity relying on standard Web technologies. Layers of annotations defined as data associated to a media fragment from the corpus are stored in a database and can be managed through standard interfaces with authentication. Interfaces tailored specifically to the needed task can then be developed in an agile way relying on simple but reliable services for the management of the centralized annotations. We then present our implementation of an active learning scenario for person annotation in video relying on the CAMOMILE server; during a dry run experiment the manual annotation of 716 speech segments was thus propagated to 3504 labeled tracks. The code of the CAMOMILE framework is distributed in open source.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The CAMOMILE Collaborative Annotation Platform for Multi-modal Multi-lingual and Multi-media Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L16-1226" target="_blank">https://aclanthology.org/L16-1226</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Motivated by the adage that a ``picture is worth a thousand words&#39;&#39; it can be reasoned that automatically enriching the textual content of a document with relevant images can increase the readability of a document. Moreover features extracted from the additional image data inserted into the textual content of a document may in principle be also be used by a retrieval engine to better match the topic of a document with that of a given query. In this paper we describe our approach of building a ground truth dataset to enable further research into automatic addition of relevant images to text documents. The dataset is comprised of the official ImageCLEF 2010 collection (a collection of images with textual metadata) to serve as the images available for automatic enrichment of text a set of 25 benchmark documents that are to be enriched which in this case are children&#39;s short stories and a set of manually judged relevant images for each query story obtained by the standard procedure of depth pooling. We use this benchmark dataset to evaluate the effectiveness of standard information retrieval methods as simple baselines for this task. The results indicate that using the whole story as a weighted query where the weight of each query term is its tf-idf value achieves an precision of 0:1714 within the top 5 retrieved images on an average.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Developing a Dataset for Evaluating Approaches for Document Expansion with Images</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L16-1299" target="_blank">https://aclanthology.org/L16-1299</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We introduce an approach based on using the dependency grammar representations of sentences to compute sentence similarity for extractive multi-document summarization. We adapt and investigate the effects of two untyped dependency tree kernels which have originally been proposed for relation extraction to the multi-document summarization problem. In addition we propose a series of novel dependency grammar based kernels to better represent the syntactic and semantic similarities among the sentences. The proposed methods incorporate the type information of the dependency relations for sentence similarity calculation. To our knowledge this is the first study that investigates using dependency tree based sentence similarity for multi-document summarization.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Sentence Similarity based on Dependency Tree Kernels for Multi-document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L16-1452" target="_blank">https://aclanthology.org/L16-1452</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We report on the creation of a lexical resource for the identification of potentially unspecific or imprecise constructions in German requirements documentation from the car manufacturing industry. In requirements engineering such expressions are called ``weak words&#39;&#39;: they are not sufficiently precise to ensure an unambiguous interpretation by the contractual partners who for the definition of their cooperation typically rely on specification documents (Melchisedech 2000); an example are dimension adjectives such as kurz or lang (`short&#39; `long&#39;) which need to be modified by adverbials indicating the exact duration size etc. Contrary to standard practice in requirements engineering where the identification of such weak words is merely based on stopword lists we identify weak uses in context by querying annotated text. The queries are part of the resource as they define the conditions when a word use is weak. We evaluate the recognition of weak uses on our development corpus and on an unseen evaluation corpus reaching stable F1-scores above 0.95.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Lexical Resource for the Identification of ``Weak Words&#39;&#39; in German Specification Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L16-1454" target="_blank">https://aclanthology.org/L16-1454</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper explores the incorporation of lexico-semantic heuristics into a deterministic Coreference Resolution (CR) system for classifying named entities at document-level. The highest precise sieves of a CR tool are enriched with both a set of heuristics for merging named entities labeled with different classes and also with some constraints that avoid the incorrect merging of similar mentions. Several tests show that this strategy improves both NER labeling and CR. The CR tool can be applied in combination with any system for named entity recognition using the CoNLL format and brings benefits to text analytics tasks such as Information Extraction. Experiments were carried out in Spanish using three different NER tools.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Incorporating Lexico-semantic Heuristics into Coreference Resolution Sieves for Named Entity Recognition at Document-level</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L16-1535" target="_blank">https://aclanthology.org/L16-1535</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Preprocessing is a preliminary step in many fields including IR and NLP. The effect of basic preprocessing settings on English for text summarization is well-studied. However there is no such effort found for the Urdu language (with the best of our knowledge). In this study we analyze the effect of basic preprocessing settings for single-document text summarization for Urdu on a benchmark corpus using various experiments. The analysis is performed using the state-of-the-art algorithms for extractive summarization and the effect of stopword removal lemmatization and stemming is analyzed. Results showed that these pre-processing settings improve the results.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Analyzing Pre-processing Settings for Urdu Single-document Extractive Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L16-1585" target="_blank">https://aclanthology.org/L16-1585</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This project approaches the problem of language documentation and revitalization from a rather untraditional angle. To improve and facilitate language documentation of endangered languages we attempt to use corpus linguistic methods and speech and language technologies to reduce the time needed for transcription and annotation of audio and video language recordings. The paper demonstrates this approach on the example of the endangered and seriously under-resourced variety of Eastern Chatino (CTP). We show how initial speech corpora can be created that can facilitate the development of speech and language technologies for under-resourced languages by utilizing Forced Alignment tools to time align transcriptions. Time-aligned transcriptions can be used to train speech corpora and utilize automatic speech recognition tools for the transcription and annotation of untranscribed data. Speech technologies can be used to reduce the time and effort necessary for transcription and annotation of large collections of audio and video recordings in digital language archives addressing the transcription bottleneck problem that most language archives and many under-documented languages are confronted with. This approach can increase the availability of language resources from low-resourced and endangered languages to speech and language technology research and development.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Endangered Language Documentation: Bootstrapping a Chatino Speech Corpus Forced Aligner ASR</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L16-1632" target="_blank">https://aclanthology.org/L16-1632</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present texigt a command-line tool for the extraction of structured linguistic data from LaTeX source documents and a language resource that has been generated using this tool: a corpus of interlinear glossed text (IGT) extracted from open access books published by Language Science Press. Extracted examples are represented in a simple XML format that is easy to process and can be used to validate certain aspects of interlinear glossed text. The main challenge involved is the parsing of TeX and LaTeX documents. We review why this task is impossible in general and how the texhs Haskell library uses a layered architecture and selective early evaluation (expansion) during lexing and parsing in order to provide access to structured representations of LaTeX documents at several levels. In particular its parsing modules generate an abstract syntax tree for LaTeX documents after expansion of all user-defined macros and lexer-level commands that serves as an ideal interface for the extraction of interlinear glossed text by texigt. This architecture can easily be adapted to extract other types of linguistic data structures from LaTeX source documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extracting Interlinear Glossed Text from LaTeX Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L16-1638" target="_blank">https://aclanthology.org/L16-1638</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Natural language processing applications are frequently integrated to solve complex linguistic problems but the lack of interoperability between these tools tends to be one of the main issues found in that process. That is often caused by the different linguistic formats used across the applications which leads to attempts to both establish standard formats to represent linguistic information and to create conversion tools to facilitate this integration. Pepper is an example of the latter as a framework that helps the conversion between different linguistic annotation formats. In this paper we describe the use of Pepper to convert a corpus linguistically annotated by the annotation scheme AWA into the relANNIS format with the ultimate goal of interacting with AWA documents through the ANNIS interface. The experiment converted 40 megabytes of AWA documents allowed their use on the ANNIS interface and involved making architectural decisions during the mapping from AWA into relANNIS using Pepper. The main issues faced during this process were due to technical issues mainly caused by the integration of the different systems and projects namely AWA Pepper and ANNIS.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Interoperability of Annotation Schemes: Using the Pepper Framework to Display AWA Documents in the ANNIS Interface</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L16-1639" target="_blank">https://aclanthology.org/L16-1639</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Text Complexity Analysis is an useful task in Education. For example it can help teachers select appropriate texts for their students according to their educational level. This task requires the analysis of several text features that people do mostly manually (e.g. syntactic complexity words variety etc.). In this paper we present a tool useful for Complexity Analysis called Coh-Metrix-Esp. This is the Spanish version of Coh-Metrix and is able to calculate 45 readability indices. We analyse how these indices behave in a corpus of ``simple&#39;&#39; and ``complex&#39;&#39; documents and also use them as features in a complexity binary classifier for texts in Spanish. After some experiments with machine learning algorithms we got 0.9 F-measure for a corpus that contains tales for kids and adults and 0.82 F-measure for a corpus with texts written for students of Spanish as a foreign language.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Coh-Metrix-Esp: A Complexity Analysis Tool for Documents Written in Spanish</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/L16-1745" target="_blank">https://aclanthology.org/L16-1745</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Key-Value Memory Networks for Directly Reading Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D16-1147" target="_blank">https://aclanthology.org/D16-1147</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D16-1172" target="_blank">https://aclanthology.org/D16-1172</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents Disco a prototype for supporting knowledge workers in exploring reviewing and sorting collections of textual data. The goal is to facilitate accelerate and improve the discovery of information. To this end it combines Semantic Relatedness techniques with a review workflow developed in a tangible environment. Disco uses a semantic model that is leveraged on-line in the course of search sessions and accessed through natural hand-gesture in a simple and intuitive way.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DISCO: A System Leveraging Semantic Search in Document Review</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C16-2014" target="_blank">https://aclanthology.org/C16-2014</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we describe textbfLangforia a multilingual processing pipeline to annotate texts with multiple layers: formatting parts of speech named entities dependencies semantic roles and entity links. Langforia works as a web service where the server hosts the language processing components and the client the input and result visualization. To annotate a text or a Wikipedia page the user chooses an NLP pipeline and enters the text in the interface or selects the page URL. Once processed the results are returned to the client where the user can select the annotation layers s/he wants to visualize. We designed Langforia with a specific focus for Wikipedia although it can process any type of text. Wikipedia has become an essential encyclopedic corpus used in many NLP projects. However processing articles and visualizing the annotations are nontrivial tasks that require dealing with multiple markup variants encodings issues and tool incompatibilities across the language versions. This motivated the development of a new architecture. A demonstration of Langforia is available for six languages: English French German Spanish Russian and Swedish at urlhttp://vilde.cs.lth.se:9000/ as well as a web API: urlhttp://vilde.cs.lth.se:9000/api. Langforia is also provided as a standalone library and is compatible with cluster computing.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Langforia: Language Pipelines for Annotating Large Collections of Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C16-2016" target="_blank">https://aclanthology.org/C16-2016</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Recent years have witnessed significant increase in the number of large scale digital collections of archival documents such as news articles books etc. Typically users access these collections through searching or browsing. In this paper we investigate another way of accessing temporal collections - across-time comparison i.e. comparing query-relevant information at different periods in the past. We propose an interactive framework called HistoryComparator for contrastively analyzing concepts in archival document collections at different time periods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>HistoryComparator: Interactive Across-Time Comparison in Document Archives</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C16-2018" target="_blank">https://aclanthology.org/C16-2018</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Any real world events or trends that can affect the company&#39;s growth trajectory can be considered as risk. There has been a growing need to automatically identify extract and analyze risk related statements from news events. In this demonstration we will present a risk analytics framework that processes enterprise project management reports in the form of textual data and news documents and classify them into valid and invalid risk categories. The framework also extracts information from the text pertaining to the different categories of risks like their possible cause and impacts. Accordingly we have used machine learning based techniques and studied different linguistic features like n-gram POS dependency future timing uncertainty factors in texts and their various combinations. A manual annotation study from management experts using risk descriptions collected for a specific organization was conducted to evaluate the framework. The evaluation showed promising results for automated risk analysis and identification.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Framework for Mining Enterprise Risk and Risk Factors from News Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C16-2038" target="_blank">https://aclanthology.org/C16-2038</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this demonstration paper we describe Ambient Search a system that displays and retrieves documents in real time based on speech input. The system operates continuously in ambient mode i.e. it generates speech transcriptions and identifies main keywords and keyphrases while also querying its index to display relevant documents without explicit query. Without user intervention the results are dynamically updated; users can choose to interact with the system at any time employing a conversation protocol that is enriched with the ambient information gathered continuously. Our evaluation shows that Ambient Search outperforms another implicit speech-based information retrieval system. Ambient search is available as open source software.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Demonstrating Ambient Search: Implicit Document Retrieval for Speech Streams</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C16-2049" target="_blank">https://aclanthology.org/C16-2049</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>PKUSUMSUM is a Java platform for multilingual document summarization and it sup-ports multiple languages integrates 10 automatic summarization methods and tackles three typical summarization tasks. The summarization platform has been released and users can easily use and update it. In this paper we make a brief description of the char-acteristics the summarization methods and the evaluation results of the platform and al-so compare PKUSUMSUM with other summarization toolkits.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>PKUSUMSUM : A Java Platform for Multilingual Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C16-2060" target="_blank">https://aclanthology.org/C16-2060</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Summarization aims to represent source documents by a shortened passage. Existing methods focus on the extraction of key information but often neglect coherence. Hence the generated summaries suffer from a lack of readability. To address this problem we have developed a graph-based method by exploring the links between text to produce coherent summaries. Our approach involves finding a sequence of sentences that best represent the key information in a coherent way. In contrast to the previous methods that focus only on salience the proposed method addresses both coherence and informativeness based on textual linkages. We conduct experiments on the DUC2004 summarization task data set. A performance comparison reveals that the summaries generated by the proposed system achieve comparable results in terms of the ROUGE metric and show improvements in readability by human evaluation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploring Text Links for Coherent Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C16-1021" target="_blank">https://aclanthology.org/C16-1021</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Extracting summaries via integer linear programming and submodularity are popular and successful techniques in extractive multi-document summarization. However many interesting optimization objectives are neither submodular nor factorizable into an integer linear program. We address this issue and present a general optimization framework where any function of input documents and a system summary can be plugged in. Our framework includes two kinds of summarizers -- one based on genetic algorithms the other using a swarm intelligence approach. In our experimental evaluation we investigate the optimization of two information-theoretic summary evaluation metrics and find that our framework yields competitive results compared to several strong summarization baselines. Our comparative analysis of the genetic and swarm summarizers reveals interesting complementary properties.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A General Optimization Framework for Multi-Document Summarization Using Genetic Algorithms and Swarm Intelligence</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C16-1024" target="_blank">https://aclanthology.org/C16-1024</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present experiments in incrementally learning a dependency parser. The parser will be used in the WordsEye Linguistics Tools (WELT) (Ulinski et al. 2014) which supports field linguists documenting a language&#39;s syntax and semantics. Our goal is to make syntactic annotation faster for field linguists. We have created a new parallel corpus of descriptions of spatial relations and motion events based on pictures and video clips used by field linguists for elicitation of language from native speaker informants. We collected descriptions for each picture and video from native speakers in English Spanish German and Egyptian Arabic. We compare the performance of MSTParser (McDonald et al. 2006) and MaltParser (Nivre et al. 2006) when trained on small amounts of this data. We find that MaltParser achieves the best performance. We also present the results of experiments using the parser to assist with annotation. We find that even when the parser is trained on a single sentence from the corpus annotation time significantly decreases.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Incrementally Learning a Dependency Parser to Support Language Documentation in Field Linguistics</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C16-1043" target="_blank">https://aclanthology.org/C16-1043</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In the age of information exploding multi-document summarization is attracting particular attention for the ability to help people get the main ideas in a short time. Traditional extractive methods simply treat the document set as a group of sentences while ignoring the global semantics of the documents. Meanwhile neural document model is effective on representing the semantic content of documents in low-dimensional vectors. In this paper we propose a document-level reconstruction framework named DocRebuild which reconstructs the documents with summary sentences through a neural document model and selects summary sentences to minimize the reconstruction error. We also apply two strategies sentence filtering and beamsearch to improve the performance of our method. Experimental results on the benchmark datasets DUC 2006 and DUC 2007 show that DocRebuild is effective and outperforms the state-of-the-art unsupervised algorithms.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Unsupervised Multi-Document Summarization Framework Based on Neural Document Model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C16-1143" target="_blank">https://aclanthology.org/C16-1143</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Research in multi-document summarization has focused on newswire corpora since the early beginnings. However the newswire genre provides genre-specific features such as sentence position which are easy to exploit in summarization systems. Such easy to exploit genre-specific features are available in other genres as well. We therefore present the new hMDS corpus for multi-document summarization which contains heterogeneous source documents from multiple text genres as well as summaries with different lengths. For the construction of the corpus we developed a novel construction approach which is suited to build large and heterogeneous summarization corpora with little effort. The method reverses the usual process of writing summaries for given source documents: it combines already available summaries with appropriate source documents. In a detailed analysis we show that our new corpus is significantly different from the homogeneous corpora commonly used and that it is heterogeneous along several dimensions. Our experimental evaluation using well-known state-of-the-art summarization systems shows that our corpus poses new challenges in the field of multi-document summarization. Last but not least we make our corpus publicly available to the research community at the corpus web page urlhttps://github.com/AIPHES/hMDS.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The Next Step for Multi-Document Summarization: A Heterogeneous Multi-Genre Corpus Built with a Novel Construction Approach</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C16-1145" target="_blank">https://aclanthology.org/C16-1145</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Cross document event coreference (CDEC) is an important task that aims at aggregating event-related information across multiple documents. We revisit the evaluation for CDEC and discover that past works have adopted different often inconsistent evaluation settings which either overlook certain mistakes in coreference decisions or make assumptions that simplify the coreference task considerably. We suggest a new evaluation methodology which overcomes these limitations and allows for an accurate assessment of CDEC systems. Our new evaluation setting better reflects the corpus-wide information aggregation ability of CDEC systems by separating event-coreference decisions made across documents from those made within a document. In addition we suggest a better baseline for the task and semi-automatically identify several inconsistent annotations in the evaluation dataset.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Revisiting the Evaluation for Cross Document Event Coreference</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C16-1183" target="_blank">https://aclanthology.org/C16-1183</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present Ambient Search an open source system for displaying and retrieving relevant documents in real time for speech input. The system works ambiently that is it unobstructively listens to speech streams in the background identifies keywords and keyphrases for query construction and continuously serves relevant documents from its index. Query terms are ranked with Word2Vec and TF-IDF and are continuously updated to allow for ongoing querying of a document collection. The retrieved documents in our case Wikipedia articles are visualized in real time in a browser interface. Our evaluation shows that Ambient Search compares favorably to another implicit information retrieval system on speech streams. Furthermore we extrinsically evaluate multiword keyphrase generation showing positive impact for manual transcriptions.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Ambient Search: A Document Retrieval System for Speech Streams</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C16-1196" target="_blank">https://aclanthology.org/C16-1196</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We extend classic review mining work by building a binary classifier that predicts whether a review of a documentary film was written by an expert or a layman with 90.70% accuracy (F1 score) and compare the characteristics of the predicted classes. A variety of standard lexical and syntactic features was used for this supervised learning task. Our results suggest that experts write comparatively lengthier and more detailed reviews that feature more complex grammar and a higher diversity in their vocabulary. Layman reviews are more subjective and contextualized in peoples&#39; everyday lives. Our error analysis shows that laymen are about twice as likely to be mistaken as experts than vice versa. We argue that the type of author might be a useful new feature for improving the accuracy of predicting the rating helpfulness and authenticity of reviews. Finally the outcomes of this work might help researchers and practitioners in the field of impact assessment to gain a more fine-grained understanding of the perception of different types of media consumers and reviewers of a topic genre or information product.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Says Wholdots? Identification of Expert versus Layman Critics&#39; Reviews of Documentary Films</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C16-1200" target="_blank">https://aclanthology.org/C16-1200</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Int&#39;egration de la similarit&#39;e entre phrases comme crit`ere pour le r&#39;esum&#39;e multi-document (Integrating sentence similarity as a constraint for multi-document summarization)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2016.jeptalnrecital-poster.22" target="_blank">https://aclanthology.org/2016.jeptalnrecital-poster.22</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Dans cet article nous nous int&#39;eressons `a l&#39;indexation de documents de domaines de sp&#39;ecialit&#39;e par l&#39;interm&#39;ediaire de leurs termes-cl&#39;es. Plus particuli`erement nous nous int&#39;eressons `a l&#39;indexation telle qu&#39;elle est r&#39;ealis&#39;ee par les documentalistes de biblioth`eques num&#39;eriques. Apr`es analyse de la m&#39;ethodologie de ces indexeurs professionnels nous proposons une m&#39;ethode `a base de graphe combinant les informations pr&#39;esentes dans le document et la connaissance du domaine pour r&#39;ealiser une indexation (hybride) libre et contr^ol&#39;ee. Notre m&#39;ethode permet de proposer des termes-cl&#39;es ne se trouvant pas n&#39;ecessairement dans le document. Nos exp&#39;eriences montrent aussi que notre m&#39;ethode surpasse significativement l&#39;approche `a base de graphe &#39;etat de l&#39;art.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Mod&#39;elisation unifi&#39;ee du document et de son domaine pour une indexation par termes-cl&#39;es libre et contr^ol&#39;ee (Unified document and domain-specific model for keyphrase extraction and assignment )</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2016.jeptalnrecital-long.18" target="_blank">https://aclanthology.org/2016.jeptalnrecital-long.18</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Les repr&#39;esentations de documents au moyen d&#39;approches `a base de r&#39;eseaux de neurones ont montr&#39;e des am&#39;eliorations significatives dans de nombreuses t^aches du traitement du langage naturel. Dans le cadre d&#39;applications r&#39;eelles o`u des conditions d&#39;enregistrement difficiles peuvent ^etre rencontr&#39;ees la transcription automatique de documents parl&#39;es peut g&#39;en&#39;erer un nombre de mots mal transcrits important. Cet article propose une repr&#39;esentation des documents parl&#39;es tr`es bruit&#39;es utilisant des caract&#39;eristiques apprises par un auto-encodeur profond supervis&#39;e. La m&#39;ethode propos&#39;ee s&#39;appuie `a la fois sur les documents bruit&#39;es et leur &#39;equivalent propre annot&#39;e manuellement pour estimer une repr&#39;esentation plus robuste des documents bruit&#39;es. Cette repr&#39;esentation est &#39;evalu&#39;ee sur le corpus DECODA sur une t^ache de classification th&#39;ematique de conversations t&#39;el&#39;ephoniques atteignant une pr&#39;ecision de 83% avec un gain d&#39;environ 6%.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Auto-encodeurs pour la compr&#39;ehension de documents parl&#39;es (Auto-encoders for Spoken Document Understanding)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2016.jeptalnrecital-jep.9" target="_blank">https://aclanthology.org/2016.jeptalnrecital-jep.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>La pr&#39;esente communication pr&#39;esente les projets scientifiques et les r&#39;ealisations de deux collections h&#39;eberg&#39;ees par la plateforme de ressources orales Cocoon : la Collection Pangloss qui concerne principalement des langues de tradition orale (sans &#39;ecriture) du monde entier ; et la Collection AuCo d&#39;edi&#39;ee aux langues du Vietnam et de pays voisins. L&#39;objectif est un progr`es solidaire des recherches et de la documentation linguistique. L&#39;accent est mis sur les perspectives ouvertes pour la recherche en phon&#39;etique/phonologie par certaines r&#39;ealisations r&#39;ecentes dans le cadre de ces deux Collections.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Contribuer au progr`es solidaire des recherches et de la documentation : la Collection Pangloss et la Collection AuCo (Contributing to joint progress in documentation and research: some achievements and future perspectives of the Pangloss Collection and the AuCo Collection)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2016.jeptalnrecital-jep.18" target="_blank">https://aclanthology.org/2016.jeptalnrecital-jep.18</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>L&#39;identification du r^ole d&#39;un locuteur dans des &#39;emissions de t&#39;el&#39;evision est un probl`eme de classification de personne selon une liste de r^oles comme pr&#39;esentateur journaliste invit&#39;e etc. `A cause de la nonsynchronie entre les modalit&#39;es ainsi que par le manque de corpus de vid&#39;eos annot&#39;ees dans toutes les modalit&#39;es seulement une des modalit&#39;es est souvent utilis&#39;ee. Nous pr&#39;esentons dans cet article une fusion multimodale des espaces de repr&#39;esentations de l&#39;audio du texte et de l&#39;image pour la reconnaissance du r^ole du locuteur pour des donn&#39;ees asynchrones. Les espaces de repr&#39;esentations monomodaux sont entra^in&#39;es sur des corpus de donn&#39;ees exog`enes puis ajust&#39;es en utilisant des r&#39;eseaux de neurones profonds sur un corpus d&#39;&#39;emissions franccaises pour notre t^ache de classification. Les exp&#39;eriences r&#39;ealis&#39;ees sur le corpus de donn&#39;ees REPERE ont mis en &#39;evidence les gains d&#39;une fusion au niveau des espaces de repr&#39;esentations par rapport aux m&#39;ethodes de fusion tardive standard.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Fusion d&#39;espaces de repr&#39;esentations multimodaux pour la reconnaissance du r^ole du locuteur dans des documents t&#39;el&#39;evisuels (Multimodal embedding fusion for robust speaker role recognition in video broadcast )</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2016.jeptalnrecital-jep.41" target="_blank">https://aclanthology.org/2016.jeptalnrecital-jep.41</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2016</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Writing intended to inform frequently contains references to document entities (DEs) a mixed class that includes orthographically structured items (e.g. illustrations sections lists) and discourse entities (arguments suggestions points). Such references are vital to the interpretation of documents but they often eschew identifiers such as ``Figure 1&#39;&#39; for inexplicit phrases like ``in this figure&#39;&#39; or ``from these premises&#39;&#39;. We examine inexplicit references to DEs termed DE references and recast the problem of their automatic detection into the determination of relevant word senses. We then show the feasibility of machine learning for the detection of DE-relevant word senses using a corpus of human-labeled synsets from WordNet. We test cross-domain performance by gathering lemmas and synsets from three corpora: website privacy policies Wikipedia articles and Wikibooks textbooks. Identifying DE references will enable language technologies to use the information encoded by them permitting the automatic generation of finely-tuned descriptions of DEs and the presentation of richly-structured information to readers.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>This Table is Different: A WordNet-Based Approach to Identifying References to Document Entities</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2016.gwc-1.60" target="_blank">https://aclanthology.org/2016.gwc-1.60</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Finding the Origin of a Translated Historical Document</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y15-2012" target="_blank">https://aclanthology.org/Y15-2012</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Measuring Popularity of Machine-Generated Sentences Using Term Count Document Frequency and Dependency Language Model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y15-2037" target="_blank">https://aclanthology.org/Y15-2037</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Comprehensive Filter Feature Selection for Improving Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y15-1020" target="_blank">https://aclanthology.org/Y15-1020</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Sentiment Classification of Arabic Documents: Experiments with multi-type features and ensemble algorithms</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y15-1023" target="_blank">https://aclanthology.org/Y15-1023</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Hybrid Method of Semi-supervised Learning and Feature Weighted Learning for Domain Adaptation of Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y15-1057" target="_blank">https://aclanthology.org/Y15-1057</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Words are not Equal: Graded Weighting Model for Building Composite Document Vectors</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-5903" target="_blank">https://aclanthology.org/W15-5903</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A temporal expression recognition system for medical documents by</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-5912" target="_blank">https://aclanthology.org/W15-5912</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Joint semantic discourse models for automatic multi-document summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-5612" target="_blank">https://aclanthology.org/W15-5612</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>On Strategies of Human Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-5618" target="_blank">https://aclanthology.org/W15-5618</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Distributed Representations of Words and Documents for Discriminating Similar Languages</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-5403" target="_blank">https://aclanthology.org/W15-5403</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>E-law Module Supporting Lawyers in the Process of Knowledge Discovery from Legal Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-5308" target="_blank">https://aclanthology.org/W15-5308</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Machine Translation with Word Vector Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-4908" target="_blank">https://aclanthology.org/W15-4908</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Searching for Context: a Study on Document-Level Labels for Translation Quality Estimation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-4916" target="_blank">https://aclanthology.org/W15-4916</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Discursive Grid Approach to Model Local Coherence in Multi-document Summaries</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-4608" target="_blank">https://aclanthology.org/W15-4608</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>AllSummarizer system at MultiLing 2015: Multilingual single and multi-document summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-4634" target="_blank">https://aclanthology.org/W15-4634</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MultiLing 2015: Multilingual Summarization of Single and Multi-Documents On-line Fora and Call-center Conversations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-4638" target="_blank">https://aclanthology.org/W15-4638</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Interactions between Narrative Schemas and Document Categories</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-4501" target="_blank">https://aclanthology.org/W15-4501</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-Document Non-Fiction Narrative Alignment</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-4509" target="_blank">https://aclanthology.org/W15-4509</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Lexical Characteristics Analysis of Chinese Clinical Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-3813" target="_blank">https://aclanthology.org/W15-3813</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Restoring the intended structure of Hungarian ophthalmology documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-3819" target="_blank">https://aclanthology.org/W15-3819</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Generative Model for Extracting Parallel Fragments from Comparable Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-3407" target="_blank">https://aclanthology.org/W15-3407</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>BUCC Shared Task: Cross-Language Document Similarity</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-3411" target="_blank">https://aclanthology.org/W15-3411</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>AUT Document Alignment Framework for BUCC Workshop Shared Task</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-3412" target="_blank">https://aclanthology.org/W15-3412</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>LINA: Identifying Comparable Documents from Wikipedia</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-3413" target="_blank">https://aclanthology.org/W15-3413</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Predicting Machine Translation Adequacy with Document Embeddings</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-3051" target="_blank">https://aclanthology.org/W15-3051</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Machine Translation Evaluation with Gist Consistency and Text Cohesion</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-2504" target="_blank">https://aclanthology.org/W15-2504</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Detecting Document-level Context Triggers to Resolve Translation Ambiguity</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-2506" target="_blank">https://aclanthology.org/W15-2506</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Document-Level SMT System with Integrated Pronoun Prediction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-2510" target="_blank">https://aclanthology.org/W15-2510</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Novel Document Level Features for Statistical Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-2520" target="_blank">https://aclanthology.org/W15-2520</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Vector Space Models for Scientific Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-1525" target="_blank">https://aclanthology.org/W15-1525</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Event analysis for information extraction from business-based technical documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-0808" target="_blank">https://aclanthology.org/W15-0808</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Semantic Interoperability for Cross-lingual and cross-document Event Detection</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W15-0814" target="_blank">https://aclanthology.org/W15-0814</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Query-Based Single Document Summarization Using an Ensemble Noisy Auto-Encoder</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/U15-1001" target="_blank">https://aclanthology.org/U15-1001</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>TeamUFAL: WSD+EL as Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/S15-2059" target="_blank">https://aclanthology.org/S15-2059</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SemEval-2015 Task 4: TimeLine: Cross-Document Event Ordering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/S15-2132" target="_blank">https://aclanthology.org/S15-2132</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SPINOZA_VU: An NLP Pipeline for Cross Document TimeLines</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/S15-2133" target="_blank">https://aclanthology.org/S15-2133</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>GPLSIUA: Combining Temporal Information and Topic Modeling for Cross-Document Event Ordering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/S15-2138" target="_blank">https://aclanthology.org/S15-2138</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>HeidelToul: A Baseline Approach for Cross-document Event Ordering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/S15-2139" target="_blank">https://aclanthology.org/S15-2139</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Collective Document Classification with Implicit Inter-document Semantic Relationships</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/S15-1012" target="_blank">https://aclanthology.org/S15-1012</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extending a Single-Document Summarizer to Multi-Document: a Hierarchical Approach</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/S15-1020" target="_blank">https://aclanthology.org/S15-1020</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Implicit Entity Recognition in Clinical Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/S15-1028" target="_blank">https://aclanthology.org/S15-1028</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Easy-read Documents as a Gold Standard for Evaluation of Text Simplification Output</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/R15-2005" target="_blank">https://aclanthology.org/R15-2005</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Identifying and linking named entities across information sources is the basis of knowledge acquisition and at the heart of Web search recommendations and analytics. An important problem in this context is cross-document co-reference resolution (CCR): computing equivalence classes of textual mentions denoting the same entity within and across documents. Prior methods employ ranking clustering or probabilistic graphical models using syntactic features and distant features from knowledge bases. However these methods exhibit limitations regarding run-time and robustness. This paper presents the CROCS framework for unsupervised CCR improving the state of the art in two ways. First we extend the way knowledge bases are harnessed by constructing a notion of semantic summaries for intra-document co-reference chains using co-occurring entity mentions belonging to different chains. Second we reduce the computational cost by a new algorithm that embeds sample-based bisection using spectral clustering or graph partitioning in a hierarchical clustering process. This allows scaling up CCR to large corpora. Experiments with three datasets show significant gains in output quality compared to the best prior methods and the run-time efficiency of CROCS.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-Document Co-Reference Resolution using Sample-Based Clustering with Knowledge Enrichment</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Q15-1002" target="_blank">https://aclanthology.org/Q15-1002</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Inferring the information structure of scientific documents is useful for many NLP applications. Existing approaches to this task require substantial human effort. We propose a framework for constraint learning that reduces human involvement considerably. Our model uses topic models to identify latent topics and their key linguistic features in input documents induces constraints from this information and maps sentences to their dominant information structure categories through a constrained unsupervised model. When the induced constraints are combined with a fully unsupervised model the resulting model challenges existing lightly supervised feature-based models as well as unsupervised models that use manually constructed declarative knowledge. Our results demonstrate that useful declarative knowledge can be learned from data with very limited human involvement.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Declarative Knowledge Induction for Constraint-Based Learning of Information Structure in Scientific Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Q15-1010" target="_blank">https://aclanthology.org/Q15-1010</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Classification by Inversion of Distributed Language Representations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P15-2008" target="_blank">https://aclanthology.org/P15-2008</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Level Time-anchoring for TimeLine Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P15-2059" target="_blank">https://aclanthology.org/P15-2059</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Decomposition of a Multi-Author Document Based on Naive-Bayesian Model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P15-2082" target="_blank">https://aclanthology.org/P15-2082</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Bilingual Word Embeddings from Non-Parallel Document-Aligned Data Applied to Bilingual Lexicon Induction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P15-2118" target="_blank">https://aclanthology.org/P15-2118</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Encoding Distributional Semantics into Triple-Based Knowledge Ranking for Document Enrichment</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P15-1051" target="_blank">https://aclanthology.org/P15-1051</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Summarization of Multi-Document Topic Hierarchies using Submodular Mixtures</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P15-1054" target="_blank">https://aclanthology.org/P15-1054</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learning Semantic Representations of Users and Products for Document Level Sentiment Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P15-1098" target="_blank">https://aclanthology.org/P15-1098</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Hierarchical Neural Autoencoder for Paragraphs and Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P15-1107" target="_blank">https://aclanthology.org/P15-1107</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Abstractive Multi-Document Summarization via Phrase Selection and Merging</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P15-1153" target="_blank">https://aclanthology.org/P15-1153</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>節錄式語音文件摘要使用表示法學習技術 (Extractive Spoken Document Summarization with Representation Learning Techniques) [In Chinese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O15-3004" target="_blank">https://aclanthology.org/O15-3004</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>表示法學習技術於節錄式語音文件摘要之研究(A Study on Representation Learning Techniques for Extractive Spoken Document Summarization) [In Chinese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O15-1001" target="_blank">https://aclanthology.org/O15-1001</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Discourse and Document-level Information for Evaluating Language Output Tasks</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N15-2016" target="_blank">https://aclanthology.org/N15-2016</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Speeding Document Annotation with Topic Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N15-2017" target="_blank">https://aclanthology.org/N15-2017</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using External Resources and Joint Learning for Bigram Weighting in ILP-Based Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N15-1079" target="_blank">https://aclanthology.org/N15-1079</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Code-Switching for Multilingual Historical Document Transcription</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N15-1109" target="_blank">https://aclanthology.org/N15-1109</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Clustering Sentences with Density Peaks for Multi-document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N15-1136" target="_blank">https://aclanthology.org/N15-1136</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Sampling Techniques for Streaming Cross Document Coreference Resolution</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N15-1158" target="_blank">https://aclanthology.org/N15-1158</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Making the Most of Crowdsourced Document Annotations: Confused Supervised LDA</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/K15-1020" target="_blank">https://aclanthology.org/K15-1020</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Recovering Traceability Links in Requirements Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/K15-1024" target="_blank">https://aclanthology.org/K15-1024</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Semantic similarity forms a central component in many NLP systems from lexical semantics to part of speech tagging to social media analysis. Recent years have seen a renewed interest in developing new similarity techniques buoyed in part by work on embeddings and by SemEval tasks in Semantic Textual Similarity and Cross-Level Semantic Similarity. The increased interest has led to hundreds of techniques for measuring semantic similarity which makes it difficult for practitioners to identify which state-of-the-art techniques are applicable and easily integrated into projects and for researchers to identify which aspects of the problem require future research.This tutorial synthesizes the current state of the art for measuring semantic similarity for all types of conceptual or textual pairs and presents a broad overview of current techniques what resources they use and the particular inputs or domains to which the methods are most applicable. We survey methods ranging from corpus-based approaches operating on massive or domains-specific corpora to those leveraging structural information from expert-based or collaboratively-constructed lexical resources. Furthermore we review work on multiple similarity tasks from sense-based comparisons to word sentence and document-sized comparisons and highlight general-purpose methods capable of comparing multiple types of inputs. Where possible we also identify techniques that have been demonstrated to successfully operate in multilingual or cross-lingual settings.Our tutorial provides a clear overview of currently-available tools and their strengths for practitioners who need out of the box solutions and provides researchers with an understanding of the limitations of current state of the art and what open problems remain in the field. Given the breadth of available approaches participants will also receive a detailed bibliography of approaches (including those not directly covered in the tutorial) annotated according to the approaches abilities and pointers to when open-source implementations of the algorithms may be obtained.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Semantic Similarity Frontiers: From Concepts to Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D15-2001" target="_blank">https://aclanthology.org/D15-2001</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>System Combination for Multi-document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D15-1011" target="_blank">https://aclanthology.org/D15-1011</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-document Event Coreference Resolution based on Cross-media Features</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D15-1020" target="_blank">https://aclanthology.org/D15-1020</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Named entity recognition with document-specific KB tag gazetteers</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D15-1058" target="_blank">https://aclanthology.org/D15-1058</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>LDTM: A Latent Document Type Model for Cumulative Citation Recommendation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D15-1066" target="_blank">https://aclanthology.org/D15-1066</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Online Sentence Novelty Scoring for Topical Document Streams</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D15-1067" target="_blank">https://aclanthology.org/D15-1067</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Image-Mediated Learning for Zero-Shot Cross-Lingual Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D15-1070" target="_blank">https://aclanthology.org/D15-1070</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Summarizing Topical Contents from PubMed Documents Using a Thematic Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D15-1094" target="_blank">https://aclanthology.org/D15-1094</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Framework for Comparing Groups of Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D15-1100" target="_blank">https://aclanthology.org/D15-1100</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>C3EL: A Joint Model for Cross-Document Co-Reference Resolution and Entity Linking</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D15-1101" target="_blank">https://aclanthology.org/D15-1101</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Hierarchical Recurrent Neural Network for Document Modeling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D15-1106" target="_blank">https://aclanthology.org/D15-1106</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Modeling with Gated Recurrent Neural Network for Sentiment Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D15-1167" target="_blank">https://aclanthology.org/D15-1167</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Reading Documents for Bayesian Online Change Point Detection</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D15-1184" target="_blank">https://aclanthology.org/D15-1184</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Abstractive Multi-document Summarization with Semantic Information Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D15-1219" target="_blank">https://aclanthology.org/D15-1219</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Better Document-level Sentiment Analysis from RST Discourse Parsing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D15-1263" target="_blank">https://aclanthology.org/D15-1263</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Timescale Long Short-Term Memory Neural Network for Modelling Sentences and Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D15-1280" target="_blank">https://aclanthology.org/D15-1280</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Quality assessment of postedited versus translated wildlife documentary films: a three-level approach</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2015.mtsummit-wptp.2" target="_blank">https://aclanthology.org/2015.mtsummit-wptp.2</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Quality evaluation of four translations of a kidney document: focus on reliability</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2015.mtsummit-users.8" target="_blank">https://aclanthology.org/2015.mtsummit-users.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Productivity promotion strategies for collaborative translation on huge volume technical documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2015.mtsummit-users.19" target="_blank">https://aclanthology.org/2015.mtsummit-users.19</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A machine-assisted human translation system for technical documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2015.mtsummit-users.20" target="_blank">https://aclanthology.org/2015.mtsummit-users.20</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Japanese controlled language rules to improve machine translatability of municipal documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2015.mtsummit-papers.8" target="_blank">https://aclanthology.org/2015.mtsummit-papers.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Les travaux men&#39;es dans le cadre du r&#39;esum&#39;e automatique de texte ont montr&#39;e des r&#39;esultats `a la fois tr`es encourageants mais qui sont toujours `a am&#39;eliorer. La probl&#39;ematique du r&#39;esum&#39;e automatique ne cesse d&#39;&#39;evoluer avec les nouveaux champs d&#39;application qui s&#39;imposent ce qui augmente les contraintes li&#39;ees `a cette t^ache. Nous nous int&#39;e- ressons au r&#39;esum&#39;e extractif multi-document dynamique. Pour cela nous examinons les diff&#39;erentes approches existantes en mettant l&#39;accent sur les travaux les plus r&#39;ecents. Nous montrons ensuite que la performance des syst`emes de r&#39;esum&#39;e multi-document et dynamique est encore modeste. Trois contraintes suppl&#39;ementaires sont ajout&#39;ees : la redondance inter-document la redondance `a travers le temps et la grande taille des donn&#39;ees `a traiter. Nous essayons de d&#39;eceler les insuffisances des syst`emes existants afin de bien d&#39;efinir notre probl&#39;ematique et guider ainsi nos prochains travaux.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>R&#39;esum&#39;e Automatique Multi-Document Dynamique : &#39;Etat de l&#39;Art</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2015.jeptalnrecital-recital.4" target="_blank">https://aclanthology.org/2015.jeptalnrecital-recital.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Evaluation approaches for unsupervised rank-based keyword assignment are nearly as numerous as are the existing systems. The prolific production of each newly used metric (or metric twist) seems to stem from general dis-satisfaction with the previous one and the source of that dissatisfaction has not previously been discussed in the literature. The difficulty may stem from a poor specification of the keyword assignment task in view of the rank-based approach. With a more complete specification of this task we aim to show why the previous evaluation metrics fail to satisfy researchers&#39; goals to distinguish and detect good rank-based keyword assignment systems. We put forward a characterisation of an ideal evaluation metric and discuss the consistency of the evaluation metrics with this ideal finding that the average standard normalised cumulative gain metric is most consistent with this ideal.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A critical survey on measuring success in rank-based keyword assignment to documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2015.jeptalnrecital-court.9" target="_blank">https://aclanthology.org/2015.jeptalnrecital-court.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents an exhaustive study on the generation of graph input to unsupervised graph-based non-contextual single document keyword extraction systems. A concrete hypothesis on concept coordination for documents that are scientific articles is put forward consistent with two separate graph models : one which is based on word adjacency in the linear text--an approach forming the foundation of all previous graph-based keyword extraction methods and a novel one that is based on word adjacency modulo their modifiers. In doing so we achieve a best reported NDCG score to date of 0.431 for any system on the same data. In terms of a best parameter f-score we achieve the highest reported to date (0.714) at a reasonable ranked list cut-off of n = 6 which is also the best reported f-score for any keyword extraction or generation system in the literature on the same data. The best-parameter f-score corresponds to a reduction in error of 12.6% conservatively.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Effects of Graph Generation for Unsupervised Non-Contextual Single Document Keyword Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2015.jeptalnrecital-court.10" target="_blank">https://aclanthology.org/2015.jeptalnrecital-court.10</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Risk-aware distribution of SMT outputs for translation of documents targeting many anonymous readers</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2015.iwslt-papers.17" target="_blank">https://aclanthology.org/2015.iwslt-papers.17</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Machine Translation with Word Vector Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2015.eamt-1.9" target="_blank">https://aclanthology.org/2015.eamt-1.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2015</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Searching for Context: a Study on Document-Level Labels for Translation Quality Estimation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2015.eamt-1.17" target="_blank">https://aclanthology.org/2015.eamt-1.17</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Topic-based Multi-document Summarization using Differential Evolution forCombinatorial Optimization of Sentences</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y14-1020" target="_blank">https://aclanthology.org/Y14-1020</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>On-line Summarization of Time-series Documents using a Graph-based Algorithm</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y14-1054" target="_blank">https://aclanthology.org/Y14-1054</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Significance of Bridging Real-world Documents and NLP Technologies</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W14-5205" target="_blank">https://aclanthology.org/W14-5205</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Assigning Terms to Domains by Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W14-4802" target="_blank">https://aclanthology.org/W14-4802</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Identification of Bilingual Terms from Monolingual Documents for Statistical Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W14-4803" target="_blank">https://aclanthology.org/W14-4803</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Hybrid Approach to Multi-document Summarization of Opinions in Reviews</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W14-4408" target="_blank">https://aclanthology.org/W14-4408</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-document Summarization Using Bipartite Graphs</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W14-3703" target="_blank">https://aclanthology.org/W14-3703</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>FFTM: A Fuzzy Feature Transformation Method for Medical Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W14-3419" target="_blank">https://aclanthology.org/W14-3419</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Aikuma: A Mobile App for Collaborative Language Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W14-2201" target="_blank">https://aclanthology.org/W14-2201</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Documenting Endangered Languages with the WordsEye Linguistics Tool</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W14-2202" target="_blank">https://aclanthology.org/W14-2202</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Requirement Mining in Technical Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W14-2118" target="_blank">https://aclanthology.org/W14-2118</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Open Corpus of Everyday Documents for Simplification Tasks</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W14-1210" target="_blank">https://aclanthology.org/W14-1210</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extracting Multiword Translations from Aligned Comparable Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W14-1016" target="_blank">https://aclanthology.org/W14-1016</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automated Error Detection in Digitized Cultural Heritage Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W14-0608" target="_blank">https://aclanthology.org/W14-0608</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Gazetteer and Georeferencing for Historical English Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W14-0617" target="_blank">https://aclanthology.org/W14-0617</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Less Destructive Cleaning of Web Documents by Using Standoff Annotation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W14-0403" target="_blank">https://aclanthology.org/W14-0403</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Impact of Citing Papers for Summarisation of Clinical Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/U14-1010" target="_blank">https://aclanthology.org/U14-1010</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Language identification is the task of automatically detecting the language(s) present in a document based on the content of the document. In this work we address the problem of detecting documents that contain text from more than one language (multilingual documents). We introduce a method that is able to detect that a document is multilingual identify the languages present and estimate their relative proportions. We demonstrate the effectiveness of our method over synthetic data as well as real-world multilingual documents collected from the web.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Detection and Language Identification of Multilingual Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Q14-1003" target="_blank">https://aclanthology.org/Q14-1003</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-document summarization using distortion-rate ratio</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P14-3009" target="_blank">https://aclanthology.org/P14-3009</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Effective Document-Level Features for Chinese Patent Word Segmentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P14-2033" target="_blank">https://aclanthology.org/P14-2033</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Detection of Topic and its Extrinsic Evaluation Through Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P14-2040" target="_blank">https://aclanthology.org/P14-2040</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Single Document Summarization based on Nested Tree Structure</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P14-2052" target="_blank">https://aclanthology.org/P14-2052</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Two-Stage Hashing for Fast Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P14-2081" target="_blank">https://aclanthology.org/P14-2081</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learning Polylingual Topic Models from Code-Switched Social Media Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P14-2110" target="_blank">https://aclanthology.org/P14-2110</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Adaptive HTER Estimation for Document-Specific MT Post-Editing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P14-1081" target="_blank">https://aclanthology.org/P14-1081</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Hierarchical Summarization: Scaling Up Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P14-1085" target="_blank">https://aclanthology.org/P14-1085</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploiting Timelines to Enhance Multi-document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P14-1087" target="_blank">https://aclanthology.org/P14-1087</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Approach for Automatic Keyword Extraction from Arabic Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O14-1018" target="_blank">https://aclanthology.org/O14-1018</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper deals with information retrieval on semantically enriched web-scale document collections. It particularly focuses on web-crawled content in which mentions of entities appearing in Freebase DBpedia and other Linked Open Data resources have been identified. A special attention is paid to indexing structures and advanced query mechanisms that have been employed into a new semantic retrieval system. Scalability features are discussed together with performance statistics and results of experimental evaluation of presented approaches. Examples given to demonstrate key features of the developed solution correspond to the cultural heritage domain in which the results of our work have been primarily applied.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Semantic Search in Documents Enriched by LOD-based Annotations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/1058_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2014/pdf/1058_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Data annotation in modern practice often involves multiple imperfect human annotators. Multiple annotations can be used to infer estimates of the ground-truth labels and to estimate individual annotator error characteristics (or reliability). We introduce MomResp a model that incorporates information from both natural data clusters as well as annotations from multiple annotators to infer ground-truth labels and annotator reliability for the document classification task. We implement this model and show dramatic improvements over majority vote in situations where both annotations are scarce and annotation quality is low as well as in situations where annotators disagree consistently. Because MomResp predictions are subject to label switching we introduce a solution that finds nearly optimal predicted class reassignments in a variety of settings using only information available to the model at inference time. Although MomResp does not perform well in annotation-rich situations we show evidence suggesting how this shortcoming may be overcome in future work.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Momresp: A Bayesian Model for Multi-Annotator Document Labeling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/1153_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2014/pdf/1153_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Language documentation projects supported by recent funding intiatives have created a large number of multimedia corpora of typologically diverse languages. Most of these corpora provide a manual alignment of transcription and audio data at the level of larger units such as sentences or intonation units. Their usefulness both for corpus-linguistic and psycholinguistic research and for the development of tools and teaching materials could however be increased by achieving a more fine-grained alignment of transcription and audio at the word or even phoneme level. Since most language documentation corpora contain data on small languages there usually do not exist any speech recognizers or acoustic models specifically trained on these languages. We therefore investigate the feasibility of untrained forced alignment for such corpora. We report on an evaluation of the tool (Web)MAUS (Kisler 2012) on several language documentation corpora and discuss practical issues in the application of forced alignment. Our evaluation shows that (Web)MAUS with its existing acoustic models combined with simple grapheme-to-phoneme conversion can be successfully used for word-level forced alignment of a diverse set of languages without additional training especially if a manual prealignment of larger annotation units is already avaible.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Untrained Forced Alignment of Transcriptions and Audio for Language Documentation Corpora using WebMAUS</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/1176_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2014/pdf/1176_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we introduce the Priberam Compressive Summarization Corpus a new multi-document summarization corpus for European Portuguese. The corpus follows the format of the summarization corpora for English in recent DUC and TAC conferences. It contains 80 manually chosen topics referring to events occurred between 2010 and 2013. Each topic contains 10 news stories from major Portuguese newspapers radio and TV stations along with two human generated summaries up to 100 words. Apart from the language one important difference from the DUC/TAC setup is that the human summaries in our corpus are textbackslashemphcompressive: the annotators performed only sentence and word deletion operations as opposed to generating summaries from scratch. We use this corpus to train and evaluate learning-based extractive and compressive summarization systems providing an empirical comparison between these two approaches. The corpus is made freely available in order to facilitate research on automatic summarization.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Priberam Compressive Summarization Corpus: A New Multi-Document Summarization Corpus for European Portuguese</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/187_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2014/pdf/187_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Crowdsourcing Smartphone Application for Swiss German: Putting Language Documentation in the Hands of the Users</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/214_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2014/pdf/214_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we present the evaluation of our automatic methods for detecting and extracting document structure in annual financial reports. The work presented is part of the Corporate Financial Information Environment (CFIE) project in which we are using Natural Language Processing (NLP) techniques to study the causes and consequences of corporate disclosure and financial reporting outcomes. We aim to uncover the determinants of financial reporting quality and the factors that influence the quality of information disclosed to investors beyond the financial statements. The CFIE consists of the supply of information by firms to investors and the mediating influences of information intermediaries on the timing relevance and reliability of information available to investors. It is important to compare and contrast specific elements or sections of each annual financial report across our entire corpus rather than working at the full document level. We show that the values of some metrics e.g. readability will vary across sections thus improving on previous research research based on full texts.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Detecting Document Structure in a Very Large Corpus of UK Financial Reports</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/402_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2014/pdf/402_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The authors have written the Ethic and Big Data Charter in collaboration with various agencies private bodies and associations. This Charter aims at describing any large or complex resources and in particular language resources from a legal and ethical viewpoint and ensuring the transparency of the process of creating and distributing such resources. We propose in this article an analysis of the documentation coverage of the most frequently mentioned language resources with regards to the Charter in order to show the benefit it offers.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Evaluating corpora documentation with regards to the Ethics and Big Data Charter</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/424_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2014/pdf/424_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We motivate and describe the design and development of an emerging encyclopedia of compositional semantics pursuing three objectives. We first seek to compile a comprehensive catalogue of interoperable semantic analyses i.e. a precise characterization of meaning representations for a broad range of common semantic phenomena. Second we operationalize the discovery of semantic phenomena and their definition in terms of what we call their semantic fingerprint a formal account of the building blocks of meaning representation involved and their configuration. Third we ground our work in a carefully constructed semantic test suite of minimal exemplars for each phenomenon along with a `target&#39; fingerprint that enables automated regression testing. We work towards these objectives by codifying and documenting the body of knowledge that has been constructed in a long-term collaborative effort the development of the LinGO English Resource Grammar. Documentation of its semantic interface is a prerequisite to use by non-experts of the grammar and the analyses it produces but this effort also advances our own understanding of relevant interactions among phenomena as well as of areas for future work in the grammar.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards an Encyclopedia of Compositional Semantics: Documenting the Interface of the English Resource Grammar</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/562_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2014/pdf/562_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present LQVSumm a corpus of about 2000 automatically created extractive multi-document summaries from the TAC 2011 shared task on Guided Summarization which we annotated with several types of linguistic quality violations. Examples for such violations include pronouns that lack antecedents or ungrammatical clauses. We give details on the annotation scheme and show that inter-annotator agreement is good given the open-ended nature of the task. The annotated summaries have previously been scored for Readability on a numeric scale by human annotators in the context of the TAC challenge; we show that the number of instances of violations of linguistic quality of a summary correlates with these intuitively assigned numeric scores. On a system-level the average number of violations marked in a system&#39;s summaries achieves higher correlation with the Readability scores than current supervised state-of-the-art methods for assigning a single readability score to a summary. It is our hope that our corpus facilitates the development of methods that not only judge the linguistic quality of automatically generated summaries as a whole but which also allow for detecting labeling and fixing particular violations in a text.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>LQVSumm: A Corpus of Linguistic Quality Violations in Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/578_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2014/pdf/578_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Event coreference is an important task for full text analysis. However previous work uses a variety of approaches sources and evaluation making the literature confusing and the results incommensurate. We provide a description of the differences to facilitate future research. Second we present a supervised method for event coreference resolution that uses a rich feature set and propagates information alternatively between events and their arguments adapting appropriately for each type of argument.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Supervised Within-Document Event Coreference using Information Propagation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/646_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2014/pdf/646_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we present CROMER (CROss-document Main Events and entities Recognition) a novel tool to manually annotate event and entity coreference across clusters of documents. The tool has been developed so as to handle large collections of documents perform collaborative annotation (several annotators can work on the same clusters) and enable the linking of the annotated data to external knowledge sources. Given the availability of semantic information encoded in Semantic Web resources this tool is designed to support annotators in linking entities and events to DBPedia and Wikipedia so as to facilitate the automatic retrieval of additional semantic information. In this way event modelling and chaining is made easy while guaranteeing the highest interconnection with external resources. For example the tool can be easily linked to event models such as the Simple Event Model [Van Hage et al 2011] and the Grounded Annotation Framework [Fokkens et al. 2013].</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CROMER: a Tool for Cross-Document Event and Entity Coreference</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/726_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2014/pdf/726_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The Stanford Coreference Resolution System (StCR) is a multi-pass rule-based system that scored best in the CoNLL 2011 shared task on general discourse coreference resolution. We describe how the StCR has been adapted to the specific domain of patents and give some cues on how it can be adapted to other domains. We present a linguistic analysis of the patent domain and how we were able to adapt the rules to the domain and to expand coreferences with some lexical chains. A comparative evaluation shows an improvement of the coreference resolution system denoting that (i) StCR is a valuable tool across different text genres; (ii) specialized discourse NLP may significantly benefit from general discourse NLP research.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Exercise in Reuse of Resources: Adapting General Discourse Coreference Resolution for Detecting Lexical Chains in Patent Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/850_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2014/pdf/850_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This article introduces a novel protocol and resource to evaluate Web-as-corpus topical document retrieval. To the contrary of previous work our goal is to provide an automatic reproducible and robust evaluation for this task. We rely on the OpenDirectory (DMOZ) as a source of topically annotated webpages and index them in a search engine. With this OpenDirectory search engine we can then easily evaluate the impact of various parameters such as the number of seed terms queries or documents or the usefulness of various term selection algorithms. A first fully automatic evaluation is described and provides baseline performances for this task. The article concludes with practical information regarding the availability of the index and resource files.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Evaluating Web-as-corpus Topical Document Retrieval with an Index of the OpenDirectory</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/980_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2014/pdf/980_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Utilisabilit&#39;e d&#39;une ressource propri&#39;etaire riche dans le cadre de la classification de documents [in French]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/F14-3002" target="_blank">https://aclanthology.org/F14-3002</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Centrality Measures for Non-Contextual Graph-Based Unsupervised Single Document Keyword Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/F14-2012" target="_blank">https://aclanthology.org/F14-2012</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic identification of document sections for designing a French clinical corpus (Identification automatique de zones dans des documents pour la constitution d&#39;un corpus m&#39;edical en franccais) [in French]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/F14-2030" target="_blank">https://aclanthology.org/F14-2030</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Study of Domain Dependant Multi-Polarity Words for Document Level Opinion Mining (Influence des marqueurs multi-polaires d&#39;ependant du domaine pour la fouille d&#39;opinion au niveau du texte) [in French]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/F14-1001" target="_blank">https://aclanthology.org/F14-1001</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Detection of Document Organizational Structure from Visual and Lexical Markers (D&#39;etection automatique de la structure organisationnelle de documents `a partir de marqueurs visuels et lexicaux) [in French]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/F14-1030" target="_blank">https://aclanthology.org/F14-1030</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Knowledge-based Representation for Cross-Language Document Retrieval and Categorization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/E14-1044" target="_blank">https://aclanthology.org/E14-1044</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving the Estimation of Word Importance for News Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/E14-1075" target="_blank">https://aclanthology.org/E14-1075</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An I-vector Based Approach to Compact Multi-Granularity Topic Spaces Representation of Textual Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D14-1051" target="_blank">https://aclanthology.org/D14-1051</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Modeling Term Translation for Document-informed Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D14-1060" target="_blank">https://aclanthology.org/D14-1060</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Semantic-Based Multilingual Document Clustering via Tensor Modeling</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D14-1065" target="_blank">https://aclanthology.org/D14-1065</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Fear the REAPER: A System for Automatic Multi-Document Summarization with Reinforcement Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D14-1075" target="_blank">https://aclanthology.org/D14-1075</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Multi-documents Summarization by Sentence Compression based on Expanded Constituent Parse Trees</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D14-1076" target="_blank">https://aclanthology.org/D14-1076</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Analyzing Stemming Approaches for Turkish Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D14-1077" target="_blank">https://aclanthology.org/D14-1077</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Dependency-based Discourse Parser for Single-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D14-1196" target="_blank">https://aclanthology.org/D14-1196</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Author Verification Using Common N-Gram Profiles of Text Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C14-1038" target="_blank">https://aclanthology.org/C14-1038</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Enforcing Topic Diversity in a Document Recommender for Conversations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C14-1056" target="_blank">https://aclanthology.org/C14-1056</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>docrep: A lightweight and efficient document representation framework</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C14-1072" target="_blank">https://aclanthology.org/C14-1072</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Triple based Background Knowledge Ranking for Document Enrichment</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C14-1087" target="_blank">https://aclanthology.org/C14-1087</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Query-focused Multi-Document Summarization: Combining a Topic Model with Graph-based Semi-supervised Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C14-1113" target="_blank">https://aclanthology.org/C14-1113</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Ranking Multidocument Event Descriptions for Building Thematic Timelines</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C14-1114" target="_blank">https://aclanthology.org/C14-1114</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Single Document Keyphrase Extraction Using Label Information</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C14-1139" target="_blank">https://aclanthology.org/C14-1139</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The dos and don&#39;ts of XML document localization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2014.tc-1.19" target="_blank">https://aclanthology.org/2014.tc-1.19</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-level translation quality estimation: exploring discourse and pseudo-references</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2014.eamt-1.21" target="_blank">https://aclanthology.org/2014.eamt-1.21</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>When to choose SMT: typology of documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2014.amta-users.4" target="_blank">https://aclanthology.org/2014.amta-users.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2014</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We introduce two document-level features to polish baseline sentence-level translations generated by a state-of-the-art statistical machine translation (SMT) system. One feature uses the word-embedding technique to model the relation between a sentence and its context on the target side; the other feature is a crisp document-level token-type ratio of target-side translations for source-side words to model the lexical consistency in translation. The weights of introduced features are tuned to optimize the sentence- and document-level metrics simultaneously on the basis of Pareto optimality. Experimental results on two different schemes with different corpora illustrate that the proposed approach can efficiently and stably integrate document-level information into a sentence-level SMT system. The best improvements were approximately 0.5 BLEU on test sets with statistical significance.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-level re-ranking with soft lexical and semantic features for statistical machine translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2014.amta-researchers.9" target="_blank">https://aclanthology.org/2014.amta-researchers.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Parsing: Towards Realistic Syntactic Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W13-5715" target="_blank">https://aclanthology.org/W13-5715</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using Factual Density to Measure Informativeness of Web Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W13-5621" target="_blank">https://aclanthology.org/W13-5621</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Desambiguacc~ao Lexical de Sentido com uso de Informacc~ao Multidocumento por meio de Redes de Co-ocorr^encia (Word Sense Disambiguation with the Use of Multi-document Information with Cooccurrence Nets) [in Portuguese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W13-4815" target="_blank">https://aclanthology.org/W13-4815</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Documents and Dependencies: an Exploration of Vector Space Models for Semantic Composition</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W13-3510" target="_blank">https://aclanthology.org/W13-3510</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W13-3100" target="_blank">https://aclanthology.org/W13-3100</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-document multilingual summarization corpus preparation Part 1: Arabic English Greek Chinese Romanian</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W13-3101" target="_blank">https://aclanthology.org/W13-3101</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-document multilingual summarization corpus preparation Part 2: Czech Hebrew and Spanish</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W13-3102" target="_blank">https://aclanthology.org/W13-3102</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-document multilingual summarization and evaluation tracks in ACL 2013 MultiLing Workshop</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W13-3103" target="_blank">https://aclanthology.org/W13-3103</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CIST System Report for ACL MultiLing 2013 -- Track 1: Multilingual Multi-document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W13-3105" target="_blank">https://aclanthology.org/W13-3105</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilingual Multi-Document Summarization with POLY2</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W13-3106" target="_blank">https://aclanthology.org/W13-3106</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using a Keyness Metric for Single and Multi Document Summarisation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W13-3109" target="_blank">https://aclanthology.org/W13-3109</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilingual Single-Document Summarization with MUSE</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W13-3111" target="_blank">https://aclanthology.org/W13-3111</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learning Comparable Corpora from Latent Semantic Analysis Simplified Document Space</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W13-2516" target="_blank">https://aclanthology.org/W13-2516</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Online Polylingual Topic Models for Fast Document Translation Detection</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W13-2232" target="_blank">https://aclanthology.org/W13-2232</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Deconstructing Human Literature Reviews -- A Framework for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W13-2116" target="_blank">https://aclanthology.org/W13-2116</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploring MWEs for Knowledge Acquisition from Corporate Technical Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W13-1012" target="_blank">https://aclanthology.org/W13-1012</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Classifying English Documents by National Dialect</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/U13-1003" target="_blank">https://aclanthology.org/U13-1003</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>INAOE_UPV-CORE: Extracting Word Associations from Document Corpora to estimate Semantic Textual Similarity</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/S13-1033" target="_blank">https://aclanthology.org/S13-1033</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Answering Questions from Multiple Documents -- the Role of Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/R13-2003" target="_blank">https://aclanthology.org/R13-2003</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Document Summarization using Automatic Key-Phrase Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/R13-2004" target="_blank">https://aclanthology.org/R13-2004</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>More than Bag-of-Words: Sentence-based Document Representation for Sentiment Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/R13-1072" target="_blank">https://aclanthology.org/R13-1072</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Supervised learning methods and LDA based topic model have been successfully applied in the field of multi-document summarization. In this paper we propose a novel supervised approach that can incorporate rich sentence features into Bayesian topic models in a principled way thus taking advantages of both topic model and feature based supervised learning methods. Experimental results on DUC2007 TAC2008 and TAC2009 demonstrate the effectiveness of our approach.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Novel Feature-based Bayesian Model for Query Focused Multi-document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Q13-1008" target="_blank">https://aclanthology.org/Q13-1008</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P13-4033" target="_blank">https://aclanthology.org/P13-4033</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Categorization of Turkish News Documents with Morphological Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P13-3001" target="_blank">https://aclanthology.org/P13-3001</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Bilingual Lexical Cohesion Trigger Model for Document-Level Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P13-2068" target="_blank">https://aclanthology.org/P13-2068</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Transcription of Historical Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P13-1021" target="_blank">https://aclanthology.org/P13-1021</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P13-1048" target="_blank">https://aclanthology.org/P13-1048</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P13-1121" target="_blank">https://aclanthology.org/P13-1121</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P13-1136" target="_blank">https://aclanthology.org/P13-1136</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DALE: A Word Sense Disambiguation System for Biomedical Documents Trained using Automatically Labeled Examples</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N13-3001" target="_blank">https://aclanthology.org/N13-3001</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using Document Summarization Techniques for Speech Data Subset Selection</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N13-1086" target="_blank">https://aclanthology.org/N13-1086</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improved Information Structure Analysis of Scientific Documents Through Discourse and Lexical Constraints</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N13-1113" target="_blank">https://aclanthology.org/N13-1113</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Labeling the Languages of Words in Mixed-Language Documents using Weakly Supervised Methods</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N13-1131" target="_blank">https://aclanthology.org/N13-1131</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards Coherent Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N13-1136" target="_blank">https://aclanthology.org/N13-1136</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Keyphrase-Driven Document Visualization Tool</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I13-2005" target="_blank">https://aclanthology.org/I13-2005</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Applying Graph-based Keyword Extraction to Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I13-1108" target="_blank">https://aclanthology.org/I13-1108</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automated Activity Recognition in Clinical Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I13-1160" target="_blank">https://aclanthology.org/I13-1160</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilingual document clustering : state of the art (Construction de corpus multilingues : &#39;etat de l&#39;art) [in French]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/F13-5005" target="_blank">https://aclanthology.org/F13-5005</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic search session detection exploiting results similarity from an external document collection (D&#39;etection automatique des sessions de recherche par similarit&#39;e des r&#39;esultats provenant d&#39;une collection de documents externe) [in French]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/F13-5017" target="_blank">https://aclanthology.org/F13-5017</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extraction of temporal relations between clinical events in clinical documents (Extraction des relations temporelles entre &#39;ev&#39;enements m&#39;edicaux dans des comptes rendus hospitaliers) [in French]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/F13-2018" target="_blank">https://aclanthology.org/F13-2018</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using POMDPs for Topic-Focused Multi-Document Summarization (L&#39;utilisation des POMDP pour les r&#39;esum&#39;es multi-documents orient&#39;es par une th&#39;ematique) [in French]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/F13-1003" target="_blank">https://aclanthology.org/F13-1003</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Parallel areas detection in multi-documents for multilingual alignment (D&#39;etection de zones parall`eles `a l&#39;int&#39;erieur de multi-documents pour l&#39;alignement multilingue) [in French]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/F13-1028" target="_blank">https://aclanthology.org/F13-1028</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Web pages segmentation for document selection in Question Answering (Pr&#39;e-segmentation de pages web et s&#39;election de documents pertinents en Questions-R&#39;eponses) [in French]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/F13-1035" target="_blank">https://aclanthology.org/F13-1035</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Summarization via Guided Sentence Compression</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D13-1047" target="_blank">https://aclanthology.org/D13-1047</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatically Determining a Proper Length for Multi-Document Summarization: A Bayesian Nonparametric Approach</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D13-1069" target="_blank">https://aclanthology.org/D13-1069</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Application of Localized Similarity for Web Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D13-1142" target="_blank">https://aclanthology.org/D13-1142</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Inducing Document Plans for Concept-to-Text Generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D13-1157" target="_blank">https://aclanthology.org/D13-1157</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Single-Document Summarization as a Tree Knapsack Problem</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D13-1158" target="_blank">https://aclanthology.org/D13-1158</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Lexical Chain Based Cohesion Models for Document-Level Statistical Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D13-1163" target="_blank">https://aclanthology.org/D13-1163</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D13-1179" target="_blank">https://aclanthology.org/D13-1179</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2013</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Patent translation as technical document translation: customizing a Chinese-Korean MT system to patent domain</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2013.mtsummit-wpt.4" target="_blank">https://aclanthology.org/2013.mtsummit-wpt.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Building a Diverse Document Leads Corpus Annotated with Semantic Relations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y12-1058" target="_blank">https://aclanthology.org/Y12-1058</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Domain Based Classification of Punjabi Text Documents using Ontology and Hybrid Based Approach</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W12-5009" target="_blank">https://aclanthology.org/W12-5009</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Searching for English-Vietnamese Documents on the Internet</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W12-5019" target="_blank">https://aclanthology.org/W12-5019</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Topic Extraction based on Prior Knowledge obtained from Target Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W12-3306" target="_blank">https://aclanthology.org/W12-3306</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards an ACL Anthology Corpus with Logical Document Structure. An Overview of the ACL 2012 Contributed Task</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W12-3210" target="_blank">https://aclanthology.org/W12-3210</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Combining OCR Outputs for Logical Document Structure Markup. Technical Background to the ACL 2012 Contributed Task</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W12-3212" target="_blank">https://aclanthology.org/W12-3212</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Hybrid Stepwise Approach for De-identifying Person Names in Clinical Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W12-2408" target="_blank">https://aclanthology.org/W12-2408</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Natural Language Inspired Approach for Handwritten Text Line Detection in Legacy Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W12-1015" target="_blank">https://aclanthology.org/W12-1015</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Language Classification and Segmentation of Noisy Documents in Hebrew Scripts</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W12-1016" target="_blank">https://aclanthology.org/W12-1016</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Proceedings of the Second Workshop on Computational Linguistics and Writing (CL&amp;W 2012): Linguistic and Cognitive Aspects of Document Creation and Document Engineering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W12-0300" target="_blank">https://aclanthology.org/W12-0300</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Measuring Comparability of Documents in Non-Parallel Corpora for Efficient Extraction of (Semi-)Parallel Translation Equivalents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W12-0102" target="_blank">https://aclanthology.org/W12-0102</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards Two-step Multi-document Summarisation for Evidence Based Medicine: A Quantitative Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/U12-1011" target="_blank">https://aclanthology.org/U12-1011</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Detecting Semantic Equivalence and Information Disparity in Cross-lingual Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P12-2024" target="_blank">https://aclanthology.org/P12-2024</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Identifying High-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P12-2066" target="_blank">https://aclanthology.org/P12-2066</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Labeling Documents with Timestamps: Learning from their Time Expressions</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P12-1011" target="_blank">https://aclanthology.org/P12-1011</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>TQDL: Integrated Models for Cross-Language Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O12-5002" target="_blank">https://aclanthology.org/O12-5002</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>利用機器學習於中文法律文件之標記、案件分類及量刑預測 (Exploiting Machine Learning Models for Chinese Legal Documents Labeling Case Classification and Sentencing Prediction) [In Chinese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O12-5004" target="_blank">https://aclanthology.org/O12-5004</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Comparative Study of Methods for Topic Modeling in Spoken Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O12-2004" target="_blank">https://aclanthology.org/O12-2004</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>利用機器學習於中文法律文件之標記、案件分類及量刑預測 (Exploiting Machine Learning Models for Chinese Legal Documents Labeling Case Classification and Sentencing Prediction) [In Chinese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O12-1013" target="_blank">https://aclanthology.org/O12-1013</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Improvement in Cross-Language Document Retrieval Based on Statistical Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O12-1015" target="_blank">https://aclanthology.org/O12-1015</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present a novel graph-theoretic method for the initial annotation of high-confidence training data for bootstrapping sentiment classifiers. We estimate polarity using topic-specific PageRank. Sentiment information is propagated from an initial seed lexicon through a joint graph representation of words and documents. We report improved classification accuracies across multiple domains for the base models and the maximum entropy model bootstrapped from the PageRank annotation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Bootstrapping Sentiment Labels For Unannotated Documents With Polarity PageRank</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/124_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2012/pdf/124_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper aims at assessing the automatic labeling of an undocumented unknown unwritten and under-resourced language (Mo Piu) of the North Vietnam by an expert phonetician. In the previous stage of the work 7 sets of languages were chosen among Mandarin Vietnamese Khmer English French to compete in order to select the best models of languages to be used for the phonetic labeling of Mo Piu isolated words. Two sets of languages (1^Ambox^circ Mandarin + French 2^Ambox^circ Vietnamese + French) which got the best scores showed an additional distribution of their results. Our aim is now to study this distribution more precisely and more extensively in order to statistically select the best models of languages and among them the best sets of phonetic units which minimize the wrong phonetic automatic labeling.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Comparison between two models of language for the automatic phonetic labeling of an undocumented language of the South-Asia: the case of Mo Piu</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/208_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2012/pdf/208_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Due to the increase in the number and depth of analyses required over the text like entity recognition POS tagging syntactic analysis etc. the annotation in-line has become unpractical. In Natural Language Processing (NLP) some emphasis has been placed in finding an annotation method to solve this problem. A possibility is the standoff annotation. With this annotation style it is possible to add new levels of annotation without disturbing exiting ones with minimal knock on effects. This annotation will increase the possibility of adding more linguistic information as well as more possibilities for sharing textual resources. In this paper we present a tool developed in the framework of the European Metanet4u (Enhancing the European Linguistic Infrastructure GA 270893) for creating a multi-layered XML annotation scheme based on the GrAF proposal for standoff annotations.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Iula2Standoff: a tool for creating standoff documents for the IULACT</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/307_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2012/pdf/307_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Within the framework of the Quaero project we proposed a new definition of named entities based upon an extension of the coverage of named entities as well as the structure of those named entities. In this new definition the extended named entities we proposed are both hierarchical and compositional. In this paper we focused on the annotation of a corpus composed of press archives OCRed from French newspapers of December 1890. We present the methodology we used to produce the corpus and the characteristics of the corpus in terms of named entities annotation. This annotated corpus has been used in an evaluation campaign. We present this evaluation the metrics we used and the results obtained by the participants.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extended Named Entities Annotation on OCRed Documents: From Corpus Constitution to Evaluation Campaign</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/343_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2012/pdf/343_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In order to extract meaningful phrases from corpora (e. g. in an information retrieval context) intensive knowledge of the domain in question and the respective documents is generally needed. When moving to a new domain or language the underlying knowledge bases and models need to be adapted which is often time-consuming and labor-intensive. This paper adresses the described challenge of phrase extraction from documents in different domains and languages and proposes an approach which does not use comprehensive lexica and therefore can be easily transferred to new domains and languages. The effectiveness of the proposed approach is evaluated on user generated content and documents from the patent domain in English and German.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Resource-light Approach to Phrase Extraction for English and German Documents from the Patent Domain and User Generated Content</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/466_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2012/pdf/466_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>MultiUN is a multilingual parallel corpus extracted from the official documents of the United Nations. It is available in the six official languages of the UN and a small portion of it is also available in German. This paper presents a major update on the first public version of the corpus released in 2010. This version 2 consists of over 513091 documents including more than 9% of new documents retrieved from the United Nations official document system. We applied several modifications to the corpus preparation method. In this paper we describe the methods we used for processing the UN documents and aligning the sentences. The most significant improvement compared to the previous release is the newly added multilingual sentence alignment information. The alignment information is encoded together with the text in XML instead of additional files. Our representation of the sentence alignment allows quick construction of aligned texts parallel in arbitrary number of languages which is essential for building machine translation systems.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MultiUN v2: UN Documents with Multilingual Alignments</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/641_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2012/pdf/641_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We describe the evaluation framework for spoken document retrieval for the IR for the Spoken Documents Task conducted in the ninth NTCIR Workshop. The two parts of this task were a spoken term detection (STD) subtask and an ad hoc spoken document retrieval subtask (SDR). Both subtasks target search terms passages and documents included in academic and simulated lectures of the Corpus of Spontaneous Japanese. Seven teams participated in the STD subtask and five in the SDR subtask. The results obtained through the evaluation in the workshop are discussed.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Designing an Evaluation Framework for Spoken Term Detection and Spoken Document Retrieval at the NTCIR-9 SpokenDoc Task</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/693_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2012/pdf/693_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes the development of a statistical machine translation system between French and English for scientific papers. This system will be closely integrated into the French HAL open archive a collection of more than 100.000 scientific papers. We describe the creation of in-domain parallel and monolingual corpora the development of a domain specific translation system with the created resources and its adaptation using monolingual resources only. These techniques allowed us to improve a generic system by more than 10 BLEU points.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Translation of Scientific Documents in the HAL Archive</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/703_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2012/pdf/703_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The paper presents an approach to extract irregularities in document corpora where the documents originate from different sources and the analyst&#39;s interest is to find documents which are atypical for the given source. The main contribution of the paper is a voting-based approach to irregularity detection and its evaluation on a collection of newspaper articles from two sources: Western (UK and US) and local (Kenyan) media. The evaluation of a domain expert proves that the method is very effective in uncovering interesting irregularities in categorized document corpora.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Irregularity Detection in Categorized Document Corpora</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/706_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2012/pdf/706_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Reflecting the rapid growth of science technology and culture it has become common practice to consult tools on the World Wide Web for various terms. Existing search engines provide an enormous volume of information but retrieved information is not organized. Hand-compiled encyclopedias provide organized information but the quantity of information is limited. In this paper aiming to integrate the advantages of both tools we propose a method to organize a search result based on multiple viewpoints as in Wikipedia. Because viewpoints required for explanation are different depending on the type of a term such as animal and disease we model articles in Wikipedia to extract a viewpoint structure for each term type. To identify a set of term types we independently use manual annotation and automatic document clustering for Wikipedia articles. We also propose an effective feature for clustering of Wikipedia articles. We experimentally show that the document clustering reduces the cost for the manual annotation while maintaining the accuracy for modeling Wikipedia articles.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Effects of Document Clustering in Modeling Wikipedia-style Term Descriptions</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/714_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2012/pdf/714_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we present a metric that measures comparability of documents across different languages. The metric is developed within the FP7 ICT ACCURAT project as a tool for aligning comparable corpora on the document level; further these aligned comparable documents are used for phrase alignment and extraction of translation equivalents with the aim to extend phrase tables of statistical MT systems without the need to use parallel texts. The metric uses several features such as lexical information document structure keywords and named entities which are combined in an ensemble manner. We present the results by measuring the reliability and effectiveness of the metric and demonstrate its application and the impact for the task of parallel phrase extraction from comparable corpora.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Development and Application of a Cross-language Document Comparability Metric</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/804_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2012/pdf/804_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Increases in the use of web data for corpus-building coupled with the use of specialist single-use corpora make for an increasing reliance on language that changes quickly affecting the long-term validity of studies based on these methods. This drift&#39; through time affects both users of open-source corpora and those attempting to interpret the results of studies based on web data. The attrition of documents online also called link rot or document half-life has been studied many times for the purposes of optimising search engine web crawlers producing robust and reliable archival systems and ensuring the integrity of distributed information stores however the affect that attrition has upon corpora of varying construction remains largely unknown. This paper presents a preliminary investigation into the differences in attrition rate between corpora selected using different corpus construction methods. It represents the first step in a larger longitudinal analysis and as such presents URI-based content clues chosen to relate to studies from other areas. The ultimate goal of this larger study is to produce a detailed enumeration of the primary biases online and identify sampling strategies which control and minimise unwanted effects of document attrition.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Attrition in Web Corpora: an Exploration</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/806_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2012/pdf/806_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Custom machine translation (MT) engines systematically outperform general-domain MT engines when translating within the relevant custom domain. This paper investigates the use of the Jensen-Shannon divergence measure for automatically routing new documents within a translation system with multiple MT engines to the appropriate custom MT engine in order to obtain the best translation. Three distinct domains are compared and the impact of the language size and preprocessing of the documents on the Jensen-Shannon score is addressed. Six test datasets are then compared to the three known-domain corpora to predict which of the three custom MT engines they would be routed to at runtime given their Jensen-Shannon scores. The results are promising for incorporating this divergence measure into a translation workflow.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Assessing Divergence Measures for Automated Document Routing in an Adaptive MT System</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/843_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2012/pdf/843_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Document zone identification aims to automatically classify sequences of text-spans (e.g. sentences) within a document into predefined zone categories. Current approaches to document zone identification mostly rely on supervised machine learning methods which require a large amount of annotated data which is often difficult and expensive to obtain. In order to overcome this bottleneck we propose graphical models based on the popular Latent Dirichlet Allocation (LDA) model. The first model which we call zoneLDA aims to cluster the sentences into zone classes using only unlabelled data. We also study an extension of zoneLDA called zoneLDAb which makes distinction between common words and non-common words within the different zone types. We present results on two different domains: the scientific domain and the technical domain. For the latter one we propose a new document zone classification schema which has been annotated over a collection of 689 documents achieving a Kappa score of 85%. Overall our experiments show promising results for both of the domains outperforming the baseline model. Furthermore on the technical domain the performance of the models are comparable to the supervised approach using the same feature sets. We thus believe that graphical models are a promising avenue of research for automatic document zoning.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised document zone identification using probabilistic graphical models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/881_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2012/pdf/881_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This article reports an intrinsic automatic summarization evaluation in the scientific lecture domain. The lecture takes place in a Smart Room that has access to different types of documents produced from different media. An evaluation framework is presented to analyze the performance of systems producing summaries answering a user need. Several ROUGE metrics are used and a manual content responsiveness evaluation was carried out in order to analyze the performance of the evaluated approaches. Various multilingual summarization approaches are analyzed showing that the use of different types of documents outperforms the use of transcripts. In fact not using any part of the spontaneous speech transcription in the summary improves the performance of automatic summaries. Moreover the use of semantic information represented in the different textual documents coming from different media helps to improve summary quality.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Summarizing a multimodal set of documents in a Smart Room</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/882_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2012/pdf/882_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The extraction of dictionaries from parallel text corpora is an established technique. However as parallel corpora are a scarce resource in recent years the extraction of dictionaries using comparable corpora has obtained increasing attention. In order to find a mapping between languages almost all approaches suggested in the literature rely on a seed lexicon. The work described here achieves competitive results without requiring such a seed lexicon. Instead it presupposes mappings between comparable documents in different languages. For some common types of textual resources (e.g. encyclopedias or newspaper texts) such mappings are either readily available or can be established relatively easily. The current work is based on Wikipedias where the mappings between languages are determined by the authors of the articles. We describe a neural-network inspired algorithm which first characterizes each Wikipedia article by a number of keywords and then considers the identification of word translations as a variant of word alignment in a noisy environment. We present results and evaluations for eight language pairs involving Germanic Romanic and Slavic languages as well as Chinese.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Identifying Word Translations from Comparable Documents Without a Seed Lexicon</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/888_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2012/pdf/888_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>&#39;Etude comparative entre trois approches de r&#39;esum&#39;e automatique de documents arabes (Comparative Study of Three Approaches to Automatic Summarization of Arabic Documents) [in French]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/F12-2017" target="_blank">https://aclanthology.org/F12-2017</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>BiTermEx Un prototype d&#39;extraction de mots compos&#39;es `a partir de documents comparables via la m&#39;ethode compositionnelle (BiTermEx A prototype for the extraction of multiword terms from comparable documents through the compositional approach) [in French]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/F12-2035" target="_blank">https://aclanthology.org/F12-2035</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Une Approche de Recherche d&#39;Information Structur&#39;ee fond&#39;ee sur la Correction d&#39;Erreurs `a l&#39;Indexation des Documents (Structured Information Retrieval Approach based on Indexing Time Error Correction) [in French]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/F12-2048" target="_blank">https://aclanthology.org/F12-2048</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Segmentation et Regroupement en Locuteurs d&#39;une collection de documents audio (Cross-show speaker diarization) [in French]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/F12-1055" target="_blank">https://aclanthology.org/F12-1055</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Percol0 - un syst`eme multimodal de d&#39;etection de personnes dans des documents vid&#39;eo (Percol0 - A multimodal person detection system in video documents) [in French]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/F12-1070" target="_blank">https://aclanthology.org/F12-1070</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>D&#39;etection et caract&#39;erisation des r&#39;egions d&#39;erreurs dans des transcriptions de contenus multim&#39;edia : application `a la recherche des noms de personnes (Error region detection and characterization in transcriptions of multimedia documents : application to person name search) [in French]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/F12-1102" target="_blank">https://aclanthology.org/F12-1102</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>User Edits Classification Using Document Revision Histories</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/E12-1036" target="_blank">https://aclanthology.org/E12-1036</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Joint Entity and Event Coreference Resolution across Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D12-1045" target="_blank">https://aclanthology.org/D12-1045</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Discovering Diverse and Salient Threads in Document Collections</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D12-1065" target="_blank">https://aclanthology.org/D12-1065</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extending Machine Translation Evaluation Metrics with Lexical Cohesion to Document Level</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D12-1097" target="_blank">https://aclanthology.org/D12-1097</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Wide Decoding for Phrase-Based Statistical Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D12-1108" target="_blank">https://aclanthology.org/D12-1108</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Constructing Task-Specific Taxonomies for Document Collection Browsing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D12-1117" target="_blank">https://aclanthology.org/D12-1117</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Collaborative Computer-Assisted Translation Applied to Pedagogical Documents and Literary Works</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C12-3032" target="_blank">https://aclanthology.org/C12-3032</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Domain Based Classification of Punjabi Text Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C12-3037" target="_blank">https://aclanthology.org/C12-3037</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Domain Based Punjabi Text Document Clustering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C12-3049" target="_blank">https://aclanthology.org/C12-3049</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Subjective Logic Framework for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C12-2078" target="_blank">https://aclanthology.org/C12-2078</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document and Corpus Level Inference For Unsupervised and Transductive Learning of Information Structure of Scientific Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C12-2097" target="_blank">https://aclanthology.org/C12-2097</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Identifying Temporal Relations by Sentence and Document Optimizations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C12-2134" target="_blank">https://aclanthology.org/C12-2134</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Deriving Paraphrases for Highly Inflected Languages from Comparable Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C12-1012" target="_blank">https://aclanthology.org/C12-1012</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Deriving a Lexicon for a Precision Grammar from Language Documentation Resources: A Case Study of Chintang</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C12-1016" target="_blank">https://aclanthology.org/C12-1016</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>On the Effectiveness of using Sentence Compression Models for Query-Focused Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C12-1029" target="_blank">https://aclanthology.org/C12-1029</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Comparing Taxonomies for Organising Collections of Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C12-1054" target="_blank">https://aclanthology.org/C12-1054</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extractive Multi-Document Summarization with Integer Linear Programming and Support Vector Regression</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C12-1056" target="_blank">https://aclanthology.org/C12-1056</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>N-gram Fragment Sequence Based Unsupervised Domain-Specific Document Readability</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C12-1080" target="_blank">https://aclanthology.org/C12-1080</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploiting Category-Specific Information for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C12-1128" target="_blank">https://aclanthology.org/C12-1128</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Supervised Aggregation Framework for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C12-1136" target="_blank">https://aclanthology.org/C12-1136</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Inverse Document Density: A Smooth Measure for Location-Dependent Term Irregularities</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C12-1159" target="_blank">https://aclanthology.org/C12-1159</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>RelationListwise for Query-Focused Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C12-1181" target="_blank">https://aclanthology.org/C12-1181</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SentTopic-MultiRank: a Novel Ranking Model for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C12-1182" target="_blank">https://aclanthology.org/C12-1182</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Beyond translation memories: finding similar documents in comparable corpora</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2012.tc-1.14" target="_blank">https://aclanthology.org/2012.tc-1.14</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2012</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Statistical Machine Translation prototype using UN parallel documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2012.eamt-1.4" target="_blank">https://aclanthology.org/2012.eamt-1.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Translating Common English and Chinese Verb-Noun Pairs in Technical Documents with Collocational and Bilingual Information</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y11-1052" target="_blank">https://aclanthology.org/Y11-1052</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploring Emotional Words for Chinese Document Chief Emotion Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y11-1064" target="_blank">https://aclanthology.org/Y11-1064</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Question Generation from Swedish Documents as a Tool for Information Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W11-4650" target="_blank">https://aclanthology.org/W11-4650</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Document Discourse Parsing Using Traditional and Hierarchical Machine Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W11-4501" target="_blank">https://aclanthology.org/W11-4501</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Generative Approach for Multi-Document Summarization using Semantic-Discursive information</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W11-4531" target="_blank">https://aclanthology.org/W11-4531</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Handwritten Text Recognition for Historical Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W11-4114" target="_blank">https://aclanthology.org/W11-4114</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Reducing OCR Errors in Gothic-Script Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W11-4115" target="_blank">https://aclanthology.org/W11-4115</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving the Accessibility of Line Graphs in Multimodal Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W11-2306" target="_blank">https://aclanthology.org/W11-2306</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Minimally Supervised Approach for Detecting and Ranking Document Translation Pairs</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W11-2125" target="_blank">https://aclanthology.org/W11-2125</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Just-in-Time Document Retrieval System for Dialogues or Monologues</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W11-2045" target="_blank">https://aclanthology.org/W11-2045</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Identifying Parallel Documents from a Large Bilingual Collection of Texts: Application to Parallel Article Extraction in Wikipedia.</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W11-1212" target="_blank">https://aclanthology.org/W11-1212</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using a Wikipedia-based Semantic Relatedness Measure for Document Clustering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W11-1105" target="_blank">https://aclanthology.org/W11-1105</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>GrawlTCQ: Terminology and Corpora Building by Ranking Simultaneously Terms Queries and Documents using Graph Random Walks</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W11-1106" target="_blank">https://aclanthology.org/W11-1106</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Simultaneous Similarity Learning and Feature-Weight Learning for Document Clustering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W11-1107" target="_blank">https://aclanthology.org/W11-1107</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards Multi-Document Summarization of Scientific Articles:Making Interesting Comparisons with SciSumm</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W11-0502" target="_blank">https://aclanthology.org/W11-0502</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extractive Multi-Document Summaries Should Explicitly Not Contain Document Specific Content</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W11-0507" target="_blank">https://aclanthology.org/W11-0507</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Normalized-Cut Alignment Model for Mapping Hierarchical Semantic Structures onto Spoken Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W11-0324" target="_blank">https://aclanthology.org/W11-0324</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The Role of Information Extraction in the Design of a Document Triage Application for Biocuration</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W11-0206" target="_blank">https://aclanthology.org/W11-0206</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Document Summarization by Capturing the Information Users are Interested in</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/R11-1011" target="_blank">https://aclanthology.org/R11-1011</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Exploration into the Use of Contextual Document Clustering for Cluster Sentiment Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/R11-1020" target="_blank">https://aclanthology.org/R11-1020</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Investigating Advanced Techniques for Document Content Similarity Applied to External Plagiarism Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/R11-1033" target="_blank">https://aclanthology.org/R11-1033</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Language Modeling for Document Selection in Question Answering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/R11-1104" target="_blank">https://aclanthology.org/R11-1104</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SciSumm: A Multi-Document Summarization System for Scientific Articles</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P11-4020" target="_blank">https://aclanthology.org/P11-4020</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Latent Topic Extracting Method based on Events in a Document and its Application</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P11-3006" target="_blank">https://aclanthology.org/P11-3006</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Error Analysis of Relation Extraction in Social Media Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P11-3012" target="_blank">https://aclanthology.org/P11-3012</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards a Framework for Abstractive Summarization of Multimodal Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P11-3014" target="_blank">https://aclanthology.org/P11-3014</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>From Bilingual Dictionaries to Interlingual Document Representations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P11-2026" target="_blank">https://aclanthology.org/P11-2026</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Query Snowball: A Co-occurrence-based Approach to Multi-document Summarization for Question Answering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P11-2039" target="_blank">https://aclanthology.org/P11-2039</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P11-2045" target="_blank">https://aclanthology.org/P11-2045</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Reordering Constraint Based on Document-Level Context</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P11-2076" target="_blank">https://aclanthology.org/P11-2076</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Probabilistic Document Modeling for Syntax Removal in Text Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P11-2113" target="_blank">https://aclanthology.org/P11-2113</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Semi-Supervised SimHash for Efficient Document Similarity Search</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P11-1010" target="_blank">https://aclanthology.org/P11-1010</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Class of Submodular Functions for Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P11-1052" target="_blank">https://aclanthology.org/P11-1052</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Lexically-Triggered Hidden Markov Models for Clinical Document Coding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P11-1075" target="_blank">https://aclanthology.org/P11-1075</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P11-1080" target="_blank">https://aclanthology.org/P11-1080</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Simple supervised document geolocation with geodesic grids</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P11-1096" target="_blank">https://aclanthology.org/P11-1096</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Rare Word Translation Extraction from Aligned Comparable Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P11-1133" target="_blank">https://aclanthology.org/P11-1133</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Decomposition of a Document into Authorial Components</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P11-1136" target="_blank">https://aclanthology.org/P11-1136</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using Bilingual Information for Cross-Language Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P11-1155" target="_blank">https://aclanthology.org/P11-1155</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>英文技術文獻中一般動詞與其受詞之中文翻譯的語境效用 (Collocational Influences on the Chinese Translations of Non-Technical English Verbs and Their Objects in Technical Documents) [In Chinese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O11-1006" target="_blank">https://aclanthology.org/O11-1006</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Mining Parallel Documents Using Low Bandwidth and High Precision CLIR from the Heterogeneous Web</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I11-1047" target="_blank">https://aclanthology.org/I11-1047</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Indexing Spoken Documents with Hierarchical Semantic Structures: Semantic Tree-to-string Alignment Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I11-1057" target="_blank">https://aclanthology.org/I11-1057</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CLGVSM: Adapting Generalized Vector Space Model to Cross-lingual Document Clustering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I11-1065" target="_blank">https://aclanthology.org/I11-1065</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using Context Inference to Improve Sentence Ordering for Multi-document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I11-1118" target="_blank">https://aclanthology.org/I11-1118</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using Syntactic and Shallow Semantic Kernels to Improve Multi-Modality Manifold-Ranking for Topic-Focused Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I11-1123" target="_blank">https://aclanthology.org/I11-1123</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Weakly-supervised Approach to Argumentative Zoning of Scientific Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D11-1025" target="_blank">https://aclanthology.org/D11-1025</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learning Local Content Shift Detectors from Document-level Information</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D11-1070" target="_blank">https://aclanthology.org/D11-1070</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cache-based Document-level Statistical Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D11-1084" target="_blank">https://aclanthology.org/D11-1084</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Non-negative Matrix Factorization Based Approach for Active Dual Supervision from Document and Word Labels</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D11-1088" target="_blank">https://aclanthology.org/D11-1088</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Generating Aspect-oriented Multi-Document Summarization with Event-aspect model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D11-1105" target="_blank">https://aclanthology.org/D11-1105</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>D&#39;etection de la sp&#39;ecialisation scientifique et technique des documents biom&#39;edicaux gr^ace aux informations morphologiques [Spotting scientific and technical specialization in biomedical documents using morphological clues]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2011.tal-2.6" target="_blank">https://aclanthology.org/2011.tal-2.6</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Building a Statistical Machine Translation System for Translating Patent Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2011.mtsummit-wpt.8" target="_blank">https://aclanthology.org/2011.mtsummit-wpt.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-level Consistency Verification in Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2011.mtsummit-papers.13" target="_blank">https://aclanthology.org/2011.mtsummit-papers.13</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improve SMT with Source-Side ``Topic-Document&#39;&#39; Distributions</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2011.mtsummit-papers.57" target="_blank">https://aclanthology.org/2011.mtsummit-papers.57</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Plusieurs utilisateurs ont souvent besoin d&#39;informations p&#39;edagogiques pour les int&#39;egrer dans leurs ressources p&#39;edagogiques ou pour les utiliser dans un processus d&#39;apprentissage. Une indexation de ces informations s&#39;av`ere donc utile en vue d&#39;une extraction des informations p&#39;edagogiques pertinentes en r&#39;eponse `a une requ^ete utilisateur. La plupart des syst`emes d&#39;extraction d&#39;informations p&#39;edagogiques existants proposent une indexation bas&#39;ee sur une annotation manuelle ou semi-automatique des informations p&#39;edagogiques t^ache qui n&#39;est pas pr&#39;ef&#39;er&#39;ee par les utilisateurs. Dans cet article nous proposons une approche d&#39;indexation d&#39;objets p&#39;edagogiques (D&#39;efinition Exemple Exercice etc.) bas&#39;ee sur une annotation s&#39;emantique par Exploration Contextuelle des documents. L&#39;index g&#39;en&#39;er&#39;e servira `a une extraction des objets pertinents r&#39;epondant `a une requ^ete utilisateur s&#39;emantique. Nous proc&#39;edons ensuite `a un classement des objets extraits selon leur pertinence en utilisant l&#39;algorithme Rocchio. Notre objectif est de mettre en valeur une indexation `a partir de contextes s&#39;emantiques et non pas `a partir de seuls termes linguistiques.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extraction Automatique d&#39;Informations P&#39;edagogiques Pertinentes `a partir de Documents Textuels</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2011.jeptalnrecital-recital.2" target="_blank">https://aclanthology.org/2011.jeptalnrecital-recital.2</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Le r&#39;esum&#39;e automatique cross-lingue consiste `a g&#39;en&#39;erer un r&#39;esum&#39;e r&#39;edig&#39;e dans une langue diff&#39;erente de celle utilis&#39;ee dans les documents sources. Dans cet article nous proposons une approche de r&#39;esum&#39;e automatique multi-document bas&#39;ee sur une repr&#39;esentation par graphe qui prend en compte des scores de qualit&#39;e de traduction lors du processus de s&#39;election des phrases. Nous &#39;evaluons notre m&#39;ethode sur un sous-ensemble manuellement traduit des donn&#39;ees utilis&#39;ees lors de la campagne d&#39;&#39;evaluation internationale DUC 2004. Les r&#39;esultats exp&#39;erimentaux indiquent que notre approche permet d&#39;am&#39;eliorer la lisibilit&#39;e des r&#39;esum&#39;es g&#39;en&#39;er&#39;es sans pour autant d&#39;egrader leur informativit&#39;e.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Utilisation d&#39;un score de qualit&#39;e de traduction pour le r&#39;esum&#39;e multi-document cross-lingue (Using translation quality scores for cross-language multi-document summarization)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2011.jeptalnrecital-long.5" target="_blank">https://aclanthology.org/2011.jeptalnrecital-long.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Nous pr&#39;esentons diff&#39;erentes m&#39;ethodes de r&#39;eordonnancement de phrases pour le r&#39;esum&#39;e automatique fond&#39;e sur une classification des phrases `a r&#39;esumer en classes th&#39;ematiques. Nous comparons ces m&#39;ethodes `a deux baselines : ordonnancement des phrases selon leur pertinence et ordonnancement selon la date et la position dans le document d&#39;origine. Nous avons fait &#39;evaluer les r&#39;esum&#39;es obtenus sur le corpus RPM2 par 4 annotateurs et pr&#39;esentons les r&#39;esultats.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Ordonner un r&#39;esum&#39;e automatique multi-documents fond&#39;e sur une classification des phrases en classes lexicales (Ordering a multi-document summary based on sentences subtopic clustering)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2011.jeptalnrecital-court.18" target="_blank">https://aclanthology.org/2011.jeptalnrecital-court.18</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2011</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Tapta: A user-driven translation system for patent documents based on domain-aware Statistical Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2011.eamt-1.2" target="_blank">https://aclanthology.org/2011.eamt-1.2</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>PodCastle: A Spoken Document Retrieval Service Improved by Anonymous User Contributions</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y10-1001" target="_blank">https://aclanthology.org/Y10-1001</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Query Focused Multi Document Automatic Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y10-1063" target="_blank">https://aclanthology.org/Y10-1063</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Generation of Summaries that Appropriately and Adequately Express the Contents of Original Documents Using Word-Association Knowledge</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y10-1080" target="_blank">https://aclanthology.org/Y10-1080</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Combine Person Name and Person Identity Recognition and Document Clustering for Chinese Person Name Disambiguation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W10-4154" target="_blank">https://aclanthology.org/W10-4154</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Experiments with CST-Based Multidocument Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W10-2312" target="_blank">https://aclanthology.org/W10-2312</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Eliminating Redundancy by Spectral Relaxation for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W10-2316" target="_blank">https://aclanthology.org/W10-2316</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Summarization of Biomedical Documents Using Word Sense Disambiguation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W10-1907" target="_blank">https://aclanthology.org/W10-1907</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extracting Distinctive Features of Swine (H1N1) Flu through Data Mining Clinical Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W10-1918" target="_blank">https://aclanthology.org/W10-1918</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using an Online Tool for the Documentation of Edo Language</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W10-1813" target="_blank">https://aclanthology.org/W10-1813</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document-Level Automatic MT Evaluation based on Discourse Representations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W10-1750" target="_blank">https://aclanthology.org/W10-1750</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W10-1100" target="_blank">https://aclanthology.org/W10-1100</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Reliability and Type of Consumer Health Documents on the World Wide Web: an Annotation Study</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W10-1106" target="_blank">https://aclanthology.org/W10-1106</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Image Collection Using Amazon&#39;s Mechanical Turk</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W10-0707" target="_blank">https://aclanthology.org/W10-0707</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Crowdsourcing Document Relevance Assessment with Mechanical Turk</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W10-0727" target="_blank">https://aclanthology.org/W10-0727</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Single Document Keyphrase Extraction Using Sentence Clustering and Latent Dirichlet Allocation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/S10-1032" target="_blank">https://aclanthology.org/S10-1032</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>WINGNUS: Keyphrase Extraction Utilizing Document Logical Structure</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/S10-1035" target="_blank">https://aclanthology.org/S10-1035</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SEERLAB: A System for Extracting Keyphrases from Scholarly Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/S10-1039" target="_blank">https://aclanthology.org/S10-1039</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P10-2028" target="_blank">https://aclanthology.org/P10-2028</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Semi-Supervised Key Phrase Extraction Approach: Learning from Title Phrases through a Document Semantic Network</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P10-2055" target="_blank">https://aclanthology.org/P10-2055</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P10-1056" target="_blank">https://aclanthology.org/P10-1056</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using Document Level Cross-Event Inference to Improve Event Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P10-1081" target="_blank">https://aclanthology.org/P10-1081</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Hybrid Hierarchical Model for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P10-1084" target="_blank">https://aclanthology.org/P10-1084</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-Language Document Summarization Based on Machine Translation Quality Prediction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P10-1094" target="_blank">https://aclanthology.org/P10-1094</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>美國專利書「獨立項數」之搭配詞初探: 以LexisNexis 法律資料庫為例 (Collocation Features of Independent Claim in U.S. Patent Documents: Information Retrieval from LexisNexis)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O10-2004" target="_blank">https://aclanthology.org/O10-2004</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Utility Evaluation of Cross-document Information Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N10-1036" target="_blank">https://aclanthology.org/N10-1036</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extracting Parallel Sentences from Comparable Corpora using Document Level Alignment</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N10-1063" target="_blank">https://aclanthology.org/N10-1063</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Clinical Information Retrieval using Document and PICO Structure</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N10-1124" target="_blank">https://aclanthology.org/N10-1124</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-document Summarization via Budgeted Maximization of Submodular Functions</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N10-1134" target="_blank">https://aclanthology.org/N10-1134</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The focus of information retrieval evaluations such as NISTs TREC evaluations (e.g. Voorhees 2003) is on evaluation of the information content of system responses. On the other hand retrieval tasks usually involve two different dimensions: reporting relevant information and providing sources of information including corroborating evidence and alternative documents. Under the DARPA Global Autonomous Language Exploitation (GALE) program Distillation provides succinct direct responses to the formatted queries using the outputs of automated transcription and translation technologies. These responses are evaluated in two dimensions: information content which measures the amount of relevant and non-redundant information and document support which measures the number of alternative sources provided in support of reported information. The final metric in the overall GALE distillation evaluation combines the results of scoring of both query responses and document citations. In this paper we describe our evaluation framework with emphasis on the scoring of document citations and an analysis of how systems perform at providing sources of information.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Evaluation of Document Citations in Phase 2 Gale Distillation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2010/pdf/108_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2010/pdf/108_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>A corpus called DutchParl is created which aims to contain all digitally available parliamentary documents written in the Dutch language. The first version of DutchParl contains documents from the parliaments of The Netherlands Flanders and Belgium. The corpus is divided along three dimensions: per parliament scanned or digital documents written recordings of spoken text and others. The digital collection contains more than 800 million tokens the scanned collection more than 1 billion. All documents are available as UTF-8 encoded XML files with extensive metadata in Dublin Core standard. The text itself is divided into pages which are divided into paragraphs. Every document page and paragraph has a unique URN which resolves to a web page. Every page element in the XML files is connected to a facsimile image of that page in PDF or JPEG format. We created a viewer in which both versions can be inspected simultaneously. The corpus is available for download in several formats. The corpus can be used for corpus-linguistic and political science research and is suitable for performing scalability tests for XML information systems.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DutchParl. The Parliamentary Documents in Dutch</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2010/pdf/263_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2010/pdf/263_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Language resources are typically defined and created for application in speech technology contexts but the documentation of languages which are unlikely ever to be provided with enabling technologies nevertheless plays an important role in defining the heritage of a speech community and in the provision of basic insights into the language oriented components of human cognition. This is particularly true of endangered languages. The present case study concerns the documentation both of the birth and of the endangerment within a rather short space of time of a spirit language Medefaidrin created and used as a vehicular language by a religious community in South-Eastern Nigeria. The documentation shows phonological orthographic morphological syntactic and textual typological features of Medefaidrin which indicate that typological properties of English were a model for the creation of the language rather than typological properties of the enclaving language Ibibio. The documentation is designed as part of the West African Language Archive (WALA) following OLAC metadata standards.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Medefaidrin: Resources Documenting the Birth and Death Language Life-cycle</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2010/pdf/342_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2010/pdf/342_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes the development of a structured document collection containing user-generated text and numerical metadata for exploring the exploitation of metadata in information retrieval (IR). The collection consists of more than 61000 documents extracted from YouTube video pages on basketball in general and NBA (National Basketball Association) in particular together with a set of 40 topics and their relevance judgements. In addition a collection of nearly 250000 user profiles related to the NBA collection is available. Several baseline IR experiments report the effect of using video-associated metadata on retrieval effectiveness. The results surprisingly show that searching the videos titles only performs significantly better than searching additional metadata text fields of the videos such as the tags or the description.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Building a Domain-specific Document Collection for Evaluating Metadata Effects on Information Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2010/pdf/353_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2010/pdf/353_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This article presents the use of NLP techniques (text mining text analysis) to develop specific tools that allow to create linguistic resources related to the cultural heritage domain. The aim of our approach is to create tools for the building of an online knowledge network automatically extracted from text materials concerning this domain. A particular methodology was experimented by dividing the automatic acquisition of texts and consequently the creation of reference corpus in two phases. In the first phase on-line documents have been extracted from lists of links provided by human experts. All documents extracted from the web by means of automatic spider have been stored in a repository of text materials. On the basis of these documents automatic parsers create the reference corpus for the cultural heritage domain. Relevant information and semantic concepts are then extracted from this corpus. In a second phase all these semantically relevant elements (such as proper names names of institutions names of places and other relevant terms) have been used as basis for a new search strategy of text materials from heterogeneous sources. In this case also specialized crawlers (TP-crawler) have been used to work on a bulk of text materials available on line.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cultural Heritage: Knowledge Extraction from Web Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2010/pdf/415_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2010/pdf/415_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes and evaluates a prototype quality assurance system for LSP corpora. The system will be employed in compiling a corpus of 11 M tokens for various linguistic and terminological purposes. The system utilizes a number of linguistic features as quality indicators. These represent two dimensions of quality namely readability/formality (e.g. word length and passive constructions) and density of specialized knowledge (e.g. out-of-vocabulary items). Threshold values for each indicator are induced from a reference corpus of general (fiction magazines and newspapers) and specialized language (the domains of Health/Medicine and Environment/Climate). In order to test the efficiency of the indicators a number of terminologically relevant irrelevant and possibly relevant texts are manually selected from target web sites as candidate texts. By applying the indicators to these candidate texts the system is able to filter out non-LSP and poor LSP texts with a precision of 100% and a recall of 55%. Thus the experiment described in this paper constitutes fundamental work towards a formulation of best practice for implementing quality assurance when selecting appropriate texts for an LSP corpus. The domain independence of the quality indicators still remains to be thoroughly tested on more than just two domains.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Quality Indicators of LSP Texts --- Selection and Measurements Measuring the Terminological Usefulness of Documents for an LSP Corpus</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2010/pdf/505_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2010/pdf/505_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes the acquisition preparation and properties of a corpus extracted from the official documents of the United Nations (UN). This corpus is available in all 6 official languages of the UN consisting of around 300 million words per language. We describe the methods we used for crawling document formatting and sentence alignment. This corpus also includes a common test set for machine translation. We present the results of a French-Chinese machine translation experiment performed on this corpus.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MultiUN: A Multilingual Corpus from United Nation Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2010/pdf/686_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2010/pdf/686_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We describe the compilation of a large corpus of French-Dutch sentence pairs from official Belgian documents which are available in the online version of the publication Belgisch Staatsblad/Moniteur belge and which have been published between 1997 and 2006. After downloading files in batch we filtered out documents which have no translation in the other language documents which contain several languages (by checking on discriminating words) and pairs of documents with a substantial difference in length. We segmented the documents into sentences and aligned the latter which resulted in 5 million sentence pairs (only one-to-one links were included in the parallel corpus); there are 2.4 million unique pairs. Sample-based evaluation of the sentence alignment results indicates a near 100% accuracy which can be explained by the text genre the procedure filtering out weakly parallel articles and the restriction to one-to-one links. The corpus is larger than a number of well-known French-Dutch resources. It is made available to the community. Further investigation is needed in order to determine the original language in which documents were written.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Belgisch Staatsblad Corpus: Retrieving French-Dutch Sentences from Official Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2010/pdf/758_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2010/pdf/758_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents two corpora produced within the RPM2 project: a multi-document summarization corpus and a sentence compression corpus. Both corpora are in French. The first one is the only one we know in this language. It contains 20 topics with 20 documents each. A first set of 10 documents per topic is summarized and then the second set is used to produce an update summarization (new information). 4 annotators were involved and produced a total of 160 abstracts. The second corpus contains all the sentences of the first one. 4 annotators were asked to compress the 8432 sentences. This is the biggest corpus of compressed sentences we know whatever the language. The paper provides some figures in order to compare the different annotators: compression rates number of tokens per sentence percentage of tokens kept according to their POS position of dropped tokens in the sentence compression phase etc. These figures show important differences from an annotator to the other. Another point is the different strategies of compression used according to the length of the sentence.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A French Human Reference Corpus for Multi-Document Summarization and Sentence Compression</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2010/pdf/919_Paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2010/pdf/919_Paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Discourse Constraints for Document Compression</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/J10-3005" target="_blank">https://aclanthology.org/J10-3005</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Evaluating Models of Latent Document Semantics in the Presence of OCR Errors</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D10-1024" target="_blank">https://aclanthology.org/D10-1024</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Translingual Document Representations from Discriminative Projections</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D10-1025" target="_blank">https://aclanthology.org/D10-1025</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>NLP on Spoken Documents Without ASR</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D10-1045" target="_blank">https://aclanthology.org/D10-1045</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Document Summarization Using A* Search and Discriminative Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D10-1047" target="_blank">https://aclanthology.org/D10-1047</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Collective Cross-Document Relation Extraction Without Labelled Data</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D10-1099" target="_blank">https://aclanthology.org/D10-1099</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Level Structured Models for Document-Level Sentiment Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D10-1102" target="_blank">https://aclanthology.org/D10-1102</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Expansion Based on WordNet for Robust IR</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C10-2002" target="_blank">https://aclanthology.org/C10-2002</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Comparative Study on Ranking and Selection Strategies for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C10-2060" target="_blank">https://aclanthology.org/C10-2060</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Local Space-Time Smoothing for Version Controlled Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C10-2063" target="_blank">https://aclanthology.org/C10-2063</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Study on Position Information in Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C10-2106" target="_blank">https://aclanthology.org/C10-2106</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Dynamic Parameters for Cross Document Coreference</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C10-2114" target="_blank">https://aclanthology.org/C10-2114</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Streaming Cross Document Entity Coreference Resolution</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C10-2121" target="_blank">https://aclanthology.org/C10-2121</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards Automatic Building of Document Keywords</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C10-2132" target="_blank">https://aclanthology.org/C10-2132</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extracting and Ranking Product Features in Opinion Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C10-2167" target="_blank">https://aclanthology.org/C10-2167</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Sentence Ordering with Event-Enriched Semantics and Two-Layered Clustering for Multi-Document News Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C10-2170" target="_blank">https://aclanthology.org/C10-2170</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Dual-Space Re-ranking Model for Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C10-2174" target="_blank">https://aclanthology.org/C10-2174</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Imposing Hierarchical Browsing Structures onto Spoken Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C10-2177" target="_blank">https://aclanthology.org/C10-2177</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Simultaneous Ranking and Clustering of Sentences: A Reinforcement Approach to Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C10-1016" target="_blank">https://aclanthology.org/C10-1016</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Bipolar Person Name Identification of Topic Documents Using Principal Component Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C10-1020" target="_blank">https://aclanthology.org/C10-1020</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Enhancing Cross Document Coreference of Web Documents with Context Similarity and Very Large Scale Text Categorization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C10-1055" target="_blank">https://aclanthology.org/C10-1055</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Document Summarization via the Minimum Dominating Set</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C10-1111" target="_blank">https://aclanthology.org/C10-1111</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Large Scale Parallel Document Mining for Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C10-1124" target="_blank">https://aclanthology.org/C10-1124</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards a Unified Approach to Simultaneous Single-Document and Multi-Document Summarizations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C10-1128" target="_blank">https://aclanthology.org/C10-1128</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Dans cet article nous nous int&#39;eressons au r&#39;esum&#39;e automatique de textes arabes. Nous commenccons par pr&#39;esenter une &#39;etude analytique r&#39;ealis&#39;ee sur un corpus de travail qui nous a permis de d&#39;eduire suite `a des observations empiriques un ensemble de relations et de frames (r`egles ou patrons) rh&#39;etoriques; ensuite nous pr&#39;esentons notre m&#39;ethode de production de r&#39;esum&#39;es pour les textes arabes. La m&#39;ethode que nous proposons se base sur la Th&#39;eorie de la Structure Rh&#39;etorique (RST) (Mann et al. 1988) et utilise des connaissances purement linguistiques. Le principe de notre proposition s&#39;appuie sur trois piliers. Le premier pilier est le rep&#39;erage des relations rh&#39;etoriques entres les diff&#39;erentes unit&#39;es minimales du texte dont l&#39;une poss`ede le statut de noyau -- segment de texte primordial pour la coh&#39;erence -- et l&#39;autre a le statut noyau ou satellite -- segment optionnel. Le deuxi`eme pilier est le dressage et la simplification de l&#39;arbre RST. Le troisi`eme pilier est la s&#39;election des phrases noyaux formant le r&#39;esum&#39;e final qui tiennent en compte le type de relation rh&#39;etoriques choisi pour l&#39;extrait.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>R&#39;esum&#39;e automatique de documents arabes bas&#39;e sur la technique RST</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2010.jeptalnrecital-recital.8" target="_blank">https://aclanthology.org/2010.jeptalnrecital-recital.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>La r&#39;ecente &#39;eclosion du Web2.0 engendre un accroissement consid&#39;erable de volumes textuels et intensifie ainsi l&#39;importance d&#39;une r&#39;eflexion sur l&#39;exploitation des connaissances `a partir de grandes collections de documents. Dans cet article nous pr&#39;esentons une approche de rechercher d&#39;information qui s&#39;inspire des certaines recherches issues de la psychologie cognitive pour la fouille de larges collections de documents. Nous utilisons un document comme requ^ete permettant de r&#39;ecup&#39;erer des informations `a partir d&#39;une collection repr&#39;esent&#39;ee dans un espace s&#39;emantique. Nous d&#39;efinissons les notions d&#39;identit&#39;e s&#39;emantique et de pollution s&#39;emantique dans un espace de documents. Nous illustrons notre approche par la description d&#39;un syst`eme appel&#39;e BRAT (Blogosphere Random Analysis using Texts) bas&#39;e sur les notions pr&#39;ealablement introduites d&#39;identit&#39;e et de pollution s&#39;ematique appliqu&#39;ees `a une t^ache d&#39;identification des actualit&#39;es dans la blogosph`ere mondiale lors du concours TREC&#39;09. Les premiers r&#39;esultats produits sont tout `a fait encourageant et indiquent les pistes des recherches `a mettre en oeuvre afin d&#39;am&#39;eliorer les performances de BRAT.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Une approche cognitive de la fouille de grandes collections de documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2010.jeptalnrecital-long.6" target="_blank">https://aclanthology.org/2010.jeptalnrecital-long.6</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Les m&#39;ethodes de segmentation th&#39;ematique exploitant une mesure de la coh&#39;esion lexicale peuvent ^etre appliqu&#39;ees telles quelles `a des transcriptions automatiques de programmes t&#39;el&#39;evisuels. Cependant elles sont moins efficaces dans ce contexte ne prenant en compte ni les particularit&#39;es des &#39;emissions TV ni celles des transcriptions. Nous &#39;etudions ici l&#39;apport de relations s&#39;emantiques pour rendre les techniques de segmentation th&#39;ematique plus robustes. Nous proposons une m&#39;ethode pour exploiter ces relations dans une mesure de la coh&#39;esion lexicale et montrons qu&#39;elles permettent d&#39;augmenter la F1-mesure de +1.97 et +11.83 sur deux corpus compos&#39;es respectivement de 40h de journaux t&#39;el&#39;evis&#39;es et de 40h d&#39;&#39;emissions de reportage. Ces am&#39;eliorations d&#39;emontrent que les relations s&#39;emantiques peuvent rendre les m&#39;ethodes de segmentation moins sensibles aux erreurs de transcription et au manque de r&#39;ep&#39;etitions constat&#39;e dans certaines &#39;emissions t&#39;el&#39;evis&#39;ees.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Utilisation de relations s&#39;emantiques pour am&#39;eliorer la segmentation th&#39;ematique de documents t&#39;el&#39;evisuels</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2010.jeptalnrecital-long.22" target="_blank">https://aclanthology.org/2010.jeptalnrecital-long.22</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Machine Translation traditionally treats documents as sets of independent sentences. In many genres however documents are highly structured and their structure contains information that can be used to improve translation quality. We present a preliminary approach to document translation that uses structural features to modify the behaviour of a language model at sentence-level granularity. To our knowledge this is the first attempt to incorporate structural information into statistical MT. In experiments on structured English/French documents from the Hansard corpus we demonstrate small but statistically significant improvements.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Translating Structured Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2010.amta-papers.24" target="_blank">https://aclanthology.org/2010.amta-papers.24</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The present study examines from users&#39; perspective the performance of Google&#39;s online translation service on the documents of the United Nations. Since at least 2004 United Nations has been exploring piloting and implementing computer assisted translation (CAT) with Trados as an officially selected vehicle. A more recent development is the spontaneous adoption of Google translation among Chinese translators as an easy versatile and labor-saving tool. With machine translation getting real among developers and end-users there seems to be a need to conduct a reality check to see how well it serves its purpose. The current study examines Google translation and its degree of assistance to the Chinese professional translators at the United Nations in particular. It uses a variety of UN documents to test and evaluate the performance of Google translation from English to Chinese. The sampled UN documents consist of 3 resolutions 2 letters 2 provisional agendas 1 plenary verbatim 1 report 1 note by the Secretariat and 1 budget. The results vindicate Google&#39;s cutting edge in machine translation when English to Chinese is concerned thanks to its powerful infrastructure and immense translation database. The conversion between the two languages takes only an instant even for a fairly long piece. On top of that Google gets terminology right more frequently and seems better able to make an intelligent guess when compared with other translation tools like MS Bing. But Google&#39;s Chinese is far from intelligible especially at the sentence level primarily because of serious problems with word order and sentence parsing. There are also technical problems like adding or omitting words and erroneous rendering of numbers. Nevertheless Google translation offers translators an option to work on its rough draft for the benefit of saving time and pain in typing. The challenges of post-editing however may offset the time saved. Even though Google translation may not necessarily net in speed gains when it is used to assist translation it certainly is a beneficial labor saver including mental labor when it performs at its very best.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Machine translation from English to Chinese: A study of Google&#39;s performance with the UN documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2010.amta-government.3" target="_blank">https://aclanthology.org/2010.amta-government.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2010</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>CACI has developed and delivered systems for document exploitation and processing to Government customers around the world. Many of these systems include advanced language processing capabilities in order to enable rapid triage of vast collections of foreign language documents separating the content that requires immediate human attention from the less immediately pressing material. AppTek provides key patent-pending Machine Translation technology for this critical process rendering material in Arabic Farsi and other languages into an English rendition that enables both further automated processing and rapid review by monolingual analysts to identify the documents that require immediate linguist attention. Both CACI and AppTek have been working with customers to develop capabilities that enable them the users to be the ones in command of making their systems learn and continuously improve. We will describe how we put this critical user requirement into the systems and the key role that the user&#39;s involvement played in this. We will also discuss some of the key components of the system and what the customer-centric evolution of the system will be including our document translation workflow the machine translation technology within it and our approaches to supporting the technology and sustaining its success designed around adapting to user needs.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>User-generated System for Critical Document Triage and Exploitation--Version 2011</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2010.amta-government.8" target="_blank">https://aclanthology.org/2010.amta-government.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Collaborative Summarization: When Collaborative Filtering Meets Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y09-2005" target="_blank">https://aclanthology.org/Y09-2005</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Summarization Approaches Based on Document Probability Distributions</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y09-2010" target="_blank">https://aclanthology.org/Y09-2010</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Customizing an English-Korean Machine Translation System for Patent/Technical Documents Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y09-2033" target="_blank">https://aclanthology.org/Y09-2033</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Re-ranking via Wikipedia Articles for Definition/Biography Type Questions</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y09-2037" target="_blank">https://aclanthology.org/Y09-2037</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Query-Focused Multi-Document Summarization Using Co-Training Based Semi-Supervised Learning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y09-1021" target="_blank">https://aclanthology.org/Y09-1021</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Reducing Redundancy in Multi-document Summarization Using Lexical Semantic Similarity</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W09-2812" target="_blank">https://aclanthology.org/W09-2812</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Evaluating Automation Strategies in Language Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W09-1905" target="_blank">https://aclanthology.org/W09-1905</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Interlinear Glossing and its Role in Theoretical and Descriptive Studies of African and other Lesser--Documented Languages</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W09-0711" target="_blank">https://aclanthology.org/W09-0711</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Precision and Mathematical Form in First and Subsequent Mentions of Numerical Facts and their Relation to Document Structure</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W09-0620" target="_blank">https://aclanthology.org/W09-0620</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Content Analysis of Museum Documentation in a Transdisciplinary Perspective</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W09-0301" target="_blank">https://aclanthology.org/W09-0301</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Integrating Document Structure into a Multi-Document Summarizer</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/R09-1009" target="_blank">https://aclanthology.org/R09-1009</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Topic-based Multi-Document Summarization with Probabilistic Latent Semantic Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/R09-1028" target="_blank">https://aclanthology.org/R09-1028</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-document Event Extraction and Tracking: Task Evaluation Techniques and Challenges</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/R09-1032" target="_blank">https://aclanthology.org/R09-1032</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Classification-driven Approach to Document Planning</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/R09-1059" target="_blank">https://aclanthology.org/R09-1059</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MARS: Multilingual Access and Retrieval System with Enhanced Query Translation and Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P09-4006" target="_blank">https://aclanthology.org/P09-4006</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Clustering Technique in Multi-Document Personal Name Disambiguation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P09-3011" target="_blank">https://aclanthology.org/P09-3011</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Creating a Gold Standard for Sentence Clustering in Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P09-3012" target="_blank">https://aclanthology.org/P09-3012</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Integrated Multi-document Summarization Approach based on Word Hierarchical Representation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P09-2029" target="_blank">https://aclanthology.org/P09-2029</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extracting Comparative Sentences from Korean Text Documents Using Comparative Lexical Patterns and Machine Learning Techniques</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P09-2039" target="_blank">https://aclanthology.org/P09-2039</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Document Summarization using Sentence-based Topic Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P09-2075" target="_blank">https://aclanthology.org/P09-2075</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Arabic Cross-Document Coreference Resolution</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P09-2090" target="_blank">https://aclanthology.org/P09-2090</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Profile Based Cross-Document Coreference Using Kernelized Fuzzy Relational Clustering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P09-1047" target="_blank">https://aclanthology.org/P09-1047</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Summarizing multiple spoken documents: finding evidence from untranscribed audio</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P09-1062" target="_blank">https://aclanthology.org/P09-1062</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Hierarchical Web Document Classification Based on Hierarchically Trained Domain Specific Words</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O09-2006" target="_blank">https://aclanthology.org/O09-2006</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>中英文專利文書之文句對列 (Sentence alignment of English and Chinese patent documents) [In Chinese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O09-1007" target="_blank">https://aclanthology.org/O09-1007</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-document Temporal and Spatial Person Tracking System Demonstration</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N09-5001" target="_blank">https://aclanthology.org/N09-5001</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Solving the ``Who&#39;s Mark Johnson Puzzle&#39;&#39;: Information Extraction Based Cross Document Coreference</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N09-3002" target="_blank">https://aclanthology.org/N09-3002</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploring Content Models for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N09-1041" target="_blank">https://aclanthology.org/N09-1041</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Global Models of Document Structure using Latent Permutations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N09-1042" target="_blank">https://aclanthology.org/N09-1042</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Single-Document Key Fact Extraction from Newswire Articles</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/E09-1048" target="_blank">https://aclanthology.org/E09-1048</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Feature-Based Method for Document Alignment in Comparable News Corpora</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/E09-1096" target="_blank">https://aclanthology.org/E09-1096</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>How well does active learning textitactually work? Time-based evaluation of cost-reduction strategies for language documentation.</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D09-1031" target="_blank">https://aclanthology.org/D09-1031</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Document Summarisation Using Generic Relation Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D09-1044" target="_blank">https://aclanthology.org/D09-1044</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised morphological segmentation and clustering with document boundaries</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D09-1070" target="_blank">https://aclanthology.org/D09-1070</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilingual Spectral Clustering Using Document Similarity Propagation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D09-1091" target="_blank">https://aclanthology.org/D09-1091</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Person Cross Document Coreference with Name Perplexity Estimates</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D09-1104" target="_blank">https://aclanthology.org/D09-1104</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Study on the Semantic Relatedness of Query and Document Terms in Information Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D09-1139" target="_blank">https://aclanthology.org/D09-1139</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Character-level Analysis of Semi-Structured Documents for Set Expansion</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D09-1156" target="_blank">https://aclanthology.org/D09-1156</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Latent Document Re-Ranking</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D09-1163" target="_blank">https://aclanthology.org/D09-1163</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Translation of Patent Documents at the United States Patent and Trademark Office</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2009.mtsummit-wpt.14" target="_blank">https://aclanthology.org/2009.mtsummit-wpt.14</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Incorporating Knowledge of Source Language Text in a System for Dictation of Document Translations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2009.mtsummit-papers.13" target="_blank">https://aclanthology.org/2009.mtsummit-papers.13</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MT and Document Localization at Adobe: From Pilot to Production</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2009.mtsummit-commercial.5" target="_blank">https://aclanthology.org/2009.mtsummit-commercial.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Le r&#39;esum&#39;e automatique de texte est une probl&#39;ematique difficile fortement d&#39;ependante de la langue et qui peut n&#39;ecessiter un ensemble de donn&#39;ees d&#39;apprentissage cons&#39;equent. L&#39;approche par extraction peut aider `a surmonter ces difficult&#39;es. (Mihalcea 2004) a d&#39;emontr&#39;e l&#39;int&#39;er^et des approches `a base de graphes pour l&#39;extraction de segments de texte importants. Dans cette &#39;etude nous d&#39;ecrivons une approche ind&#39;ependante de la langue pour la probl&#39;ematique du r&#39;esum&#39;e automatique multi-documents. L&#39;originalit&#39;e de notre m&#39;ethode repose sur l&#39;utilisation d&#39;une mesure de similarit&#39;e permettant le rapprochement de segments morphologiquement proches. De plus c&#39;est `a notre connaissance la premi`ere fois que l&#39;&#39;evaluation d&#39;une approche de r&#39;esum&#39;e automatique multi-document est conduite sur des textes en franccais.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>R&#39;esum&#39;e automatique multi-document et ind&#39;ependance de la langue : une premi`ere &#39;evaluation en franccais</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2009.jeptalnrecital-court.35" target="_blank">https://aclanthology.org/2009.jeptalnrecital-court.35</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Les sch&#39;emas de pond&#39;eration utilis&#39;es habituellement en cat&#39;egorisation de textes et plus g&#39;en&#39;eralement en recherche d&#39;information (RI) ne sont pas adapt&#39;es `a l&#39;utilisation de donn&#39;ees li&#39;ees `a des textes issus d&#39;un processus de reconnaissance de l&#39;&#39;ecriture. En particulier les candidats-mot `a la reconnaissance ne pourraient ^etre exploit&#39;es sans introduire de fausses occurrences de termes dans le document. Dans cet article nous pr&#39;esentons un nouveau sch&#39;ema de pond&#39;eration permettant d&#39;exploiter les listes de candidats-mot. Il permet d&#39;estimer le pouvoir discriminant d&#39;un terme en fonction de la probabilit&#39;e a posteriori d&#39;un candidat-mot dans une liste de candidats. Les r&#39;esultats montrent que le taux de classification de documents fortement d&#39;egrad&#39;es peut ^etre am&#39;elior&#39;e en utilisant le sch&#39;ema propos&#39;e.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Un nouveau sch&#39;ema de pond&#39;eration pour la cat&#39;egorisation de documents manuscrits</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2009.jeptalnrecital-court.43" target="_blank">https://aclanthology.org/2009.jeptalnrecital-court.43</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2009</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This article describes the needs of UOC regarding translation and how these needs are satisfied by Prompsit further developing a free rule-based machine translation system: Apertium. We initially describe the general framework regarding linguistic needs inside UOC. Then section 2 introduces Apertium and outlines the development scenario that Prompsit executed. After that section 3 outlines the specific needs of UOC and why Apertium was chosen as the machine translation engine. Then section 4 describes some of the features specially developed in this project. Section 5 explains how the linguistic data was improved to increase the quality of the output in Catalan and Spanish. And finally we draw conclusions and outline further work originating from the project.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Joint efforts to further develop and incorporate Apertium into the document management flow at Universitat Oberta de Catalunya</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2009.freeopmt-1.12" target="_blank">https://aclanthology.org/2009.freeopmt-1.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Trend-based Document Clustering for Sensitive and Stable Topic Detection</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y08-1033" target="_blank">https://aclanthology.org/Y08-1033</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Recognizing Coordinate Structures for Machine Translation of English Patent Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y08-1049" target="_blank">https://aclanthology.org/Y08-1049</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using LDA to detect semantically incoherent documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W08-2106" target="_blank">https://aclanthology.org/W08-2106</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Simple is Best: Experiments with Different Document Segmentation Strategies for Passage Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W08-1803" target="_blank">https://aclanthology.org/W08-1803</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Generating Image Captions using Topic Focused Multi-document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W08-1401" target="_blank">https://aclanthology.org/W08-1401</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Graph-Based Keyword Extraction for Single-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W08-1404" target="_blank">https://aclanthology.org/W08-1404</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MultiSum: Query-Based Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W08-1405" target="_blank">https://aclanthology.org/W08-1405</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Mixed-Source Multi-Document Speech-to-Text Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W08-1406" target="_blank">https://aclanthology.org/W08-1406</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Evaluating automatically generated user-focused multi-document summaries for geo-referenced images</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W08-1407" target="_blank">https://aclanthology.org/W08-1407</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Dynamic Programming Approach to Document Length Constraints</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W08-1125" target="_blank">https://aclanthology.org/W08-1125</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>FastSum: Fast and Accurate Query-based Multi-document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P08-2052" target="_blank">https://aclanthology.org/P08-2052</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Pairwise Document Similarity in Large Collections with MapReduce</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P08-2067" target="_blank">https://aclanthology.org/P08-2067</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Weakly-Supervised Acquisition of Open-Domain Classes and Class Attributes from Web Documents and Query Logs</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P08-1003" target="_blank">https://aclanthology.org/P08-1003</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Refining Event Extraction through Cross-Document Inference</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P08-1030" target="_blank">https://aclanthology.org/P08-1030</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learning Document-Level Semantic Properties from Free-Text Annotations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P08-1031" target="_blank">https://aclanthology.org/P08-1031</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Can You Summarize This? Identifying Correlates of Input Difficulty for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P08-1094" target="_blank">https://aclanthology.org/P08-1094</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multiple Document Summarization Using Principal Component Analysis Incorporating Semantic Vector Space Model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O08-4001" target="_blank">https://aclanthology.org/O08-4001</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>多領域文件集之詞彙概念擴展與知識架構之建立 (Conceptual Expansion and Ontological Mapping of Multi-domain Documents) [In Chinese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O08-2008" target="_blank">https://aclanthology.org/O08-2008</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system expresses an information request in the form of a topic description which is used for an initial search in order to retrieve a relevant set of documents. On basis of this set of documents unsupervised relation extraction and clustering is done by the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Relation Extraction From Web Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2008/pdf/425_paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2008/pdf/425_paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>News articles about the same event published over time have properties that challenge NLP and IR applications. A cluster of such texts typically exhibits instances of paraphrase and contradiction as sources update the facts surrounding the story often due to an ongoing investigation. The current hypothesis is that the stories evolve over time beginning with the first text published on a given topic. This is tested using a phylogenetic approach as well as one based on language modeling. The fit of the evolutionary models is evaluated with respect to how well they facilitate the recovery of chronological relationships between the documents. Over all data clusters the language modeling approach consistently outperforms the phylogenetics model. However on manually collected clusters in which the documents are published within short time spans of one another both have a similar performance and produce statistically significant results on the document chronology recovery evaluation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Modeling Document Dynamics: an Evolutionary Approach</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2008/pdf/115_paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2008/pdf/115_paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes a newly created text corpus of news articles that has been annotated for cross-document co-reference. Being able to robustly resolve references to entities across document boundaries will provide a useful capability for a variety of tasks ranging from practical information retrieval applications to challenging research in information extraction and natural language understanding. This annotated corpus is intended to encourage the development of systems that can more accurately address this problem. A manual annotation tool was developed that allowed the complete corpus to be searched for likely co-referring entity mentions. This corpus of 257K words links mentions of co-referent people locations and organizations (subject to some additional constraints). Each of the documents had already been annotated for within-document co-reference by the LDC as part of the ACE series of evaluations. The annotation process was bootstrapped with a string-matching-based linking procedure and we report on some of initial experimentation with the data. The cross-document linking information will be made publicly available.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Corpus for Cross-Document Co-reference</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2008/pdf/762_paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2008/pdf/762_paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The re-use of spoken word audio collections maintained by audiovisual archives is severely hindered by their generally limited access. The CHoral project which is part of the CATCH program funded by the Dutch Research Council aims to provide users of speech archives with online instead of on-location access to relevant fragments instead of full documents. To meet this goal a spoken document retrieval framework is being developed. In this paper the evaluation efforts undertaken so far to assess and improve various aspects of the framework are presented. These efforts include (i) evaluation of the automatically generated textual representations of the spoken word documents that enable word-based search (ii) the development of measures to estimate the quality of the textual representations for use in information retrieval and (iii) studies to establish the potential user groups of the to-be-developed technology and the first versions of the user interface supporting online access to spoken word collections.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Evaluation of Spoken Document Retrieval for Historic Speech Collections</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2008/pdf/520_paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2008/pdf/520_paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The rapid growth of the Internet means that more information is available than ever before. Multilingual multi-document summarisation offers a way to access this information even when it is not in a language spoken by the reader by extracting the gist from related documents and translating it automatically. This paper presents an experiment in which Maximal Marginal Relevance (MMR) a well known multi-document summarisation method is used to produce summaries from Romanian news articles. A task-based evaluation performed on both the original summaries and on their automatically translated versions reveals that they still contain a significant portion of the important information from the original texts. However direct evaluation of the automatically translated summaries shows that they are not very legible and this can put off some readers who want to find out more about a topic.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Evaluation of a Cross-lingual Romanian-English Multi-document Summariser</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2008/pdf/539_paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2008/pdf/539_paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The goal of the DARPA MADCAT (Multilingual Automatic Document Classification Analysis and Translation) Program is to automatically convert foreign language text images into English transcripts for use by humans and downstream applications. The first phase the program focuses on translation of handwritten Arabic documents. Linguistic Data Consortium (LDC) is creating publicly available linguistic resources for MADCAT technologies on a scale and richness not previously available. Corpora will consist of existing LDC corpora and data donations from MADCAT partners plus new data collection to provide high quality material for evaluation and to address strategic gaps (for genre dialect image quality etc.) in the existing resources. Training and test data properties will expand over time to encompass a wide range of topics and genres: letters diaries training manuals brochures signs ledgers memos instructions postcards and forms among others. Data will be ground truthed with line word and token segmentation and zoning and translations and word alignments will be produced for a subset. Evaluation data will be carefully selected from the available data pools and high quality references will be produced which can be used to compare MADCAT system performance against the human-produced gold standard.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>New Resources for Document Classification Analysis and Translation Technologies</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2008/pdf/772_paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2008/pdf/772_paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we present a new Document Management System called DrStorage. This DMS is multi-platform JCR-170 compliant supports WebDav versioning user authentication and authorization and the most widespread file formats (Adobe PDF Microsoft Office HTML...). It is also easy to customize in order to enhance its search capabilities and to support automatic metadata assignment. DrStorage has been integrated with an automatic language guesser and with an automatic keyword extractor: these metadata can be assigned automatically to documents because the DrStorages server part has benn modified to allow that metadata assignment takes place as documents are put in the repository. Metadata can greatly improve the search capabilites and the results quality of a search engine. DrStorages client has been customized with two search results view: the first called timeline view shows temporal trends of queries as an histogram the second keyword cloud shows which words are correlated and how much are correlated with the results of a particular day.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Integration of a Multilingual Keyword Extractor in a Document Management System</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2008/pdf/346_paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2008/pdf/346_paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper proposes a ping-pong document clustering method using NMF and the linkage based refinement alternately in order to improve the clustering result of NMF. The use of NMF in the ping-pong strategy can be expected effective for document clustering. However NMF in the ping-pong strategy often worsens performance because NMF often fails to improve the clustering result given as the initial values. Our method handles this problem with the stop condition of the ping-pong process. In the experiment we compared our method with the k-means and NMF by using 16 document data sets. Our method improved the clustering result of NMF significantly.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Ping-pong Document Clustering using NMF and Linkage-Based Refinement</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2008/pdf/38_paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2008/pdf/38_paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>What kinds of lexical resources are helpful for extracting useful information from domain-specific documents? Although domain-specific documents contain much useful knowledge it is not obvious how to extract such knowledge efficiently from the documents. We need to develop techniques for extracting hidden information from such domain-specific documents. These techniques do not necessarily use state-of-the-art technologies and achieve deep and accurate language understanding but are based on huge amounts of linguistic resources such as domain-specific lexical databases. In this paper we introduce two techniques for extracting informative expressions from documents: the extraction of related words that are not only taxonomically related but also thematically related and the acquisition of salient terms and phrases. With these techniques we then attempt to automatically and statistically extract domain-specific informative expressions in aviation documents as an example and evaluate the results.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extraction of Informative Expressions from Domain-specific Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2008/pdf/410_paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2008/pdf/410_paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We have analyzed the SPEX algorithm by Bernstein and Zobel (2004) for detecting co-derivative documents using duplicate n-grams. Although we totally agree with the claim that not using unique n-grams can greatly increase the efficiency and scalability of the process of detecting co-derivative documents we have found serious bottlenecks in the way SPEX finds the duplicate n-grams. While the memory requirements for computing co-derivative documents can be reduced to up to 1% by only using duplicate n-grams SPEX needs about 40 times more memory for computing the list of duplicate n-grams itself. Therefore the memory requirements of the whole process are not reduced enough to make the algorithm practical for very large collections. We propose a solution for this problem using an external sort with the suffix array in-memory sorting and temporary file compression. The proposed algorithm for computing duplicate n-grams uses a fixed amount of memory for any input size.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Detecting Co-Derivative Documents in Large Text Collections</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2008/pdf/481_paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2008/pdf/481_paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper focuses on automatically improving the readability of documents. We explore mechanisms relating to content control that could be used (i) by authors to improve the quality and consistency of the language used in authoring; and (ii) to find a means to demonstrate this to readers. To achieve this we implemented and evaluated a number of software components including those of the University of Surrey Department of Computings content analysis applications (System Quirk). The software integrates these components within the commonly available GATE software and incorporates language resources considered useful within the standards development process: a Plain English thesaurus; lookup of ISO terminology provided from a terminology management system (TMS) via ISO 16642; automatic terminology discovery using statistical and linguistic techniques; and readability metrics. Results lead us to the development of an assistive tool initially for authors of standards but not considered to be limited only to such authors and also to a system that provides automatic annotation of texts to help readers to understand them. We describe the system developed and made freely available under the auspices of the EU eContent project LIRICS.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Document Quality Control</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2008/pdf/145_paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2008/pdf/145_paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The NIST Automatic Content Extraction (ACE) Evaluation expands its focus in 2008 to encompass the challenge of cross-document and cross-language global integration and reconciliation of information. While past ACE evaluations have been limited to local (within-document) detection and disambiguation of entities relations and events the current evaluation adds global (cross-document and cross-language) entity disambiguation tasks for Arabic and English. This paper presents the 2008 ACE XDoc evaluation task and associated infrastructure. We describe the linguistic resources created by LDC to support the evaluation focusing on new approaches required for data selection data processing annotation task definitions and annotation software and we conclude with a discussion of the metrics developed by NIST to support the evaluation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Linguistic Resources and Evaluation Techniques for Evaluation of Cross-Document Automatic Content Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2008/pdf/677_paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2008/pdf/677_paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Although the World Wide Web has late become an important source to consult for the meaning of words a number of technical terms related to high technology are not found on the Web. This paper describes a method to produce an encyclopedic dictionary for high-tech terms from patent information. We used a collection of unexamined patent applications published by the Japanese Patent Office as a source corpus. Given this collection we extracted terms as headword candidates and retrieved applications including those headwords. Then we extracted paragraph-style descriptions and categorized them into technical domains. We also extracted related terms for each headword. We have produced a dictionary including approximately 400000 Japanese terms as headwords. We have also implemented an interface with which users can explore our dictionary by reading text descriptions and viewing a related-term graph.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Producing an Encyclopedic Dictionary using Patent Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2008/pdf/519_paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2008/pdf/519_paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The Spoken Document Processing Working Group which is part of the special interest group of spoken language processing of the Information Processing Society of Japan is developing a test collection for evaluation of spoken document retrieval systems. A prototype of the test collection consists of a set of textual queries relevant segment lists and transcriptions by an automatic speech recognition system allowing retrieval from the Corpus of Spontaneous Japanese (CSJ). From about 100 initial queries application of the criteria that a query should have more than five relevant segments that consist of about one minute speech segments yielded 39 queries. Targeting the test collection an ad hoc retrieval experiment was also conducted to assess the baseline retrieval performance by applying a standard method for spoken document retrieval.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Test Collections for Spoken Document Retrieval from Lecture Audio Data</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2008/pdf/400_paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2008/pdf/400_paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes the interaction among language resources for an adequate concept annotation of domain texts in several languages. The architecture includes domain ontology domain texts language specific lexicons regular grammars and disambiguation rules. Ontology plays a central role in the architecture. We assume that it represents the meaning of the terms in the lexicons. Thus the lexicons for the languages of the project (http://www.lt4el.eu/ - the LT4eL (Language Technology for eLearning) project is supported by the European Community under the Information Society and Media Directorate Learning and Cultural Heritage Unit.) are constructed on the base of the ontology. The grammars and disambiguation rules facilitate the annotation of the text with concepts from the ontology. The established in this way relation between ontology and text supports different searches for content in the annotated documents. This is considered the preparatory phase for the integration of a semantic search facility in Learning Management Systems. The implementation and performance of this search are discussed in the context of related work as well as other types of searches. Also the results from some preliminary steps towards evaluation of the concept-based and text-based search are presented.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Language Resources for Semantic Document Annotation and Crosslingual Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2008/pdf/478_paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2008/pdf/478_paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Progress in the Machine Translation (MT) research community particularly for statistical approaches is intensely data-driven. Acquiring source language documents for testing creating training datasets for customized MT lexicons and building parallel corpora for MT evaluation require translators and non-native speaking analysts to handle large document collections. These collections are further complicated by differences in format encoding source media and access to metadata describing the documents. Automated tools that allow language professionals to quickly annotate translate and evaluate foreign language documents are essential to improving MT quality and efficacy. The purpose of this paper is present our research approach to improving MT through pre-processing source language documents. In particular we will discuss the development and use of MTriage an application environment that enables the translator to markup documents with metadata for MT parameterization and routing. The use of MTriage as a web-enabled front end to multiple MT engines has leveraged the capabilities of our human translators for creating lexicons from NFW (Not-Found-Word) lists writing reference translations and creating parallel corpora for MT development and evaluation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MTriage: Web-enabled Software for the Creation Machine Translation and Annotation of Smart Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2008/pdf/844_paper.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2008/pdf/844_paper.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Identifying Similar and Co-referring Documents Across Languages</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I08-6002" target="_blank">https://aclanthology.org/I08-6002</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Script Independent Word Spotting in Multilingual Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I08-6007" target="_blank">https://aclanthology.org/I08-6007</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Document Graph Based Query Focused Multi-Document Summarizer</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I08-6008" target="_blank">https://aclanthology.org/I08-6008</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Multi-Document Multi-Lingual Automatic Summarization System</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I08-2101" target="_blank">https://aclanthology.org/I08-2101</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Sentence Ordering based on Cluster Adjacency in Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I08-2103" target="_blank">https://aclanthology.org/I08-2103</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Fast Duplicated Documents Detection using Multi-level Prefix-filter</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I08-2121" target="_blank">https://aclanthology.org/I08-2121</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Lexical Chains as Document Features</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I08-1015" target="_blank">https://aclanthology.org/I08-1015</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Entity-driven Rewrite for Multi-document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I08-1016" target="_blank">https://aclanthology.org/I08-1016</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A New Approach to Automatic Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I08-1017" target="_blank">https://aclanthology.org/I08-1017</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Identifying Cross-Document Relations between Sentences</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I08-1019" target="_blank">https://aclanthology.org/I08-1019</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Experiments on Semantic-based Clustering for Cross-document Coreference</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I08-1020" target="_blank">https://aclanthology.org/I08-1020</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Identification of Rhetorical Roles using Conditional Random Fields for Legal Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I08-1063" target="_blank">https://aclanthology.org/I08-1063</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Who is Who and What is What: Experiments in Cross-Document Co-Reference</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D08-1029" target="_blank">https://aclanthology.org/D08-1029</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Exploration of Document Impact on Graph-Based Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D08-1079" target="_blank">https://aclanthology.org/D08-1079</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Topic-Driven Multi-Document Summarization with Encyclopedic Knowledge and Spreading Activation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D08-1080" target="_blank">https://aclanthology.org/D08-1080</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Scalable MMR Approach to Sentence Scoring for Multi-Document Update Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C08-2006" target="_blank">https://aclanthology.org/C08-2006</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Bayes Risk-based Dialogue Management for Document Retrieval System with Speech Interface</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C08-2015" target="_blank">https://aclanthology.org/C08-2015</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Improved Hierarchical Bayesian Model of Language for Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C08-1004" target="_blank">https://aclanthology.org/C08-1004</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Modeling Chinese Documents with Topical Word-Character Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C08-1044" target="_blank">https://aclanthology.org/C08-1044</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CollabRank: Towards a Collaborative Approach to Single-Document Keyphrase Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C08-1122" target="_blank">https://aclanthology.org/C08-1122</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Dans cet article nous proposons une approche int&#39;egr&#39;ee localis&#39;ee pour la g&#39;en&#39;eration. Dans cette approche le traitement int&#39;egr&#39;e des d&#39;ecisions linguistiques est limit&#39;e `a la production des propositions dont les d&#39;ecisions qui concernent leurs g&#39;en&#39;erations sont d&#39;ependantes. La g&#39;en&#39;eration se fait par groupes de propositions de tailles limit&#39;ees avec traitement int&#39;egr&#39;e des d&#39;ecisions linguistiques qui concernent la production des propositions qui appartiennent au m^eme groupe. Notre approche apporte une solution pour le probl`eme de complexit&#39;e computationnelle de la g&#39;en&#39;eration int&#39;egr&#39;ee classique. Elle fournit ainsi une alternative `a la g&#39;en&#39;eration s&#39;epar&#39;ee (s&#39;equentielle ou interactive) qui pr&#39;esente plusieurs d&#39;efauts mais qui est impl&#39;ement&#39;ee de mani`ere r&#39;epandue dans les syst`emes de g&#39;en&#39;erations existants.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>G&#39;en&#39;eration int&#39;egr&#39;ee localis&#39;ee pour la production de documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2008.jeptalnrecital-recital.9" target="_blank">https://aclanthology.org/2008.jeptalnrecital-recital.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Dans cet article nous pr&#39;esentons les am&#39;eliorations que nous avons apport&#39;ees au syst`eme ExtraNews de r&#39;esum&#39;e automatique de documents multiples. Ce syst`eme se base sur l&#39;utilisation d&#39;un algorithme g&#39;en&#39;etique qui permet de combiner les phrases des documents sources pour former les extraits qui seront crois&#39;es et mut&#39;es pour g&#39;en&#39;erer de nouveaux extraits. La multiplicit&#39;e des crit`eres de s&#39;election d&#39;extraits nous a inspir&#39;e une premi`ere am&#39;elioration qui consiste `a utiliser une technique d&#39;optimisation multi-objectif en vue d&#39;&#39;evaluer ces extraits. La deuxi`eme am&#39;elioration consiste `a int&#39;egrer une &#39;etape de pr&#39;e-filtrage de phrases qui a pour objectif la r&#39;eduction du nombre des phrases des textes sources en entr&#39;ee. Une &#39;evaluation des am&#39;eliorations apport&#39;ees `a notre syst`eme est r&#39;ealis&#39;ee sur les corpus de DUC&#39;04 et DUC&#39;07.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Int&#39;egration d&#39;une &#39;etape de pr&#39;e-filtrage et d&#39;une fonction multiobjectif en vue d&#39;am&#39;eliorer le syst`eme ExtraNews de r&#39;esum&#39;e de documents multiples</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2008.jeptalnrecital-long.11" target="_blank">https://aclanthology.org/2008.jeptalnrecital-long.11</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Cet article porte sur le regroupement automatique de documents sur une base &#39;ev&#39;enementielle. Apr`es avoir pr&#39;ecis&#39;e la notion d&#39;&#39;ev&#39;enement nous nous int&#39;eressons `a la repr&#39;esentation des documents d&#39;un corpus de d&#39;ep^eches puis `a une approche d&#39;apprentissage pour r&#39;ealiser les regroupements de mani`ere non supervis&#39;ee fond&#39;ee sur k-means. Enfin nous &#39;evaluons le syst`eme de regroupement de documents sur un corpus de taille r&#39;eduite et nous discutons de l&#39;&#39;evaluation quantitative de ce type de t^ache.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Regroupement automatique de documents en classes &#39;ev&#39;enementielles</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2008.jeptalnrecital-court.13" target="_blank">https://aclanthology.org/2008.jeptalnrecital-court.13</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Les moteurs de recherches sur le web produisent des r&#39;esultats comparables et assez satisfaisants pour la recherche de documents &#39;ecrits en caract`eres latins. Cependant ils pr&#39;esentent de s&#39;erieuses lacunes d`es que l&#39;ont s&#39;int&#39;eresse `a des langues peu dot&#39;ees ou des langues s&#39;emitiques comme l&#39;arabe. Dans cet article nous pr&#39;esentons une &#39;etude analytique et qualitative de la recherche d&#39;information en langue arabe en mettant l&#39;accent sur l&#39;insuffisance des outils de recherche actuels souvent mal adapt&#39;es aux sp&#39;ecificit&#39;es de la langue arabe. Pour argumenter notre analyse nous pr&#39;esentons des r&#39;esultats issus d&#39;observations et de tests autour de certains ph&#39;enom`enes linguistiques de l&#39;arabe &#39;ecrit. Pour la validation des ces observations nous avons test&#39;e essentiellement le moteur de recherche Google.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Dissym&#39;etrie entre l&#39;indexation des documents et le traitement des requ^etes pour la recherche d&#39;information en langue arabe</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2008.jeptalnrecital-court.22" target="_blank">https://aclanthology.org/2008.jeptalnrecital-court.22</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2008</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper presents a method for exploiting document-level similarity between the documents in the training corpus for a corpus-driven (statistical or example-based) machine translation system and the input documents it must translate. The method is simple to implement efficient (increases the translation time of an example-based system by only a few percent) and robust (still works even when the actual document boundaries in the input text are not known). Experiments on French-English and Arabic-English showed relative gains over the same system without using document-level similarity of up to 7.4% and 5.4% respectively on the BLEU metric.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploiting Document-Level Context for Data-Driven Machine Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2008.amta-papers.2" target="_blank">https://aclanthology.org/2008.amta-papers.2</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Refinement of Document Clustering by Using NMF</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y07-1045" target="_blank">https://aclanthology.org/Y07-1045</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Spoken Document Retrieval in a Highly Inflectional Language</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W07-2408" target="_blank">https://aclanthology.org/W07-2408</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Visualising Discourse Structure in Interactive Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W07-2313" target="_blank">https://aclanthology.org/W07-2313</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Processing of Diabetic Patients&#39; Hospital Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W07-1705" target="_blank">https://aclanthology.org/W07-1705</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Information Extraction from Patients&#39; Free Form Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W07-1025" target="_blank">https://aclanthology.org/W07-1025</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Indexing of Specialized Documents: Using Generic vs. Domain-Specific Document Representations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W07-1026" target="_blank">https://aclanthology.org/W07-1026</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Arabic Cross-Document Person Name Normalization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W07-0804" target="_blank">https://aclanthology.org/W07-0804</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Timestamped Graphs: Evolutionary Models of Text for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W07-0204" target="_blank">https://aclanthology.org/W07-0204</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SHEF: Semantic Tagging and Summarization Techniques Applied to Cross-document Coreference</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/S07-1063" target="_blank">https://aclanthology.org/S07-1063</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Ensemble document clustering using weighted hypergraph generated by NMF</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P07-2020" target="_blank">https://aclanthology.org/P07-2020</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Measuring Importance and Query Relevance in Topic-focused Multi-document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P07-2049" target="_blank">https://aclanthology.org/P07-2049</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Topic Analysis for Psychiatric Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P07-1129" target="_blank">https://aclanthology.org/P07-1129</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Finding document topics for improving topic segmentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P07-1061" target="_blank">https://aclanthology.org/P07-1061</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards an Iterative Reinforcement Approach for Simultaneous Document Summarization and Keyword Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P07-1070" target="_blank">https://aclanthology.org/P07-1070</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>應用文件重排序與局部查詢擴展於中文文件檢索之研究 (Improving Retrieval Effectiveness by Document Reranking and Local Expansion) [In Chinese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O07-2010" target="_blank">https://aclanthology.org/O07-2010</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Fast Method for Parallel Document Identification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N07-2008" target="_blank">https://aclanthology.org/N07-2008</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Similarity Measures to Distinguish Native vs. Non-Native Essay Writers</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N07-2013" target="_blank">https://aclanthology.org/N07-2013</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Hybrid Document Indexing with Spectral Embedding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N07-2029" target="_blank">https://aclanthology.org/N07-2029</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-Instance Tuning of Unsupervised Document Clustering Algorithms</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N07-1032" target="_blank">https://aclanthology.org/N07-1032</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Document Relationship Fusion via Constraints on Probabilistic Databases</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N07-1042" target="_blank">https://aclanthology.org/N07-1042</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Topic Segmentation with Hybrid Document Indexing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D07-1037" target="_blank">https://aclanthology.org/D07-1037</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Bayesian Document Generative Model with Explicit Multiple Topics</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D07-1044" target="_blank">https://aclanthology.org/D07-1044</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Enhancing Single-Document Summarization by Combining RankNet and Third-Party Sources</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D07-1047" target="_blank">https://aclanthology.org/D07-1047</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Statistical Language Modeling Approach to Lattice-Based Spoken Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D07-1085" target="_blank">https://aclanthology.org/D07-1085</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Building Lexicon for Sentiment Analysis from Massive Collection of HTML Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/D07-1115" target="_blank">https://aclanthology.org/D07-1115</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Patent documentation -- comparison of two MT strategies</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2007.mtsummit-wpt.5" target="_blank">https://aclanthology.org/2007.mtsummit-wpt.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Enhancing image-based Arabic document translation using noisy channel correction model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2007.mtsummit-papers.12" target="_blank">https://aclanthology.org/2007.mtsummit-papers.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Pour des raisons vari&#39;ees diverses communaut&#39;es se sont int&#39;eress&#39;ees aux corpus multilingues. Parmi ces corpus les textes parall`eles sont utilis&#39;es aussi bien en terminologie lexicographie ou comme source d&#39;informations pour les syst`emes de traduction par l&#39;exemple. L&#39;Union Europ&#39;eenne qui a entra^in&#39;e la production de document l&#39;egislatif dans vingtaine de langues est une des sources de ces textes parall`eles. Aussi avec le Web comme vecteur principal de diffusion de ces textes parall`eles cet objet d&#39;&#39;etude est pass&#39;e `a un nouveau statut : celui de document. Cet article d&#39;ecrit un syst`eme d&#39;alignement prenant en compte un grand nombre de langues simultan&#39;ement (textgreater 2) et les caract&#39;eristiques structurelles des documents analys&#39;es.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extraction endog`ene d&#39;une structure de document pour un alignement multilingue</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2007.jeptalnrecital-recitalposter.4" target="_blank">https://aclanthology.org/2007.jeptalnrecital-recitalposter.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2007</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>L&#39;un des objectifs du projet ALVIS est d&#39;int&#39;egrer des informations linguistiques dans des moteurs de recherche sp&#39;ecialis&#39;es. Dans ce contexte nous avons conccu une plate-forme d&#39;enrichissement linguistique de documents issus du Web OGMIOS exploitant des outils de TAL existants. Les documents peuvent ^etre en franccais ou en anglais. Cette architecture est distribu&#39;ee afin de r&#39;epondre aux contraintes li&#39;ees aux traitements de gros volumes de textes et adaptable pour permettre l&#39;analyse de sous-langages. La plate-forme est d&#39;evelopp&#39;ee en Perl et disponible sous forme de modules CPAN. C&#39;est une structure modulaire dans lequel il est possible d&#39;int&#39;egrer de nouvelles ressources ou de nouveaux outils de TAL. On peut ainsi d&#39;efinir des configuration diff&#39;erentes pour diff&#39;erents domaines et types de collections. Cette plateforme robuste permet d&#39;analyser en masse des donn&#39;ees issus du web qui sont par essence tr`es h&#39;et&#39;erog`enes. Nous avons &#39;evalu&#39;e les performances de la plateforme sur plusieurs collections de documents. En distribuant les traitements sur vingt machines une collection de 55 329 documents du domaine de la biologie (106 millions de mots) a &#39;et&#39;e annot&#39;ee en 35 heures tandis qu&#39;une collection de 48 422 d&#39;ep^eches relatives aux moteurs de recherche (14 millions de mots) a &#39;et&#39;e annot&#39;ee en 3 heures et 15 minutes.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>OGMIOS : une plate-forme d&#39;annotation linguistique de collection de documents issus du Web</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2007.jeptalnrecital-poster.10" target="_blank">https://aclanthology.org/2007.jeptalnrecital-poster.10</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Clustering Method Based on Frequent Co-occurring Words</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y06-1066" target="_blank">https://aclanthology.org/Y06-1066</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extracting formal specifications from natural language regulatory documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W06-3902" target="_blank">https://aclanthology.org/W06-3902</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Graph-based Generalized Latent Semantic Analysis for Document Representation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W06-3810" target="_blank">https://aclanthology.org/W06-3810</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Partitioning Parallel Documents Using Binary Segmentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W06-3111" target="_blank">https://aclanthology.org/W06-3111</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Which Side are You on? Identifying Perspectives at the Document and Sentence Levels</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W06-2915" target="_blank">https://aclanthology.org/W06-2915</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Querying XML documents with multi-dimensional markup</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W06-2706" target="_blank">https://aclanthology.org/W06-2706</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Multiclassifier based Document Categorization System: profiting from the Singular Value Decomposition Dimensionality Reduction Technique</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W06-2604" target="_blank">https://aclanthology.org/W06-2604</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Sentence ordering with manifold-based classification in multi-document summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W06-1662" target="_blank">https://aclanthology.org/W06-1662</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Dating of Documents and Temporal Text Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W06-0903" target="_blank">https://aclanthology.org/W06-0903</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploring Semantic Constraints for Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W06-0805" target="_blank">https://aclanthology.org/W06-0805</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Dimensionality Reduction Aids Term Co-Occurrence Based Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W06-0701" target="_blank">https://aclanthology.org/W06-0701</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A System for Summarizing and Visualizing Arguments in Subjective Documents: Toward Supporting Decision Making</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W06-0303" target="_blank">https://aclanthology.org/W06-0303</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>User-directed Sentiment Analysis: Visualizing the Affective Content of Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W06-0304" target="_blank">https://aclanthology.org/W06-0304</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Proceedings of the Workshop on Information Extraction Beyond The Document</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W06-0200" target="_blank">https://aclanthology.org/W06-0200</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Topic-Focused Multi-Document Summarization Using an Approximate Oracle Score</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P06-2020" target="_blank">https://aclanthology.org/P06-2020</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Obfuscating Document Stylometry to Preserve Author Anonymity</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P06-2058" target="_blank">https://aclanthology.org/P06-2058</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Construction of Polarity-Tagged Corpus from HTML Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P06-2059" target="_blank">https://aclanthology.org/P06-2059</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Bottom-Up Approach to Sentence Ordering for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P06-1049" target="_blank">https://aclanthology.org/P06-1049</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Comparison of Document Sentence and Term Event Spaces</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P06-1076" target="_blank">https://aclanthology.org/P06-1076</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Are These Documents Written from Different Perspectives? A Test of Different Perspectives Based on Statistical Distribution Divergence</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P06-1133" target="_blank">https://aclanthology.org/P06-1133</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilingual Document Clustering: An Heuristic Approach Based on Cognate Named Entities</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P06-1144" target="_blank">https://aclanthology.org/P06-1144</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Spoken Document Processing for Retrieval and Browsing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N06-5004" target="_blank">https://aclanthology.org/N06-5004</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Identifying Perspectives at the Document and Sentence Levels Using Statistical Models</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N06-3005" target="_blank">https://aclanthology.org/N06-3005</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Representation and Multilevel Measures of Document Similarity</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N06-3007" target="_blank">https://aclanthology.org/N06-3007</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Temporal Classification of Text and Automatic Document Dating</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N06-2008" target="_blank">https://aclanthology.org/N06-2008</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improved Affinity Graph Based Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N06-2046" target="_blank">https://aclanthology.org/N06-2046</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Language Model Information Retrieval with Document Expansion</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N06-1052" target="_blank">https://aclanthology.org/N06-1052</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards Spoken-Document Retrieval for the Internet: Lattice Indexing For Large-Scale Web-Search Architectures</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N06-1053" target="_blank">https://aclanthology.org/N06-1053</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Language Model-Based Document Clustering Using Random Walks</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N06-1061" target="_blank">https://aclanthology.org/N06-1061</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes the Pavia Typological Database (PTD) a follow-up to the MED-TYP database (Sans`o 2004). The PTD is an ever-growing repository of primary linguistic data (words clauses sentences) documenting a number of morphosyntactic phenomena in the languages of Europe (and including in some cases languages from the Mediterranean area). Its prospective users are typologists wanting to access primary typologically uninterpreted (but glossed) data but also anyone interested in linguistic variation on a continental scale. The paper discusses the background and motivation for the creation of the PTD its present coverage the techniques used to annotate the primary data and the general architecture of the database.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Documenting variation across Europe and the Mediterranean: the Pavia Typological Database</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2006/pdf/35_pdf.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2006/pdf/35_pdf.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The aim of the paper is twofold. Firstly an approach is presented how to select the correct antecedent for an anaphoric element according to the kind of text segments in which both of them occur. Basically information on logical text structure (e.g. chapters sections paragraphs) is used in order to select the antecedent life span of a linguistic expression i.e. some linguistic expressions are more likely to be chosen as an antecedent throughout the whole text than others. In addition an appropriate search scope for an anaphora expressed by an expression can be defined according to the document structuring elements that include the linguistic expression. Corpus investigations give rise to the supposition that logical text structure influences the search scope of candidates for antecedents. Second a solution is presented how to integrate the resources used for anaphora resolution. In this approach multi-layered XML annotation is used in order to make a set of resources accessible for the anaphora resolution system.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploiting logical document structure for anaphora resolution</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2006/pdf/266_pdf.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2006/pdf/266_pdf.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>A series of different automatic query expansion techniques has been suggested in Information Retrieval. To estimate how suitable a document term is as an expansion term the most popular of them use a measure of the frequency of the co-occurrence of this term with one or several query terms. The benefit of the use of the linguistic relations that hold between query terms is often questioned. If a linguistic phenomenon is taken into account it is the phrase structure or lexical compound. We propose a technique that is based on the restricted lexical cooccurrence (collocation) of query terms. We use the knowledge on collocations formed by query terms for two tasks: (i) document relevance clustering done in the first stage of local query expansion and (ii) choice of suitable expansion terms from the relevant document cluster. In this paper we describe the first task providing evidence from first preliminary experiments on Spanish material that local relevance clustering benefits largely from knowledge on collocations.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Local Document Relevance Clustering in IR Using Collocation Information</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2006/pdf/381_pdf.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2006/pdf/381_pdf.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We describe a number of experiments carried out to address the problem of creating summaries from multiple sources in multiple languages. A centroid-based sentence extraction system has been developed which decides the content of the summary using texts in different languages and uses sentences from English sources alone to create the final output. We describe the evaluation of the system in the recent Multilingual Summarization Evaluation MSE 2005 using the pyramids and ROUGE methods.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilingual Multidocument Summarization Tools and Evaluation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2006/pdf/498_pdf.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2006/pdf/498_pdf.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes the development of an Arabic document image collection containing 34651 documents from 1378 different books and 25 topics with their relevance judgments. The books from which the collection is obtained are a part of a larger collection 75000 books being scanned for archival and retrieval at the bibliotheca Alexandrina (BA). The documents in the collection vary widely in topics fonts and degradation levels. Initial baseline experiments were performed to examine the effectiveness of different index terms with and without blind relevance feedback on Arabic OCR degraded text.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Building a Heterogeneous Information Retrieval Collection of Printed Arabic Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2006/pdf/509_pdf.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2006/pdf/509_pdf.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>The paper describes the ALVIS annotation format and discusses the problems that we encountered for the indexing of large collections of documents for topic specific search engines. This paper is exemplified on the biological domain and on MedLine abstracts as developing a specialized search engine for biologist is one of the ALVIS case studies. The ALVIS principle for linguistic annotations is based on existing works and standard propositions. We made the choice of stand-off annotations rather than inserted mark-up and annotations are encoded as XML elements which form the linguistic subsection of the document record.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The ALVIS Format for Linguistically Annotated Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2006/pdf/742_pdf.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2006/pdf/742_pdf.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Document Summarization of Evaluative Text</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/E06-1039" target="_blank">https://aclanthology.org/E06-1039</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Business Process Outsourcing in Document Management</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2006.tc-1.13" target="_blank">https://aclanthology.org/2006.tc-1.13</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Dans cet article nous cherchons `a caract&#39;eriser linguistiquement des segments textuels d&#39;efinis pragmatiquement relativement `a des besoins de r&#39;e&#39;edition de documents et au sein desquels l&#39;information est susceptible d&#39;&#39;evoluer dans le temps. Sur la base d&#39;un corpus de textes encyclop&#39;ediques en franccais nous analysons la distribution de marqueurs textuels et discursifs et leur pertinence en nous focalisant principalement sur un traitement s&#39;emantique particulier de la temporalit&#39;e.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Rep&#39;erage de segments d&#39;information &#39;evolutive dans des documents de type encyclop&#39;edique</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2006.jeptalnrecital-recital.5" target="_blank">https://aclanthology.org/2006.jeptalnrecital-recital.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Dans cet article nous cherchons `a affiner la notion de comparabilit&#39;e des corpus. Nous &#39;etudions en particulier la distinction entre les documents scientifiques et vulgaris&#39;es dans le domaine m&#39;edical. Nous supposons que cette distinction peut apporter des informations importantes par exemple en recherche d&#39;information. Nous supposons par l`a m^eme que les documents &#39;etant le reflet de leur contexte de production fournissent des crit`eres n&#39;ecessaires `a cette distinction. Nous &#39;etudions plusieurs crit`eres linguistiques typographiques lexicaux et autres pour la caract&#39;erisation des documents m&#39;edicaux scientifiques et vulgaris&#39;es. Les r&#39;esultats pr&#39;esent&#39;es sont acquis sur les donn&#39;ees en russe et en japonais. Certains des crit`eres &#39;etudi&#39;es s&#39;av`erent effectivement pertinents. Nous faisons &#39;egalement quelques r&#39;eflexions et propositions quant `a la distinction des cat&#39;egories scientifique et vulgaris&#39;ee et aux questionnements th&#39;eoriques.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Relever des crit`eres pour la distinction automatique entre les documents m&#39;edicaux scientifiques et vulgaris&#39;es en russe et en japonais</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2006.jeptalnrecital-poster.16" target="_blank">https://aclanthology.org/2006.jeptalnrecital-poster.16</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Nous pr&#39;esentons un syst`eme de synth`ese d&#39;information pour la production de r&#39;esum&#39;es multidocuments orient&#39;es par une requ^ete complexe. Apr`es une analyse du profil de l&#39;utilisateur exprim&#39;e par des questions complexes nous comparons la similarit&#39;e entre les documents `a r&#39;esumer avec les questions `a deux niveaux : global et d&#39;etaill&#39;e. Cette &#39;etude d&#39;emontre l&#39;importance d&#39;&#39;etudier pour une requ^ete la pertinence d&#39;une phrase `a l&#39;int&#39;erieur de la structure th&#39;ematique du document. Cette m&#39;ethodologie a &#39;et&#39;e appliqu&#39;ee lors de notre participation `a la campagne d&#39;&#39;evaluation DUC 2005 o`u notre syst`eme a &#39;et&#39;e class&#39;e parmi les meilleurs.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>R&#39;esum&#39;e multidocuments orient&#39;e par une requ^ete complexe</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2006.jeptalnrecital-long.10" target="_blank">https://aclanthology.org/2006.jeptalnrecital-long.10</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Cet article &#39;etudie la r&#39;esolution des r&#39;ef&#39;erences `a des entit&#39;es lorsqu&#39;une repr&#39;esentation informatique de ces entit&#39;es est disponible. Nous nous int&#39;eressons `a un corpus de dialogues entre humains portant sur les grands titres de la presse francophone du jour et proposons une m&#39;ethode pour d&#39;etecter et r&#39;esoudre les r&#39;ef&#39;erences faites par les locuteurs aux articles des journaux. La d&#39;etection des expressions nominales qui r&#39;ef`erent `a ces documents est r&#39;ealis&#39;ee gr^ace `a une grammaire alors que le probl`eme de la d&#39;etection des pronoms qui r&#39;ef`erent aux documents est abord&#39;e par des moyens statistiques. La r&#39;esolution de ces expressions `a savoir l&#39;attribution des r&#39;ef&#39;erents fait quant `a elle l&#39;objet d&#39;un algorithme inspir&#39;e de la r&#39;esolution des cor&#39;ef&#39;erences. Ces propositions sont &#39;evalu&#39;ees par le biais de mesures quantitatives sp&#39;ecifiques.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>R&#39;esolution des r&#39;ef&#39;erences aux documents dans un corpus de dialogues humains</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2006.jeptalnrecital-long.23" target="_blank">https://aclanthology.org/2006.jeptalnrecital-long.23</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document and Media Exploitation Evaluation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2006.amta-users.4" target="_blank">https://aclanthology.org/2006.amta-users.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2006</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The impact of phrases in document clustering for Swedish</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W05-1724" target="_blank">https://aclanthology.org/W05-1724</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Feature-Based Segmentation of Narrative Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W05-0405" target="_blank">https://aclanthology.org/W05-0405</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Structuring Documents Efficiently</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/U05-1018" target="_blank">https://aclanthology.org/U05-1018</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-document Summarisation and the PASCAL Textual Entailment Challenge</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/U05-1030" target="_blank">https://aclanthology.org/U05-1030</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>SPEECH OGLE: Indexing Uncertainty for Spoken Document Search</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P05-3011" target="_blank">https://aclanthology.org/P05-3011</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Reformatting Web Documents via Header Trees</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P05-3031" target="_blank">https://aclanthology.org/P05-3031</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Field Information Extraction and Cross-Document Fusion</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P05-1060" target="_blank">https://aclanthology.org/P05-1060</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Practical Passage-based Approach for Chinese Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O05-1005" target="_blank">https://aclanthology.org/O05-1005</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>以概念分群為基礎之新聞事件自動摘要 (Concept Cluster Based News Document Summarization) [In Chinese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O05-1007" target="_blank">https://aclanthology.org/O05-1007</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Induction of Word and Phrase Alignments for Automatic Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/J05-4004" target="_blank">https://aclanthology.org/J05-4004</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Sentence Fusion for Multidocument News Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/J05-3002" target="_blank">https://aclanthology.org/J05-3002</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Domain Specific Word Extraction from Hierarchical Web Documents: A First Step Toward Building Lexicon Trees from Web Corpora</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I05-3009" target="_blank">https://aclanthology.org/I05-3009</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Language Independent Algorithm for Single and Multiple Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I05-2004" target="_blank">https://aclanthology.org/I05-2004</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Discovery of Attribute Words from Web Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I05-1010" target="_blank">https://aclanthology.org/I05-1010</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Clustering with Grouping and Chaining Algorithms</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I05-1025" target="_blank">https://aclanthology.org/I05-1025</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Machine Learning Approach to Sentence Ordering for Multidocument Summarization and Its Evaluation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/I05-1055" target="_blank">https://aclanthology.org/I05-1055</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatically Learning Cognitive Status for Multi-Document Summarization of Newswire</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/H05-1031" target="_blank">https://aclanthology.org/H05-1031</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Controlled Language and the Implementation of Machine Translation for Technical Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2005.tc-1.13" target="_blank">https://aclanthology.org/2005.tc-1.13</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We introduce a light-weight interlingua for a cross-language document retrieval system in the medical domain. It is composed of equivalence classes of semantically primitive language-specific subwords which are clustered by interlingual and intralingual synonymy. Each subword cluster represents a basic conceptual entity of the language-independent interlingua. Documents as well as queries are mapped to this interlingua level on which retrieval operations are performed. Evaluation experiments reveal that this interlingua-based retrieval model outperforms a direct translation approach.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Subword Clusters as Light-Weight Interlingua for Multilingual Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2005.mtsummit-papers.3" target="_blank">https://aclanthology.org/2005.mtsummit-papers.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes one approach to document authoring and natural language generation being pursued by the Summer Institute of Linguistics in cooperation with the University of Maryland Baltimore County. We will describe the tools provided for document authoring including a glimpse at the underlying controlled language and the semantic representation of the textual meaning. We will also introduce The Bible Translator&#39;s Assistantcopyright (TBTA) which is used to elicit and enter target language data as well as perform the actual text generation process. We conclude with a discussion of the usefulness of this paradigm from a Bible translation perspective and suggest several ways in which this work will benefit the field of computational linguistics.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Authoring the Bible for Minority Language Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2005.mtsummit-papers.9" target="_blank">https://aclanthology.org/2005.mtsummit-papers.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Cet article pr&#39;esente une m&#39;ethode d&#39;indexation automatique de documents bas&#39;ee sur une approche linguistique et statistique. Cette derni`ere est une combinaison s&#39;equentielle de l&#39;analyse linguistique du document `a indexer par l&#39;extraction des termes significatifs du document et de l&#39;analyse statistique par la d&#39;ecomposition en valeurs singuli`eres des mots composant le document. La pond&#39;eration des termes tire avantage de leur contexte local par rapport au document global par rapport `a la base de donn&#39;ees et de leur position par rapport aux autres termes les co-occurrences. Le syst`eme d&#39;indexation pr&#39;esent&#39;e fait des propositions d&#39;affectations du document `a un r&#39;ef&#39;erentiel m&#39;etier dont les th`emes sont pr&#39;ed&#39;efinis. Nous pr&#39;esentons les r&#39;esultats de l&#39;exp&#39;erimentation de ce syst`eme men&#39;ee sur un corpus des p^oles m&#39;etiers de la soci&#39;et&#39;e Suez-Environnement.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>De la linguistique aux statistiques pour indexer des documents dans un r&#39;ef&#39;erentiel m&#39;etier</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2005.jeptalnrecital-recitalcourt.9" target="_blank">https://aclanthology.org/2005.jeptalnrecital-recitalcourt.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2005</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Les corpus parall`eles sont d&#39;une importance capitale pour les applications multilingues de traitement automatique des langues. Malheureusement leur raret&#39;e est le maillon faible de plusieurs applications d&#39;int&#39;er^et. Extraire de tels corpus duWeb est une solution viable mais elle introduit une nouvelle probl&#39;ematique : il n&#39;est pas toujours trivial d&#39;identifier les documents parall`eles parmi tous ceux qui ont &#39;et&#39;e extraits. Dans cet article nous nous int&#39;eressons `a l&#39;identification automatique des paires de documents parall`eles contenues dans un corpus bilingue. Nous montrons que cette t^ache peut ^etre accomplie avec pr&#39;ecision en utilisant un ensemble restreint d&#39;invariants lexicaux. Nous &#39;evaluons &#39;egalement notre approche sur une t^ache de traduction automatique et montrons qu&#39;elle obtient des r&#39;esultats sup&#39;erieurs `a un syst`eme de r&#39;ef&#39;erence faisant usage d&#39;un lexique bilingue.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Paradocs: un syst`eme d&#39;identification automatique de documents parall`eles</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2005.jeptalnrecital-long.23" target="_blank">https://aclanthology.org/2005.jeptalnrecital-long.23</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Phrase-Based HMM Approach to Document/Abstract Alignment</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W04-3216" target="_blank">https://aclanthology.org/W04-3216</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>LexPageRank: Prestige in Multi-Document Text Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W04-3247" target="_blank">https://aclanthology.org/W04-3247</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Document Biography Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W04-3256" target="_blank">https://aclanthology.org/W04-3256</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using Soundex Codes for Indexing Names in ASR Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W04-2905" target="_blank">https://aclanthology.org/W04-2905</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>WordNet-based text document clustering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W04-2013" target="_blank">https://aclanthology.org/W04-2013</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Support Vector Machine Approach to Extracting Gene References into Function from Biological Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W04-1209" target="_blank">https://aclanthology.org/W04-1209</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Re-ranking based on Global and Local Terms</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W04-1103" target="_blank">https://aclanthology.org/W04-1103</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extending Document Summarization to Information Graphics</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W04-1002" target="_blank">https://aclanthology.org/W04-1002</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Handling Figures in Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W04-1011" target="_blank">https://aclanthology.org/W04-1011</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Evaluation of Summaries Using Document Graphs</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W04-1012" target="_blank">https://aclanthology.org/W04-1012</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Document Person Name Resolution</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W04-0701" target="_blank">https://aclanthology.org/W04-0701</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross Document Co-Reference Resolution Applications for People in the Legal Domain</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W04-0702" target="_blank">https://aclanthology.org/W04-0702</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Reference Resolution over a Restricted Domain: References to Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W04-0710" target="_blank">https://aclanthology.org/W04-0710</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Extensible Framework for Efficient Document Management using RDF and OWL</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W04-0608" target="_blank">https://aclanthology.org/W04-0608</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Text Type Structure and Logical Document Structure</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W04-0207" target="_blank">https://aclanthology.org/W04-0207</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Customizing Parallel Corpora at the Document Level</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P04-3005" target="_blank">https://aclanthology.org/P04-3005</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Weakly Supervised Learning for Cross-document Person Name Disambiguation Supported by Information Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P04-1076" target="_blank">https://aclanthology.org/P04-1076</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>應用語料庫和語意相依法則於中文語音文件之摘要 (Spoken Document Summarization Using Topic-Related Corpus and Semantic Dependency Grammar) [In Chinese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O04-1021" target="_blank">https://aclanthology.org/O04-1021</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Scaleable Multi-document Centroid-based Summarizer</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N04-3007" target="_blank">https://aclanthology.org/N04-3007</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-Document Coreference on a Large Scale Corpus</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N04-1002" target="_blank">https://aclanthology.org/N04-1002</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Acquiring Hyponymy Relations from Web Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N04-1010" target="_blank">https://aclanthology.org/N04-1010</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Securing Interpretability: The Case of Ega Language Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2004/pdf/138.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2004/pdf/138.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cost-effective Cross-lingual Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2004/pdf/201.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2004/pdf/201.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Highlighting Latent Structure in Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2004/pdf/232.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2004/pdf/232.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploiting Semantic Web Technologies for Intelligent Access to Historical Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2004/pdf/248.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2004/pdf/248.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper describes a novel clustering-based text summarization system that uses Multiple Sequence Alignment to improve the alignment of sentences within topic clusters. While most current clustering-based summarization systems base their summaries only on the common information contained in a collection of highly-related sentences our system constructs more informative summaries that incorporate both the redundant and unique contributions of the sentences in the cluster. When evaluated using ROUGE the summaries produced by our system represent a substantial improvement over the baseline which is at 63% of the human performance.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Document Summarization Using Multiple-Sequence Alignment</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2004/pdf/408.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2004/pdf/408.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Multi-document summaries produced via sentence extraction often suffer from a number of cohesion problems including dangling anaphora sudden shifts in topic and incorrect or awkward chronological ordering. Therefore the development of an automated revision process to correct such problems is a research area of current interest. We present the RevisionBank a corpus of 240 extractive multi-document summaries that have been manually revised to promote cohesion. The summaries were revised by six linguistic students using a constrained set of revision operations that we previously developed. In the current paper we describe the process of developing a taxonomy of cohesion problems and corrective revision operators that address such problems as well as an annotation schema for our corpus. Finally we discuss how our taxonomy and corpus can be used for the study of revision-based multi-document summarization as well as for summary evaluation.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>RevisionBank: A Resource for Revision-based Multi-document Summarization and Evaluation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2004/pdf/409.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2004/pdf/409.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Clusters of multiple news stories related to the same topic exhibit a number of interesting properties. For example when documents have been published at various points in time or by different authors or news agencies one finds many instances of paraphrasing information overlap and even contradiction. The current paper presents the Cross-document Structure Theory (CST) Bank a collection of multi-document clusters in which pairs of sentences from different documents have been annotated for cross-document structure theory relationships. We will describe how we built the corpus including our method for reducing the number of sentence pairs to be annotated by our hired judges using lexical similarity measures. Finally we will describe how CST and the CST Bank can be applied to different research areas such as multi-document summarization.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>CST Bank: A Corpus for the Study of Cross-document Structural Relationships</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2004/pdf/411.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2004/pdf/411.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper reports on a number of experiments in which we applied standard techniques from NLP in the context of documentation of endangered languages. We concentrated on the use of existing freely available toolkits. Specifically we explore the use of Finite-State Morphological Analysis Maximum Entropy Part-of-Speech Tagging and N-Gram Language Modeling.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Applying Computational Linguistic Techniques in a Documentary Project for Q&#39;anjob&#39;al (Mayan Guatemala)</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2004/pdf/412.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2004/pdf/412.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>For many years the Istituto di Teoria e Tecniche dell&#39;Informazione Giuridica (ITTIG) of the Consiglio Nazionale delle Ricerche has studied the evolution of legal language creating databases for documentation and digital retrieval of law texts. The ITTIG is attending to document legal language through information technology in order to provide as wide an access as possible to its findings. The Institute has recently created an on-line digital database that includes the full text of the most important Italian laws (Codes and Constitutions) from the 16th to the 20th century. The ITTIG is also in the process of preparing another database made up of contexts from the original 10th to the 20th century legal sources.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Methods of Digital Access for Legal Language Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2004/pdf/450.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2004/pdf/450.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>N-Gram Language Modeling for Robust Multi-Lingual Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2004/pdf/510.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2004/pdf/510.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Multi-Modal Documentation System for Warao</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2004/pdf/599.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2004/pdf/599.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Pumping Documents Through a Domain and Genre Classification Pipeline</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2004/pdf/641.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2004/pdf/641.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MEAD - A Platform for Multidocument Multilingual Text Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2004/pdf/757.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2004/pdf/757.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learning Domain Ontologies from Document Warehouses and Dedicated Web Sites</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/J04-2002" target="_blank">https://aclanthology.org/J04-2002</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Grammar Modularity and its Impact on Grammar Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C04-1001" target="_blank">https://aclanthology.org/C04-1001</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Answer-Focused Multi-Document Summarization Using a Question-Answering Engine</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C04-1063" target="_blank">https://aclanthology.org/C04-1063</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Dependency-based Sentence Alignment for Multiple Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C04-1064" target="_blank">https://aclanthology.org/C04-1064</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Re-ranking Based on Automatically Acquired Key Terms in Chinese Information Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C04-1069" target="_blank">https://aclanthology.org/C04-1069</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Corpus and Evaluation Measures for Multiple Document Summarization with Multiple Sources</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C04-1077" target="_blank">https://aclanthology.org/C04-1077</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using syntactic information to extract relevant terms for multi-document summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C04-1094" target="_blank">https://aclanthology.org/C04-1094</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Syntactic Simplification for Improving Content Selection in Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C04-1129" target="_blank">https://aclanthology.org/C04-1129</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extracting Hyponyms of Prespecified Hypernyms from Itemizations and Headings in Web Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C04-1135" target="_blank">https://aclanthology.org/C04-1135</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Multiple-Document Summarization System with User Interaction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C04-1144" target="_blank">https://aclanthology.org/C04-1144</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Integrating Cross-Lingually Relevant News Articles and Monolingual Web Documents in Bilingual Lexicon Acquisition</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C04-1149" target="_blank">https://aclanthology.org/C04-1149</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>From Controlled Document Authoring to Interactive Document Normalization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C04-1166" target="_blank">https://aclanthology.org/C04-1166</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Connecting Writers and Translators - XML Based Content Management of Product Documentation at Autodesk</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2004.tc-1.6" target="_blank">https://aclanthology.org/2004.tc-1.6</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilingual document management and workflow in the European institutions</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2004.tc-1.7" target="_blank">https://aclanthology.org/2004.tc-1.7</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Pour la recherche documentaire il est souvent int&#39;eressant d&#39;avoir une bonne mesure de confiance dans les r&#39;eponses trouv&#39;ees par le moteur de recherche. Une bonne estimation de pertinence peut permettre de faire un choix entre plusieurs r&#39;eponses (venant &#39;eventuellement de diff&#39;erents syst`emes) d&#39;appliquer des m&#39;ethodes d&#39;enrichissement additionnelles selon les besoins ou encore de permettre `a l&#39;utilisateur de prendre des d&#39;ecisions (comme d&#39;approfondir la recherche `a travers un dialogue). Nous proposons une m&#39;ethode permettant de faire une telle estimation utilisant des connaissances extraites d&#39;un ensemble de requ^etes connues pour en d&#39;eduire des pr&#39;edictions sur d&#39;autres requ^etes pos&#39;ees au syst`eme de recherche documentaire.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>M&#39;ethodes statistiques et apprentissage automatique pour l&#39;&#39;evaluation de requ^etes en recherche documentaire</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2004.jeptalnrecital-recitalposter.5" target="_blank">https://aclanthology.org/2004.jeptalnrecital-recitalposter.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Deux premi`eres &#39;etapes vers les documents auto-explicatifs</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2004.jeptalnrecital-long.11" target="_blank">https://aclanthology.org/2004.jeptalnrecital-long.11</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Cet article pr&#39;esente l&#39;int&#39;egration au sein d&#39;un analyseur syntaxique (Xerox Incremental Parser) de r`egles sp&#39;ecifiques qui permettent de lier l&#39;analyse grammaticale `a la s&#39;emantique des balises XML sp&#39;ecifiques `a un document donn&#39;e. Ces r`egles sont bas&#39;ees sur la norme XPath qui offre une tr`es grande finesse de description et permet de guider tr`es pr&#39;ecis&#39;ement l&#39;application de l&#39;analyseur sur une famille de documents partageant une m^eme DTD. Le r&#39;esultat est alors ^etre int&#39;egr&#39;e directement comme annotation dans le document trait&#39;e.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Annoter les documents XML avec un outil d&#39;analyse syntaxique</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2004.jeptalnrecital-long.30" target="_blank">https://aclanthology.org/2004.jeptalnrecital-long.30</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2004</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Several major telecommunications companies have made significant investment in either controlled language and/or machine translation over the past 10 years.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Case study: implementing MT for the translation of pre-sales marketing and post-sales software deployment documentation at Mycom International</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://link.springer.com/chapter/10.1007/978-3-540-30194-3_1" target="_blank">https://link.springer.com/chapter/10.1007/978-3-540-30194-3_1</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The SVM With Uneven Margins and Chinese Document Categorization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y03-1024" target="_blank">https://aclanthology.org/Y03-1024</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using Mutual Information to Identify New Features for Text documents of Various Domains</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y03-1041" target="_blank">https://aclanthology.org/Y03-1041</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Restricting the rhetorical input for the non-hierarchical planning of document structures</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W03-2302" target="_blank">https://aclanthology.org/W03-2302</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Generation of Video Documentaries from Discourse Structures</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W03-2313" target="_blank">https://aclanthology.org/W03-2313</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-language Machine Translation through Interactive Document Normalization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W03-2204" target="_blank">https://aclanthology.org/W03-2204</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Annotation Tool for Multimodal Dialogue Corpora using Global Document Annotation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W03-2116" target="_blank">https://aclanthology.org/W03-2116</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Patent Document Retrieval System Addressing Both Semantic and Syntactic Properties</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W03-2001" target="_blank">https://aclanthology.org/W03-2001</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Lexical Paraphrasing for Document Retrieval and Node Identification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W03-1613" target="_blank">https://aclanthology.org/W03-1613</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unsupervised Monolingual and Bilingual Word-Sense Disambiguation of Medical Documents using UMLS</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W03-1302" target="_blank">https://aclanthology.org/W03-1302</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Selecting Text Features for Gene Name Classification: from Documents to Terms</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W03-1316" target="_blank">https://aclanthology.org/W03-1316</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Differential LSI Method for Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W03-1104" target="_blank">https://aclanthology.org/W03-1104</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Issues in Pre- and Post-translation Document Expansion: Untranslatable Cognates and Missegmented Words</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W03-1110" target="_blank">https://aclanthology.org/W03-1110</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Document Clustering by Utilizing Meta-Data</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W03-1114" target="_blank">https://aclanthology.org/W03-1114</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extraction of User Preferences from a Few Positive Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W03-1116" target="_blank">https://aclanthology.org/W03-1116</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Keyword-based Document Clustering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W03-1117" target="_blank">https://aclanthology.org/W03-1117</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Sub-event based multi-document summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W03-0502" target="_blank">https://aclanthology.org/W03-0502</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-document summarization using off the shelf compression software</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W03-0503" target="_blank">https://aclanthology.org/W03-0503</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Summarization of Noisy Documents: A Pilot Study</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W03-0504" target="_blank">https://aclanthology.org/W03-0504</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Study for Document Summarization Based on Personal Annotation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W03-0506" target="_blank">https://aclanthology.org/W03-0506</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A survey for Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W03-0509" target="_blank">https://aclanthology.org/W03-0509</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Geographic reference analysis for geographic document querying</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W03-0109" target="_blank">https://aclanthology.org/W03-0109</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document classification in structured military messages</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/U03-1017" target="_blank">https://aclanthology.org/U03-1017</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>iNeATS: Interactive Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P03-2021" target="_blank">https://aclanthology.org/P03-2021</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Orthogonal Negation in Vector Spaces for Modelling Word-Meanings and Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P03-1018" target="_blank">https://aclanthology.org/P03-1018</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Evaluation Challenges in Large-Scale Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P03-1048" target="_blank">https://aclanthology.org/P03-1048</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>文件自我擴展於自動分類之應用 (Application of Document Self-Expansion to Text Categorization) [In Chinese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O03-1008" target="_blank">https://aclanthology.org/O03-1008</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>QCS: A Tool for Querying Clustering and Summarizing Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N03-4006" target="_blank">https://aclanthology.org/N03-4006</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automating XML markup of text documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N03-2001" target="_blank">https://aclanthology.org/N03-2001</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatically Predicting Information Quality in News Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N03-2033" target="_blank">https://aclanthology.org/N03-2033</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Structure</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/J03-2003" target="_blank">https://aclanthology.org/J03-2003</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Reversing Controlled Document Authoring to Normalize Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/E03-3006" target="_blank">https://aclanthology.org/E03-3006</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Suregen-2: a shell system for the generation of clinical documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/E03-2008" target="_blank">https://aclanthology.org/E03-2008</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Cross Language Document Retrieval System Based on Semantic Annotation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/E03-2012" target="_blank">https://aclanthology.org/E03-2012</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Generic Text Interface (Canevas/GTi): Producing and publishing structured legal documents at the Court of Justice of the European Communities</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2003.tc-1.12" target="_blank">https://aclanthology.org/2003.tc-1.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilingual document processing at XRCE</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2003.mtsummit-plenaries.3" target="_blank">https://aclanthology.org/2003.mtsummit-plenaries.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Dans cet article nous pr&#39;esentons une m&#39;ethode qui vise `a donner `a un utilisateur la possibilit&#39;e de parcourir rapidement un ensemble de documents par le biais d&#39;un profil utilisateur. Un profil est un ensemble de termes structur&#39;e en sous-ensembles th&#39;ematiquement homog`enes. L&#39;analyse des documents se fonde pour sa part sur l&#39;extraction des passages les plus &#39;etroitement en relation avec ce profil. Cette analyse permet en particulier d&#39;&#39;etendre le vocabulaire d&#39;efinissant un profil en fonction du document trait&#39;e en s&#39;electionnant les termes de ce dernier les plus &#39;etroitement li&#39;es aux termes du profil. Cette capacit&#39;e ouvre ainsi la voie `a une plus grande finesse du filtrage en permettant la s&#39;election d&#39;extraits de documents ayant un lien plus t&#39;enu avec les profils mais davantage susceptibles d&#39;apporter des informations nouvelles et donc int&#39;eressantes. La production du r&#39;esum&#39;e r&#39;esulte de l&#39;appariement entre les segments d&#39;elimit&#39;es lors de l&#39;analyse des documents et les th`emes du profil.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extraction de segments th&#39;ematiques pour la construction de r&#39;esum&#39;e multi-document orient&#39;e par un profil utilisateur</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2003.jeptalnrecital-recital.3" target="_blank">https://aclanthology.org/2003.jeptalnrecital-recital.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2003</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Cet article concerne la structuration automatique de documents par des m&#39;ethodes linguistiques. De telles proc&#39;edures sont rendues n&#39;ecessaires par les nouvelles t^aches de recherche d&#39;information intradocumentaires (syst`emes de questions-r&#39;eponses navigation s&#39;elective dans des documents...). Nous d&#39;eveloppons une m&#39;ethode exploitant la th&#39;eorie de l&#39;encadrement du discours de Charolles avec une application vis&#39;ee en recherche d&#39;information dans les documents g&#39;eographiques - d&#39;o`u l&#39;int&#39;er^et tout particulier port&#39;e aux cadres spatiaux et temporels. Nous d&#39;ecrivons une impl&#39;ementation de la m&#39;ethode de d&#39;elimitation de ces cadres et son exploitation pour une t^ache d&#39;indexation intratextuelle croisant les crit`eres spatiaux et temporels avec des crit`eres th&#39;ematiques.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Indexation discursive pour la navigation intradocumentaire : cadres temporels et spatiaux dans l&#39;information g&#39;eographique</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2003.jeptalnrecital-poster.4" target="_blank">https://aclanthology.org/2003.jeptalnrecital-poster.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Generating Easy References: the Case of Document Deixis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W02-2115" target="_blank">https://aclanthology.org/W02-2115</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Features for Unsupervised Document Classification</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W02-2027" target="_blank">https://aclanthology.org/W02-2027</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cascading XSL Filters for Content Selection in Multilingual Document Generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W02-1702" target="_blank">https://aclanthology.org/W02-1702</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Brief Introduction to the GeM Annotation Schema for Complex Document Layout</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W02-1703" target="_blank">https://aclanthology.org/W02-1703</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cascaded Regular Grammars over XML Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W02-1712" target="_blank">https://aclanthology.org/W02-1712</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Knowledge-Based Multilingual Document Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W02-1101" target="_blank">https://aclanthology.org/W02-1101</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Selecting sentences for multidocument summaries using randomized local search</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W02-0402" target="_blank">https://aclanthology.org/W02-0402</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Revisions that improve cohesion in multi-document summaries: a preliminary study</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W02-0404" target="_blank">https://aclanthology.org/W02-0404</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using summaries in document retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W02-0405" target="_blank">https://aclanthology.org/W02-0405</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Noisy-Channel Model for Document Compression</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P02-1057" target="_blank">https://aclanthology.org/P02-1057</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>From Single to Multi-document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P02-1058" target="_blank">https://aclanthology.org/P02-1058</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>PILLS: Multilingual generation of medical information documents with overlapping content</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2002/pdf/34.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2002/pdf/34.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Developing Infrastructure for the Evaluation of Single and Multi-document Summarization Systems in a Cross-lingual Environment</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2002/pdf/158.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2002/pdf/158.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Class Library for the Integration of NLP Tools: Definition and implementation of an Abstract Data Type Collection for the manipulation of SGML documents in a context of stand-off linguistic annotation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2002/pdf/200.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2002/pdf/200.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Methods of Language Documentation in the DOBES project</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2002/pdf/221.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2002/pdf/221.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Measuring corpus homogeneity using a range of measures for inter-document distance</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2002/pdf/232.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2002/pdf/232.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Creation of an Annotated German Broadcast Speech Database for Spoken Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2002/pdf/292.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2002/pdf/292.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multidocument Summarization with GISTexter</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2002/pdf/350.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2002/pdf/350.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An XML-based Document Suite</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C02-2018" target="_blank">https://aclanthology.org/C02-2018</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Unknown Word Extraction for Chinese Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C02-1049" target="_blank">https://aclanthology.org/C02-1049</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Effective Structural Inference for Large XML Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C02-1069" target="_blank">https://aclanthology.org/C02-1069</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Lexical Query Paraphrasing for Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C02-1161" target="_blank">https://aclanthology.org/C02-1161</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>La probl&#39;ematique de la normalisation de documents est introduite et illustr&#39;ee par des exemples issus de notices pharmaceutiques. Un paradigme pour l&#39;analyse du contenu des documents est propos&#39;e. Ce paradigme se base sur la sp&#39;ecification formelle de la s&#39;emantique des documents et utilise une notion de similarit&#39;e floue entre les pr&#39;edictions textuelles d&#39;un g&#39;en&#39;erateur de texte et le texte du document `a analyser. Une impl&#39;ementation initiale du paradigme est pr&#39;esent&#39;ee.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Normalisation de documents par analyse du contenu `a l&#39;aide d&#39;un mod`ele s&#39;emantique et d&#39;un g&#39;en&#39;erateur</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2002.jeptalnrecital-recital.8" target="_blank">https://aclanthology.org/2002.jeptalnrecital-recital.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>En recherche documentaire on repr&#39;esente souvent les documents textuels par des vecteurs lexicaux de grande dimension qui sont redondants et co^uteux. Il est utile de r&#39;eduire la dimension des ces repr&#39;esentations pour des raisons `a la fois techniques et s&#39;emantiques. Cependant les techniques classiques d&#39;analyse factorielle comme l&#39;ACP ne permettent pas de traiter des vecteurs de tr`es grande dimension. Nous avons alors utilis&#39;e une m&#39;ethode adaptative neuronale (GHA) qui s&#39;est r&#39;ev&#39;el&#39;ee efficace pour calculer un nombre r&#39;eduit de nouvelles dimensions repr&#39;esentatives des donn&#39;ees. L&#39;approche nous a permis de classer un corpus r&#39;eel de pages Web avec de bons r&#39;esultats.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Analyse Factorielle Neuronale pour Documents Textuels</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2002.jeptalnrecital-long.1" target="_blank">https://aclanthology.org/2002.jeptalnrecital-long.1</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>L&#39;int&#39;egration de co-occurrences dans les mod`eles de repr&#39;esentation vectorielle de documents s&#39;est av&#39;er&#39;ee une source d&#39;am&#39;elioration de la pertinence des mesures de similarit&#39;es textuelles calcul&#39;ees dans le cadre de ces mod`eles (Rajman et al. 2000; Besanccon 2001). Dans cette optique la d&#39;efinition des contextes pris en compte pour les co-occurrences est cruciale par son influence sur les performances des mod`eles `a base de co-occurrences. Dans cet article nous proposons d&#39;&#39;etudier deux m&#39;ethodes de filtrage des co-occurrences fond&#39;ees sur l&#39;utilisation d&#39;informations syntaxiques suppl&#39;ementaires. Nous pr&#39;esentons &#39;egalement une &#39;evaluation de ces m&#39;ethodes dans le cadre de la t^ache de la recherche documentaire.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Filtrages syntaxiques de co-occurrences pour la repr&#39;esentation vectorielle de documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2002.jeptalnrecital-long.11" target="_blank">https://aclanthology.org/2002.jeptalnrecital-long.11</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2002</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>La co&#39;edition langue↔UNL pour partager la r&#39;evision entre les langues d&#39;un document multilingue : un concept unificateur</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2002.jeptalnrecital-long.25" target="_blank">https://aclanthology.org/2002.jeptalnrecital-long.25</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2001</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Answer Mining from On-Line Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W01-1206" target="_blank">https://aclanthology.org/W01-1206</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2001</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Terminological Variants for Document Selection and Question/Answer Matching</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W01-1207" target="_blank">https://aclanthology.org/W01-1207</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2001</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Fusion for Comprehensive Event Description</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W01-1008" target="_blank">https://aclanthology.org/W01-1008</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2001</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Invited Speaker - Supporting Organisational Learning through the Enrichment of Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W01-1010" target="_blank">https://aclanthology.org/W01-1010</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2001</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Structuring `a la SDRT</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W01-0803" target="_blank">https://aclanthology.org/W01-0803</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2001</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Detecting Short Passages of Similar Text in Large Document Collections</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W01-0515" target="_blank">https://aclanthology.org/W01-0515</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2001</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>多篇文件自動摘要系統 (Multi-Document Summarization System) [In Chinese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O01-1003" target="_blank">https://aclanthology.org/O01-1003</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2001</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>基於階層式類神經網路之自動新聞文件分類方法 (Hierarchical Neural Networks for Automatic News Document Categorization) [In Chinese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O01-1004" target="_blank">https://aclanthology.org/O01-1004</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2001</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Why Inverse Document Frequency?</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/N01-1004" target="_blank">https://aclanthology.org/N01-1004</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2001</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using Suffix Arrays to Compute Term Frequency and Document Frequency for All Substrings in a Corpus</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/J01-1001" target="_blank">https://aclanthology.org/J01-1001</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2001</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>First Story Detection using a Composite Document Representation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/H01-1030" target="_blank">https://aclanthology.org/H01-1030</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2001</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multidocument Summarization via Information Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/H01-1054" target="_blank">https://aclanthology.org/H01-1054</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2001</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>NewsInEssence: A System For Domain-Independent Real-Time News Clustering and Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/H01-1056" target="_blank">https://aclanthology.org/H01-1056</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2001</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Sentence Ordering in Multidocument Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/H01-1065" target="_blank">https://aclanthology.org/H01-1065</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2001</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Quality Assurance in the Technical Documentation and Translation Process</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2001.tc-1.3" target="_blank">https://aclanthology.org/2001.tc-1.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2001</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We present an RST-based discourse annotation proposal used in the construction of a trial multilingual XML-tagged corpus of teaching material in Basque English and Spanish. The corpus feeds an experimental multilingual document generation system for the web. The main contributions of this paper are an implementation of RST through XML metadata and the adoption of gross-grained RST to avoid non-isomorphism in multilingual corpora.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Gross-grained RST through XML metadata for multilingual document generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2001.mtsummit-papers.7" target="_blank">https://aclanthology.org/2001.mtsummit-papers.7</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2001</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>L&#39;utilisation de connaissances et de traitements linguistiques &#39;evolu&#39;es en recherche documentaire ne fait pas l&#39;unanimit&#39;e dans le milieu scientifique. En effet de nombreuses exp&#39;eriences semblent montrer que les r&#39;esultats obtenus ne sont pas am&#39;elior&#39;es voire sont parfois d&#39;egrad&#39;es lorsque de telles connaissances sont utilis&#39;ees dans un syst`eme de RD. Dans ce tutoriel nous montrons que les environnements d&#39;&#39;evaluation ne sont pas adapt&#39;es aux besoins r&#39;eels d&#39;un utilisateur car celui-ci recherche presque toujours une information. Il veut donc retrouver des documents pertinents le plus rapidement possible car ce n&#39;est pas l`a le but de sa recherche. Le temps global de la recherche est donc fondamentalement important. N&#39;eanmoins le cadre d&#39;&#39;evaluation TREC nous permet de montrer que l&#39;utilisation de connaissances linguistiques permet d&#39;augmenter la pr&#39;ecision des premiers documents renvoy&#39;es ce qui est tr`es important pour diminuer le temps de recherche.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>L&#39;apport de connaissances linguistiques en recherche documentaire</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2001.jeptalnrecital-tutoriel.5" target="_blank">https://aclanthology.org/2001.jeptalnrecital-tutoriel.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2001</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>L&#39;indexation audiovisuelle indispensable pour l&#39;archivage et l&#39;exploitation des documents se r&#39;ev`ele ^etre un processus d&#39;elicat notamment `a cause de la multiplicit&#39;e de significations qui peuvent ^etre attach&#39;ees aux images. Nous proposons dans cette communication une m&#39;ethode d&#39;instanciation de &#39;&#39;patrons d&#39;indexation&#39;&#39; `a partir d&#39;un corpus d&#39;articles de journaux &#39;ecrits. Cette m&#39;ethode repose sur un processus &#39;&#39;d&#39;amorccage hi&#39;erachis&#39;e&#39;&#39; qui permet de trouver de nouveaux termes `a partir de termes connus dans leur voisinage et de leurs relations taxinomiques sous forme d&#39;ontologie.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extraction d&#39;information de documents textuels associ&#39;es `a des contenus audiovisuels</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2001.jeptalnrecital-recitalposter.3" target="_blank">https://aclanthology.org/2001.jeptalnrecital-recitalposter.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2001</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Un nombre important de requ^etes soumises aux moteurs de recherche du W3 ne satisfont pas pleinement les attentes des utilisateurs. La liste de documents propos&#39;ee en retour est souvent trop longue : son exploration repr&#39;esente un travail exag&#39;er&#39;ement laborieux pour l&#39;auteur de la requ^ete. Nous proposons d&#39;apporter une valeur ajout&#39;ee aux syst`emes de recherche documentaire (RD) existants en y ajoutant un filtrage n&#39;utilisant que des donn&#39;ees fournies par l&#39;utilisateur. L&#39;objectif de notre &#39;etude est de confronter un mod`ele dynamique de la m&#39;emoire s&#39;emantique des individus (ou des agents) d&#39;evelopp&#39;e par notre &#39;equipe `a une t^ache n&#39;ecessitant une comp&#39;etence interpr&#39;etative de la part des machines. Nous souhaitons d&#39;epasser la s&#39;emantique lexicale couramment utilis&#39;ee dans ce champ d&#39;application pour aboutir `a l&#39;utilisation d&#39;une s&#39;emantique des textes et accro^itre par ce biais `a la fois la qualit&#39;e des r&#39;esultats et la qualit&#39;e de leur pr&#39;esentation aux usagers.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>La recherche documentaire : une activit&#39;e langagi`ere</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2001.jeptalnrecital-recital.7" target="_blank">https://aclanthology.org/2001.jeptalnrecital-recital.7</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Improving Natural Language Processing by Linguistic Document Annotation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W00-1703" target="_blank">https://aclanthology.org/W00-1703</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document structure and multilingual authoring</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W00-1404" target="_blank">https://aclanthology.org/W00-1404</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DTD-driven bilingual document generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W00-1405" target="_blank">https://aclanthology.org/W00-1405</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Common Theory of Information Fusion from Multiple Text Sources Step One: Cross-Document Structure</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W00-1009" target="_blank">https://aclanthology.org/W00-1009</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Transformations and Information States</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W00-1013" target="_blank">https://aclanthology.org/W00-1013</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Centroid-based summarization of multiple documents: sentence extraction utility-based evaluation and user studies</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W00-0403" target="_blank">https://aclanthology.org/W00-0403</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extracting Key Paragraph based on Topic and Event Detection Towards Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W00-0404" target="_blank">https://aclanthology.org/W00-0404</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Document Summarization By Sentence Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W00-0405" target="_blank">https://aclanthology.org/W00-0405</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-document Summarization by Visualizing Topical Content</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W00-0409" target="_blank">https://aclanthology.org/W00-0409</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using Long Runs as Predictors of Semantic Coherence in a Partial Document Retrieval System</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W00-0102" target="_blank">https://aclanthology.org/W00-0102</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Proposal for the Integration of NLP Tools using SGML-Tagged Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2000/pdf/68.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2000/pdf/68.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Architecture for Document Routing in Spanish: Two Language Components Pre-processor and Parser</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2000/pdf/91.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2000/pdf/91.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Evaluating Summaries for Multiple Documents in an Interactive Environment</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2000/pdf/163.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2000/pdf/163.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Framework for Cross-Document Annotation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2000/pdf/201.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2000/pdf/201.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extraction of Concepts and Multilingual Information Schemes from French and English Economics Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2000/pdf/202.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2000/pdf/202.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A New Methodology for Speech Corpora Definition from Internet Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="http://www.lrec-conf.org/proceedings/lrec2000/pdf/235.pdf" target="_blank">http://www.lrec-conf.org/proceedings/lrec2000/pdf/235.pdf</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multi-Topic Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C00-2129" target="_blank">https://aclanthology.org/C00-2129</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The Week at a Glance - Cross-language Cross-document Information Extraction and Translation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C00-2147" target="_blank">https://aclanthology.org/C00-2147</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The effects of analysing cohesion on document summarisation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C00-1012" target="_blank">https://aclanthology.org/C00-1012</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Authoring Multimedia Documents using WYSIWYM Editing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C00-1033" target="_blank">https://aclanthology.org/C00-1033</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>XML and Multilingual Document Authoring: Convergent Trends</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C00-1036" target="_blank">https://aclanthology.org/C00-1036</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Evaluation of Automatically Identified Index Terms for Browsing Electronic Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/A00-1042" target="_blank">https://aclanthology.org/A00-1042</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Language Checker of Controlled Language and its Integration in a Documentation and Translation Workflow</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2000.tc-1.4" target="_blank">https://aclanthology.org/2000.tc-1.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Applying machine translation resources for cross-language information access from spoken documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2000.bcs-1.4" target="_blank">https://aclanthology.org/2000.bcs-1.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Reusability of wide-coverage linguistic resources in the construction of multilingual technical documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/2000.bcs-1.12" target="_blank">https://aclanthology.org/2000.bcs-1.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>This paper discusses an informal methodology for evaluating Machine Translation software documentation with reference to a case study in which a number of currently available MT packages are evaluated. Different types of documentation style are discussed as well as different user profiles. It is found that documentation is often inadequate in identifying the level of linguistic background and knowledge necessary to use translation software and in explaining technical (linguistic) terms needed to use the software effectively. In particular the level of knowledge and training needed to use the software is often incompatible with the user profile implied by the documentation. Also guidance on how to perform more complex tasks which may be especially idiosyncratic is often inadequate or missing altogether.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Is MT software documentation appropriate for MT users?</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://link.springer.com/chapter/10.1007/3-540-39965-8_26" target="_blank">https://link.springer.com/chapter/10.1007/3-540-39965-8_26</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Parallel corpora enriched with descriptive annotations facilitate multilingual authoring development. Departing from an annotated bitext we show how SGML markup can be recycled to produce complementary language resources. On the one hand several translation memory databases together with glossaries of proper nouns have been produced. On the other DTDs for source and target documents have been derived and put into correspondence. This paper discusses how these resources have been automatically generated and applied to an interactive bilingual authoring system. This tool is capable of handling a substantial proportion of text both in the composition and translation of structured documents.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Recycling annotated parallel corpora for bilingual document composition</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://link.springer.com/chapter/10.1007/3-540-39965-8_12" target="_blank">https://link.springer.com/chapter/10.1007/3-540-39965-8_12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Extracting Keywords from Digital Document Collections</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W99-1009" target="_blank">https://aclanthology.org/W99-1009</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>2000</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Designing a System for Swedish Spoken Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W99-1017" target="_blank">https://aclanthology.org/W99-1017</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1999</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-Language Information Retrieval for Technical Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W99-0605" target="_blank">https://aclanthology.org/W99-0605</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1999</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A mark up language for tagging discourse and annotating documents in context sensitive interpretation environments</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W99-0312" target="_blank">https://aclanthology.org/W99-0312</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1999</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Cross-Document Event Coreference: Annotations Experiments and Observations</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W99-0201" target="_blank">https://aclanthology.org/W99-0201</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1999</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Is Hillary Rodham Clinton the President? Disambiguating Names across Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W99-0202" target="_blank">https://aclanthology.org/W99-0202</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1999</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Slide Presentation from Semantically Annotated Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W99-0204" target="_blank">https://aclanthology.org/W99-0204</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1999</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Should we Translate the Documents or the Queries in Cross-language Information Retrieval?</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P99-1027" target="_blank">https://aclanthology.org/P99-1027</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1999</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Information Fusion in the Context of Multi-Document Summarization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P99-1071" target="_blank">https://aclanthology.org/P99-1071</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1999</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A New Syllable-based Approach for Retrieving Mandarin Spoken Documents Using Short Speech Queries</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O99-1008" target="_blank">https://aclanthology.org/O99-1008</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1999</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Case Study: Document Management and Localization</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1999.tc-1.3" target="_blank">https://aclanthology.org/1999.tc-1.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1999</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>In this paper we describe a language recognition algorithm for multilingual documents that is based on mixed-order n-grams Markov chains maximum likelihood and dynamic programming. We present the results of an experimental study that showed that the performance of this algorithm has practical value.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilingual document language recognition for creating corpora</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1999.mtsummit-1.46" target="_blank">https://aclanthology.org/1999.mtsummit-1.46</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1999</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An automated mandarin document revision system using both phonetic and radical approaches</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1999.mtsummit-1.67" target="_blank">https://aclanthology.org/1999.mtsummit-1.67</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1999</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Experience from translation of EU documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1999.eamt-1.3" target="_blank">https://aclanthology.org/1999.eamt-1.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1999</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Aligning and extracting translation equivalents from EU documents - a possible look on EU Integration</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1999.eamt-1.4" target="_blank">https://aclanthology.org/1999.eamt-1.4</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Surrogater : A Simple Yet Efficient Document Condensation System</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y98-1025" target="_blank">https://aclanthology.org/Y98-1025</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Automatic Chinese Document Revision System Using Bit and Character Mask Approach</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y98-1028" target="_blank">https://aclanthology.org/Y98-1028</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multiple &amp; Single Document Summarization Using DR-LINK</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/X98-1027" target="_blank">https://aclanthology.org/X98-1027</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Evolution and Evaluation of Document Retrieval Queries</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W98-1220" target="_blank">https://aclanthology.org/W98-1220</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Using Suffix Arrays to Compute Term Frequency and Document Frequency for All Substrings in a Corpus</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W98-1104" target="_blank">https://aclanthology.org/W98-1104</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Simplex NPs Clustered by Head: A Method for Identifying Significant Topics Within a Document</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W98-0610" target="_blank">https://aclanthology.org/W98-0610</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The Pausanian Notation: a method for representing the structure and the content of a hyperdocument</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W98-0201" target="_blank">https://aclanthology.org/W98-0201</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Text Summarization Based on the Global Document Annotation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P98-2151" target="_blank">https://aclanthology.org/P98-2151</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Entity-Based Cross-Document Coreferencing Using the Vector Space Model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P98-1012" target="_blank">https://aclanthology.org/P98-1012</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A Step towards the Detection of Semantic Variants of Terms in Technical Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P98-1082" target="_blank">https://aclanthology.org/P98-1082</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Role of Verbs in Document Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P98-1112" target="_blank">https://aclanthology.org/P98-1112</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Information Extraction: Beyond Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O98-4002" target="_blank">https://aclanthology.org/O98-4002</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Text Summarization Based on the Global Document Annotation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C98-2146" target="_blank">https://aclanthology.org/C98-2146</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Entity-Based Cross-Document Coreferencing Using the Vector Space Model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C98-1012" target="_blank">https://aclanthology.org/C98-1012</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A step towards the detection of semantic variants of terms in technical documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C98-1079" target="_blank">https://aclanthology.org/C98-1079</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Role of Verbs in Document Analysis</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C98-1108" target="_blank">https://aclanthology.org/C98-1108</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilingual Human Language Technology in Automotive Documentation Workflows</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1998.tc-1.14" target="_blank">https://aclanthology.org/1998.tc-1.14</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The MULTIDOC Project: MULTI-lingual product DOCumentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1998.eamt-1.8" target="_blank">https://aclanthology.org/1998.eamt-1.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We describe a statistical algorithm for machine translation intended to provide translations of large document collections at speeds far in excess of traditional machine translation systems and of sufficiently high quality to perform information retrieval on the translated document collections. The model is trained from a parallel corpus and is capable of disambiguating senses of words. Information retrieval (IR) experiments on a French language dataset from a recent cross-language information retrieval evaluation yields results superior to those obtained by participants in the evaluation and confirm the importance of word sense disambiugation in cross-language information retrieval.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Fast document translation for cross-language information retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://link.springer.com/chapter/10.1007/3-540-49478-2_14" target="_blank">https://link.springer.com/chapter/10.1007/3-540-49478-2_14</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Due to the explosive growth of the WWW very large multilingual textual resources have motivated the researches in Cross-Language Information Retrieval and online Web Machine Translation. In this paper the integration of language translation and text processing system is proposed to build a multilingual information system. A distributed English-Chinese system on WWW is introduced to illustrate how to integrate query translation search engines and web translation system. Since July 1997 more than 46000 users have accessed our system and about 250000 English web pages have been translated to pages in Chinese or bilingual English-Chinese versions. And the average satisfaction degree of users at document level is 67.47%.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Integrating query translation and document translation in a cross-language information retrieval system</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://link.springer.com/chapter/10.1007/3-540-49478-2_23" target="_blank">https://link.springer.com/chapter/10.1007/3-540-49478-2_23</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1998</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Cross-language retrieval systems use queries in one natural language to guide retrieval of documents that might be written in another. Acquisition and representation of translation knowledge plays a central role in this process. This paper explores the utility of two sources of translation knowledge for cross-language retrieval. We have implemented six query translation techniques that use bilingual term lists and one based on direct use of the translation output from an existing machine translation system; these are compared with a document translation technique that uses output from the same machine translation system. Average precision measures on a TREC collection suggest that arbitrarily selecting a single dictionary translation is typically no less effective than using every translation in the dictionary that query translation using a machine translation system can achieve somewhat better effectiveness than simpler techniques and that document translation may result in further improvements in retrieval effectiveness under some conditions.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>A comparative study of query and document translation for cross-language information retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://link.springer.com/chapter/10.1007/3-540-49478-2_42" target="_blank">https://link.springer.com/chapter/10.1007/3-540-49478-2_42</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1997</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Salience-based Content Characterisafion of Text Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W97-0702" target="_blank">https://aclanthology.org/W97-0702</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1997</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Classification Using a Finite Mixture Model</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P97-1006" target="_blank">https://aclanthology.org/P97-1006</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1997</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>中央研究院古籍全文資料庫的發展概要 (A Survey of Full-text Data Bases and Related Techniques for Chinese Ancient Documents in Academia Sinica) [in Chinese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O97-3005" target="_blank">https://aclanthology.org/O97-3005</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1997</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>EasyEnglish: A Tool for Improving Document Quality</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/A97-1024" target="_blank">https://aclanthology.org/A97-1024</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1997</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Fast Statistical Parsing of Noun Phrases for Document Indexing</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/A97-1046" target="_blank">https://aclanthology.org/A97-1046</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1997</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MULTI-lingual DOCumentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1997.mtsummit-systems.10" target="_blank">https://aclanthology.org/1997.mtsummit-systems.10</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1997</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The workflow in a document production environment using translation tools</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1997.eamt-1.8" target="_blank">https://aclanthology.org/1997.eamt-1.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1997</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Localizing Canon&#39;s user documentation in Europe</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1997.eamt-1.9" target="_blank">https://aclanthology.org/1997.eamt-1.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1996</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Integration of Document Detection and Information Extraction</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/X96-1036" target="_blank">https://aclanthology.org/X96-1036</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1996</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>List of Available Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/X96-1061" target="_blank">https://aclanthology.org/X96-1061</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1996</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Identifying the Coding System and Language of On-line Documents on the Internet</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C96-2110" target="_blank">https://aclanthology.org/C96-2110</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1996</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Building Knowledge Bases for the Generation of Software Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C96-2124" target="_blank">https://aclanthology.org/C96-2124</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1996</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Classification Using Domain Specific Kanji Characters Extracted by X2 Method</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C96-2134" target="_blank">https://aclanthology.org/C96-2134</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1996</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Content-Oriented Categorization of Document Images</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C96-2138" target="_blank">https://aclanthology.org/C96-2138</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1996</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilint - a Technical Documentation System with Multilingual Intelligence</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1996.tc-1.2" target="_blank">https://aclanthology.org/1996.tc-1.2</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1996</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Computer Support for Authoring Multilingual Software Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1996.tc-1.12" target="_blank">https://aclanthology.org/1996.tc-1.12</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1995</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Ranking Method for High Precision Rate</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/Y95-1033" target="_blank">https://aclanthology.org/Y95-1033</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1995</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Inverse Document Frequency (IDF): A Measure of Deviations from Poisson</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W95-0110" target="_blank">https://aclanthology.org/W95-0110</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1995</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>適合大量中文文件全文檢索的索引及資料壓縮技術 (Full-text Indexing and Data Compression for Chinese Documents) [In Chinese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O95-1002" target="_blank">https://aclanthology.org/O95-1002</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1995</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Pre- and Post-editing MT text for better communication and documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1995.mtsummit-1.25" target="_blank">https://aclanthology.org/1995.mtsummit-1.25</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1994</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Learning from Relevant Documents in Large Scale Routing Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/H94-1071" target="_blank">https://aclanthology.org/H94-1071</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1994</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Representation in Natural Language Text Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/H94-1072" target="_blank">https://aclanthology.org/H94-1072</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1994</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Integrated Text and Image Understanding for Document Understanding</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/H94-1084" target="_blank">https://aclanthology.org/H94-1084</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1994</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>PIRCS: a Network-Based Document Routing and Retrieval System</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/H94-1112" target="_blank">https://aclanthology.org/H94-1112</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1994</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Classification by Machine:Theory and Practice</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C94-2172" target="_blank">https://aclanthology.org/C94-2172</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1994</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Generating Multilingual Documents from a Knowledge Base The TECHDOC Project</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C94-1055" target="_blank">https://aclanthology.org/C94-1055</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1994</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Practical Issues in Automatic Documentation Generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/A94-1002" target="_blank">https://aclanthology.org/A94-1002</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1994</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Language Determination: Natural Language Processing from Scanned Document Images</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/A94-1003" target="_blank">https://aclanthology.org/A94-1003</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1994</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Modeling Content Identification from Document Images</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/A94-1004" target="_blank">https://aclanthology.org/A94-1004</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1994</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Exploiting Sophisticated Representations for Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/A94-1011" target="_blank">https://aclanthology.org/A94-1011</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1994</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>We argue that in many situations Dialogue-Based MT is likely to offer better solutions to translation needs than machine aids to translators or batch MT even if controlled languages are used. Objections to DBMT have led us to introduce the new concept of ``self-explaining document&#39;&#39; which might be used in monolingual as well as in multilingual contexts and deeply change our way of understanding important or difficult written material.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Dialogue-Based MT and self-explaining documents as an alternative to MAHT and MT of controlled languages</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1994.bcs-1.22" target="_blank">https://aclanthology.org/1994.bcs-1.22</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1993</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Detection Overview</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/X93-1005" target="_blank">https://aclanthology.org/X93-1005</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1993</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Detection Data Preparation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/X93-1006" target="_blank">https://aclanthology.org/X93-1006</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1993</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Detection Summary of Results</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/X93-1007" target="_blank">https://aclanthology.org/X93-1007</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1993</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document Filtering using Semantic Information from a Machine Readable Dictionary</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W93-0303" target="_blank">https://aclanthology.org/W93-0303</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1993</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The Representation of Interdependencies between Communicative Goals and Rhetorical Relations in the Framework of Multimedia Document Generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W93-0219" target="_blank">https://aclanthology.org/W93-0219</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1993</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Textual Constraints Rhetorical RElations and Communicative Goals and Rhetorical Relations in the Framework of Multimedia Document Generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W93-0221" target="_blank">https://aclanthology.org/W93-0221</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1993</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Intentions Rhetoric or Discourse Relations ? A Case from Multilingual Document Generation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/W93-0228" target="_blank">https://aclanthology.org/W93-0228</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1993</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>中文文件自動分類之研究 (A Study of Document Auto-Classification in Mandarin Chinese) [In Chinese]</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/O93-1010" target="_blank">https://aclanthology.org/O93-1010</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1993</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document retrieval and text retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/H93-1069" target="_blank">https://aclanthology.org/H93-1069</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1993</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Overview of DR-LINK and Its Approach to Document Filtering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/H93-1072" target="_blank">https://aclanthology.org/H93-1072</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1993</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Speech and Text-Image Processing in Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/H93-1076" target="_blank">https://aclanthology.org/H93-1076</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1993</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Machine Learning Techniques for Document Filtering</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/H93-1077" target="_blank">https://aclanthology.org/H93-1077</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1993</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>MatchPlus: A Context Vector System for Document Retrieval</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/H93-1090" target="_blank">https://aclanthology.org/H93-1090</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1993</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>DR-LINK: Document Retrieval Using Linguistic Knowledge</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/H93-1110" target="_blank">https://aclanthology.org/H93-1110</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1993</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Text Alignment in a Tool for Translating Revised Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/E93-1054" target="_blank">https://aclanthology.org/E93-1054</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1993</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Collaborative authoring of hypermedia documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1993.tc-1.5" target="_blank">https://aclanthology.org/1993.tc-1.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1992</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Documentation Parser to Extract Software Test Conditions</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P92-1042" target="_blank">https://aclanthology.org/P92-1042</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1992</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Multilinguisation d&#39;un editeur de documents structures. Application a un dictionnaire trilingue</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C92-3148" target="_blank">https://aclanthology.org/C92-3148</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1992</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Generation of On-Line Documentation in the IDAS Project</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/A92-1009" target="_blank">https://aclanthology.org/A92-1009</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1992</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Computer aided translation in an integrated document production process: Tools and applications</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1992.tc-1.5" target="_blank">https://aclanthology.org/1992.tc-1.5</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1992</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Interchange of documents in electronic form utilising international standards</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1992.tc-1.18" target="_blank">https://aclanthology.org/1992.tc-1.18</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1991</span>
                
            </p>
            <p>
                <span class="fw-bold">abstract: </span>
                <span>Knowledge-based interlingual machine translation systems produce semantically accurate translations but typically require massive knowledge acquisition. This paper describes KANT a system that reduces this requirement to produce practical scalable and accurate KBMT applications. First the set of requirements is discussed then the full KANT architecture is illustrated and finally results from a fully implemented prototype are presented.</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>An Efficient Interlingua Translation System for Multi-lingual Document Production</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1991.mtsummit-papers.9" target="_blank">https://aclanthology.org/1991.mtsummit-papers.9</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1990</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Sentence disambiguation by document oriented preference sets</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C90-2032" target="_blank">https://aclanthology.org/C90-2032</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1990</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Concept Analysis and Terminology: A Knowledge-Based Approach to Documentation</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C90-1011" target="_blank">https://aclanthology.org/C90-1011</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1990</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Translation and Computers: an overview of the Polish translation scene The growing range of document preparation systems</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1990.tc-1.2" target="_blank">https://aclanthology.org/1990.tc-1.2</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1990</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>The growing range of document preparation systems</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1990.tc-1.3" target="_blank">https://aclanthology.org/1990.tc-1.3</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1990</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Document conversion</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1990.tc-1.6" target="_blank">https://aclanthology.org/1990.tc-1.6</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1989</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Translation and republication of technical documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1989.tc-1.14" target="_blank">https://aclanthology.org/1989.tc-1.14</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1987</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Towards an Integrated Environment for Spanish Document Verification and Composition</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/E87-1009" target="_blank">https://aclanthology.org/E87-1009</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1987</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Translation of Engineering Documentation with METAL</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/1987.mtsummit-1.8" target="_blank">https://aclanthology.org/1987.mtsummit-1.8</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1984</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Machine Translation : What Type of Post-Editing on What Type of Documents for What Type of Users</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/P84-1051" target="_blank">https://aclanthology.org/P84-1051</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1980</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Hierarchical Meaning Representation and Analysis of Natural Language Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C80-1013" target="_blank">https://aclanthology.org/C80-1013</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1969</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Automatic Processing of Foreign Language Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C69-0401" target="_blank">https://aclanthology.org/C69-0401</a>
            </p>
        </li>
        <li class="list-group-item">
            <p>
                <span class="fw-bold">year: </span>
                <span>1967</span>
                
            </p>
            <p>
                <span class="fw-bold">title: </span>
                <span>Problemes Syntaxiques De L&#39;indexation Automatique De Documents</span>
                
            </p>
            <p>
                <span class="fw-bold">url: </span>
                
                <a href="https://aclanthology.org/C67-1024" target="_blank">https://aclanthology.org/C67-1024</a>
            </p>
        </li>
    </ol>
</div>
</body>
</html>